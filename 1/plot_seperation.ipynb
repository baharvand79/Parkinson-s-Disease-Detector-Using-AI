{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# --- Configuration ---\n",
    "# =============================================================================\n",
    "\n",
    "# Class definitions\n",
    "HEALTHY_CLASS = \"healthy_control\"\n",
    "PARKINSON_CLASS = \"parkinson_patient\"\n",
    "CLASSES = [HEALTHY_CLASS, PARKINSON_CLASS]\n",
    "\n",
    "# Dataset and mode selection\n",
    "ITALIAN_DATASET = \"ITALIAN_DATASET\"\n",
    "NEUROVOZ_DATASET = \"NEUROVOZ_DATASET\"\n",
    "UAMS_DATASET = \"UAMS_DATASET\"\n",
    "MPOWER_DATASET = \"MPOWER_DATASET\"\n",
    "MODE_A = \"A\"\n",
    "MODE_ALL_VALIDS = \"ALL_VALIDS\"\n",
    "\n",
    "FEATURE_MODE_DEFAULT = \"DEFAULT\"\n",
    "FEATURE_MODE_ALL = \"ALL\"\n",
    "\n",
    "# --- SELECT YOUR CONFIGURATION HERE ---\n",
    "DATASET = MPOWER_DATASET\n",
    "MODE = MODE_A\n",
    "FEATURE_MODE = FEATURE_MODE_DEFAULT\n",
    "FOLDER_NAME = \"plots\"\n",
    "\n",
    "# --- Path Setup ---\n",
    "dataset_folder_name = \"Italian\" if DATASET == ITALIAN_DATASET else \"Neurovoz\"\n",
    "dataset_folder_name = \"\"\n",
    "if DATASET == ITALIAN_DATASET:\n",
    "    dataset_folder_name = \"Italian\"\n",
    "elif DATASET == NEUROVOZ_DATASET:\n",
    "    dataset_folder_name = \"Neurovoz\"\n",
    "elif DATASET == UAMS_DATASET:\n",
    "    dataset_folder_name = \"UAMS\"\n",
    "elif DATASET == MPOWER_DATASET:\n",
    "    dataset_folder_name = \"mPower\"\n",
    "\n",
    "FEATURES_FILE = os.path.join(os.getcwd(), dataset_folder_name, \"data\", f\"features_{MODE}_{FEATURE_MODE}.npz\")\n",
    "RESULTS_OUTPUT_PATH = os.path.join(os.getcwd(), dataset_folder_name, f\"results_{MODE}_{FEATURE_MODE}\", FOLDER_NAME)\n",
    "os.makedirs(RESULTS_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# --- Helper Functions ---\n",
    "# =============================================================================\n",
    "\n",
    "def get_feature_keys(feature_mode):\n",
    "    \"\"\"Returns the list of feature keys based on the selected mode.\"\"\"\n",
    "    if feature_mode == FEATURE_MODE_DEFAULT:\n",
    "        return ['mel_spectrogram', 'mfcc']\n",
    "    elif feature_mode == FEATURE_MODE_ALL:\n",
    "        return ['spectrogram', 'mel_spectrogram', 'mfcc', 'fsc']\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown FEATURE_MODE: {feature_mode}\")\n",
    "\n",
    "# =============================================================================\n",
    "# --- Data Loading and Preparation ---\n",
    "# =============================================================================\n",
    "\n",
    "def load_features(features_file):\n",
    "    \"\"\"Loads features from the specified .npz file.\"\"\"\n",
    "    print(f\"Loading features from {features_file}\")\n",
    "    if not os.path.exists(features_file):\n",
    "        raise FileNotFoundError(f\"Features file not found: {features_file}\")\n",
    "\n",
    "    with np.load(features_file) as data:\n",
    "        features = {key: data[key] for key in data.keys()}\n",
    "\n",
    "    print(\"Loaded feature shapes:\")\n",
    "    total_memory = 0\n",
    "    for key, value in features.items():\n",
    "        memory_mb = value.nbytes / (1024 * 1024)\n",
    "        print(f\"  - {key}: {value.shape} ({memory_mb:.2f} MB)\")\n",
    "        if key != 'labels':\n",
    "            total_memory += memory_mb\n",
    "    print(f\"Total feature memory: {total_memory:.2f} MB\")\n",
    "    return features\n",
    "\n",
    "def prepare_features_efficiently(features_dict, feature_mode, max_samples=None, use_subsample=True):\n",
    "    \"\"\"Prepares features for dimensionality reduction with a focus on memory efficiency.\"\"\"\n",
    "    print(f\"--- Preparing Features for Mode: {feature_mode} (Memory Efficient) ---\")\n",
    "\n",
    "    feature_keys = get_feature_keys(feature_mode)\n",
    "    print(f\"Using features: {feature_keys}\")\n",
    "    labels = features_dict['labels']\n",
    "    n_samples = len(labels)\n",
    "\n",
    "    if use_subsample and n_samples > 2000:\n",
    "        if max_samples is None: max_samples = 2000\n",
    "        print(f\"Large dataset ({n_samples} samples). Stratified subsampling to ~{max_samples} for visualization.\")\n",
    "        indices = []\n",
    "        unique_labels = np.unique(labels)\n",
    "        samples_per_class = max_samples // len(unique_labels)\n",
    "        for label in unique_labels:\n",
    "            label_indices = np.where(labels == label)[0]\n",
    "            selected_indices = np.random.choice(label_indices, min(len(label_indices), samples_per_class), replace=False)\n",
    "            indices.extend(selected_indices)\n",
    "        indices = np.array(indices)\n",
    "        np.random.shuffle(indices)\n",
    "    else:\n",
    "        indices = np.arange(n_samples)\n",
    "\n",
    "    labels_subsampled = labels[indices]\n",
    "    n_subsamples = len(labels_subsampled)\n",
    "\n",
    "    feature_vectors = []\n",
    "    for i, idx in enumerate(indices):\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"  Processing sample {i+1}/{n_subsamples}\")\n",
    "        combined_features_list = [features_dict[key][idx].flatten() for key in feature_keys]\n",
    "        feature_vectors.append(np.concatenate(combined_features_list))\n",
    "\n",
    "    X = np.array(feature_vectors)\n",
    "    del feature_vectors\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Final feature matrix shape: {X.shape}\")\n",
    "    return X, labels_subsampled\n",
    "\n",
    "# =============================================================================\n",
    "# --- Visualization and Analysis Functions ---\n",
    "# =============================================================================\n",
    "\n",
    "def create_dimensionality_reduction_plots(features_dict, output_path, feature_mode, mode):\n",
    "    \"\"\"Creates PCA, t-SNE, and LDA plots with the specified layout.\"\"\"\n",
    "    print(\"\\n--- Creating Combined Feature Dimensionality Reduction Plots ---\")\n",
    "    try:\n",
    "        X, y = prepare_features_efficiently(features_dict, feature_mode, max_samples=1500)\n",
    "        print(\"Standardizing features...\")\n",
    "        X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "        class_names = [HEALTHY_CLASS, PARKINSON_CLASS]\n",
    "        colors = ['#2E86C1', '#E74C3C']\n",
    "\n",
    "        # ** REFINED: Changed subplot layout to 3x2 to match the example image **\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "        fig.suptitle('Dimensionality Reduction Analysis: HC vs PD', fontsize=16)\n",
    "\n",
    "        # --- PCA Explained Variance ---\n",
    "        print(\"Computing PCA Explained Variance...\")\n",
    "        n_components_full = min(50, X_scaled.shape[0] - 1, X_scaled.shape[1])\n",
    "        pca_full = PCA(n_components=n_components_full)\n",
    "        pca_full.fit(X_scaled)\n",
    "\n",
    "        # Plot 1: Explained Variance per Component\n",
    "        axes[0, 0].plot(range(1, n_components_full + 1), pca_full.explained_variance_ratio_, 'bo-', markersize=4)\n",
    "        axes[0, 0].set_xlabel('Principal Component')\n",
    "        axes[0, 0].set_ylabel('Explained Variance Ratio')\n",
    "        axes[0, 0].set_title('PCA: Explained Variance per Component')\n",
    "        axes[0, 0].grid(True, alpha=0.5, linestyle=':')\n",
    "\n",
    "        # Plot 2: Cumulative Explained Variance\n",
    "        cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "        axes[0, 1].plot(range(1, n_components_full + 1), cumulative_variance, 'ro-', markersize=4)\n",
    "        axes[0, 1].axhline(y=0.95, color='k', linestyle='--', alpha=0.8, label='95% Variance')\n",
    "        axes[0, 1].set_xlabel('Number of Components')\n",
    "        axes[0, 1].set_ylabel('Cumulative Explained Variance')\n",
    "        axes[0, 1].set_title('PCA: Cumulative Explained Variance')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.5, linestyle=':')\n",
    "\n",
    "        # --- 2D PCA plot ---\n",
    "        print(\"Computing 2D PCA Projection...\")\n",
    "        pca_2d = PCA(n_components=2)\n",
    "        X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "        for i, (name, color) in enumerate(zip(class_names, colors)):\n",
    "            mask = y == i\n",
    "            axes[1, 0].scatter(X_pca_2d[mask, 0], X_pca_2d[mask, 1], c=color, label=f'{name.replace(\"_\", \" \").title()} (n={np.sum(mask)})', alpha=0.7, s=20)\n",
    "        axes[1, 0].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.2%} variance)')\n",
    "        axes[1, 0].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.2%} variance)')\n",
    "        axes[1, 0].set_title('PCA: 2D Projection')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.5, linestyle=':')\n",
    "\n",
    "        # --- t-SNE Analysis ---\n",
    "        print(\"Computing t-SNE Projection...\")\n",
    "        perplexity = min(30, max(5, (X_scaled.shape[0] // 5) - 1))\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity, n_iter=1000, init='pca', learning_rate='auto')\n",
    "        X_tsne = tsne.fit_transform(X_scaled)\n",
    "        for i, (name, color) in enumerate(zip(class_names, colors)):\n",
    "            mask = y == i\n",
    "            axes[1, 1].scatter(X_tsne[mask, 0], X_tsne[mask, 1], c=color, label=f'{name.replace(\"_\", \" \").title()} (n={np.sum(mask)})', alpha=0.7, s=20)\n",
    "        axes[1, 1].set_xlabel('t-SNE Dimension 1')\n",
    "        axes[1, 1].set_ylabel('t-SNE Dimension 2')\n",
    "        axes[1, 1].set_title('t-SNE: 2D Projection')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.5, linestyle=':')\n",
    "\n",
    "        # --- LDA Analysis ---\n",
    "        print(\"Computing LDA Projection...\")\n",
    "        lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "        X_lda = lda.fit_transform(X_scaled, y)\n",
    "        for i, (name, color) in enumerate(zip(class_names, colors)):\n",
    "            mask = y == i\n",
    "            sns.kdeplot(X_lda[mask].ravel(), ax=axes[2, 0], color=color, label=f'{name.replace(\"_\", \" \").title()} (n={np.sum(mask)})', fill=True, alpha=0.5)\n",
    "        axes[2, 0].set_xlabel('LD1')\n",
    "        axes[2, 0].set_ylabel('Density')\n",
    "        axes[2, 0].set_title('LDA: 1D Projection')\n",
    "        axes[2, 0].legend()\n",
    "        axes[2, 0].grid(True, alpha=0.5, linestyle=':')\n",
    "\n",
    "        # Turn off the last unused subplot\n",
    "        axes[2, 1].axis('off')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        save_path = os.path.join(output_path, f\"dimensionality_reduction.png\")\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved combined feature plots to '{os.path.basename(save_path)}'\")\n",
    "        del X, y, X_scaled, X_pca_2d, X_tsne, X_lda\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate dimensionality reduction plots: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def create_individual_feature_analysis(features_dict, output_path, feature_mode, mode, max_samples=1000):\n",
    "    \"\"\"Creates separate dimensionality reduction plots for each individual feature type.\"\"\"\n",
    "    print(\"\\n--- Creating Individual Feature Analysis ---\")\n",
    "    try:\n",
    "        feature_keys = get_feature_keys(feature_mode)\n",
    "        titles = {'spectrogram': 'Spectrogram', 'mel_spectrogram': 'Mel Spectrogram', 'mfcc': 'MFCC', 'fsc': 'Spectral Centroid'}\n",
    "\n",
    "        num_features = len(feature_keys)\n",
    "        labels = features_dict['labels']\n",
    "        n_samples = len(labels)\n",
    "\n",
    "        indices = np.random.choice(n_samples, min(n_samples, max_samples), replace=False)\n",
    "        labels_sub = labels[indices]\n",
    "\n",
    "        class_names = [\"Healthy Control\", \"Parkinson Patient\"]\n",
    "        colors = ['#2E86C1', '#E74C3C']\n",
    "\n",
    "        # ** REFINED: Set layout to 3 rows (PCA, tSNE, LDA) and N columns for features **\n",
    "        fig, axes = plt.subplots(3, num_features, figsize=(7 * num_features, 15), squeeze=False)\n",
    "        fig.suptitle('Individual Feature Analysis: PCA, t-SNE, and LDA', fontsize=16)\n",
    "\n",
    "        for col, feat_key in enumerate(feature_keys):\n",
    "            feat_title = titles.get(feat_key, feat_key.replace(\"_\", \" \").title())\n",
    "            print(f\"  Processing {feat_title}...\")\n",
    "\n",
    "            X = np.array([features_dict[feat_key][i].flatten() for i in indices])\n",
    "            X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "            # --- PCA ---\n",
    "            pca = PCA(n_components=2)\n",
    "            X_pca = pca.fit_transform(X_scaled)\n",
    "            ax = axes[0, col]\n",
    "            for i, (name, color) in enumerate(zip(class_names, colors)):\n",
    "                mask = labels_sub == i\n",
    "                ax.scatter(X_pca[mask, 0], X_pca[mask, 1], c=color, label=f'{name} (n={np.sum(mask)})', alpha=0.7, s=15)\n",
    "            ax.set_title(f'PCA: {feat_title}')\n",
    "            ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
    "            ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.5, linestyle=':')\n",
    "\n",
    "            # --- t-SNE ---\n",
    "            perplexity = min(30, max(5, (X_scaled.shape[0] // 5) - 1))\n",
    "            tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity, n_iter=1000, init='pca', learning_rate='auto')\n",
    "            X_tsne = tsne.fit_transform(X_scaled)\n",
    "            ax = axes[1, col]\n",
    "            for i, (name, color) in enumerate(zip(class_names, colors)):\n",
    "                mask = labels_sub == i\n",
    "                ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1], c=color, label=f'{name} (n={np.sum(mask)})', alpha=0.7, s=15)\n",
    "            ax.set_title(f't-SNE: {feat_title}')\n",
    "            ax.set_xlabel('t-SNE Dim 1')\n",
    "            ax.set_ylabel('t-SNE Dim 2')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.5, linestyle=':')\n",
    "\n",
    "            # --- LDA ---\n",
    "            lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "            X_lda = lda.fit_transform(X_scaled, labels_sub)\n",
    "            ax = axes[2, col]\n",
    "            for i, (name, color) in enumerate(zip(class_names, colors)):\n",
    "                mask = labels_sub == i\n",
    "                sns.kdeplot(X_lda[mask].ravel(), ax=ax, color=color, label=f'{name} (n={np.sum(mask)})', fill=True, alpha=0.6)\n",
    "            ax.set_title(f'LDA: {feat_title}')\n",
    "            ax.set_xlabel('LD1')\n",
    "            ax.set_ylabel('Density')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.5, linestyle=':')\n",
    "\n",
    "            del X, X_scaled, X_pca, X_tsne, X_lda\n",
    "            gc.collect()\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        save_path = os.path.join(output_path, f\"individual_feature_analysis.png\")\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Saved individual feature analysis to '{os.path.basename(save_path)}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate individual feature analysis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def calculate_separability_metrics(features_dict, feature_mode, max_samples=2000):\n",
    "    \"\"\"\n",
    "    ** REFINED: Calculates the specific metrics requested: inter/intra-class distance,\n",
    "    Fisher's ratio, and silhouette score. **\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Calculating Separability Metrics ---\")\n",
    "    try:\n",
    "        X, y = prepare_features_efficiently(features_dict, feature_mode, max_samples=max_samples)\n",
    "        X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "        class_0_mask = (y == 0)\n",
    "        class_1_mask = (y == 1)\n",
    "\n",
    "        if not np.any(class_0_mask) or not np.any(class_1_mask):\n",
    "            print(\"Warning: One or both classes are missing in the sample. Cannot calculate metrics.\")\n",
    "            return None\n",
    "\n",
    "        # Calculate centroids for each class\n",
    "        centroid_0 = np.mean(X_scaled[class_0_mask], axis=0)\n",
    "        centroid_1 = np.mean(X_scaled[class_1_mask], axis=0)\n",
    "\n",
    "        # 1. Inter-class distance (distance between centroids)\n",
    "        inter_class_distance = np.linalg.norm(centroid_0 - centroid_1)\n",
    "\n",
    "        # 2. Intra-class distances (average distance of samples from their centroid)\n",
    "        intra_class_0 = np.mean([np.linalg.norm(x - centroid_0) for x in X_scaled[class_0_mask]])\n",
    "        intra_class_1 = np.mean([np.linalg.norm(x - centroid_1) for x in X_scaled[class_1_mask]])\n",
    "\n",
    "        # 3. Fisher's Discriminant Ratio\n",
    "        denominator = intra_class_0**2 + intra_class_1**2\n",
    "        fisher_ratio = inter_class_distance**2 / denominator if denominator > 0 else 0\n",
    "\n",
    "        # 4. Silhouette Score\n",
    "        silhouette = silhouette_score(X_scaled, y)\n",
    "\n",
    "        metrics = {\n",
    "            'inter_class_distance': inter_class_distance,\n",
    "            'intra_class_0': intra_class_0,\n",
    "            'intra_class_1': intra_class_1,\n",
    "            'fisher_ratio': fisher_ratio,\n",
    "            'silhouette_score': silhouette\n",
    "        }\n",
    "\n",
    "        print(f\"Separability Metrics for '{feature_mode}' mode:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"  - {key}: {value:.4f}\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate separability metrics: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# --- Main Execution ---\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the complete analysis workflow.\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"=== Feature Dimensionality Analysis ===\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Dataset: {DATASET}\")\n",
    "    print(f\"Mode: {MODE}\")\n",
    "    print(f\"Feature Mode: {FEATURE_MODE}\")\n",
    "    print(f\"Output Path: {RESULTS_OUTPUT_PATH}\\n\")\n",
    "\n",
    "    try:\n",
    "        # Load features from disk\n",
    "        features = load_features(FEATURES_FILE)\n",
    "\n",
    "        # Create combined and individual feature visualizations\n",
    "        create_dimensionality_reduction_plots(features, RESULTS_OUTPUT_PATH, FEATURE_MODE, MODE)\n",
    "        create_individual_feature_analysis(features, RESULTS_OUTPUT_PATH, FEATURE_MODE, MODE)\n",
    "\n",
    "        # Calculate and save separability metrics\n",
    "        metrics = calculate_separability_metrics(features, FEATURE_MODE)\n",
    "        if metrics:\n",
    "            metrics_df = pd.DataFrame([metrics])\n",
    "            metrics_file = os.path.join(RESULTS_OUTPUT_PATH, f\"separability_metrics.csv\")\n",
    "            metrics_df.to_csv(metrics_file, index=False)\n",
    "            print(f\"\\nMetrics saved to {os.path.basename(metrics_file)}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nERROR: {e}\")\n",
    "        print(\"Please ensure the feature file exists and the configuration is correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"=== Analysis Complete ===\")\n",
    "        print(f\"All generated files are saved in: {RESULTS_OUTPUT_PATH}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "fa6f7b14780ba240",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
