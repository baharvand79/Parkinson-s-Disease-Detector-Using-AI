{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b6c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import shap\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Dropout, Flatten,\n",
    "                                     Dense, LSTM, MultiHeadAttention, Concatenate, Reshape)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis\n",
    "from sklearn.metrics import (classification_report, accuracy_score, confusion_matrix,\n",
    "                             roc_curve, auc, precision_score, recall_score, f1_score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# =============================================================================\n",
    "# --- üöÄ CONFIGURATION ---\n",
    "# =============================================================================\n",
    "# Define the datasets and their specific modes for TRAINING.\n",
    "# You can list one or more datasets here.\n",
    "TRAIN_CONFIG = [\n",
    "    {\"dataset\": \"Italian\", \"mode\": \"A\", \"feature_mode\": \"ALL\"},\n",
    "\n",
    "]\n",
    "\n",
    "# Define the single dataset and its mode for TESTING.\n",
    "# - Set to a dictionary to test on a specific unseen dataset.\n",
    "# - Set to None to use a validation split from the training data for the final evaluation.\n",
    "# TEST_CONFIG = {\"dataset\": \"mPower\", \"mode\": \"ALL_VALIDS\", \"feature_mode\": \"ALL\"}\n",
    "# TEST_CONFIG = None \n",
    "TEST_CONFIG = {\"dataset\": \"Neurovoz\", \"mode\": \"A\", \"feature_mode\": \"ALL\"},\n",
    "\n",
    "# --- Path and Run ID Configuration ---\n",
    "# A unique name for this training run is generated automatically from the TRAIN_CONFIG\n",
    "train_datasets_str = '_'.join(sorted([d['dataset'] for d in TRAIN_CONFIG]))\n",
    "RUN_ID = f\"trained_on_{train_datasets_str}\"\n",
    "RESULTS_PATH = os.path.join(os.getcwd(), \"runs\", RUN_ID)\n",
    "PLOTS_PATH = os.path.join(RESULTS_PATH, \"plots\")\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "os.makedirs(PLOTS_PATH, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT_RATE = 0.5\n",
    "L2_STRENGTH = 0.01\n",
    "\n",
    "# =============================================================================\n",
    "# --- ‚è±Ô∏è RUNTIME TRACKING UTILITIES ---\n",
    "# =============================================================================\n",
    "class RuntimeTracker:\n",
    "    def __init__(self, results_path):\n",
    "        self.results_path = results_path\n",
    "        self.runtime_data = []\n",
    "        \n",
    "    def track_runtime(self, model_name, phase, start_time, end_time):\n",
    "        \"\"\"Track runtime for a specific model and phase\"\"\"\n",
    "        runtime_seconds = end_time - start_time\n",
    "        runtime_minutes = runtime_seconds / 60\n",
    "        \n",
    "        runtime_info = {\n",
    "            'Model': model_name,\n",
    "            'Phase': phase,\n",
    "            'Runtime_Seconds': runtime_seconds,\n",
    "            'Runtime_Minutes': runtime_minutes,\n",
    "            'Timestamp': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))\n",
    "        }\n",
    "        \n",
    "        self.runtime_data.append(runtime_info)\n",
    "        print(f\"‚è±Ô∏è {model_name} - {phase}: {runtime_minutes:.2f} minutes ({runtime_seconds:.2f} seconds)\")\n",
    "        \n",
    "    def save_runtime_summary(self):\n",
    "        \"\"\"Save all runtime data to CSV\"\"\"\n",
    "        runtime_df = pd.DataFrame(self.runtime_data)\n",
    "        runtime_path = os.path.join(self.results_path, \"model_runtimes.csv\")\n",
    "        runtime_df.to_csv(runtime_path, index=False)\n",
    "        \n",
    "        # Create summary statistics\n",
    "        summary_stats = runtime_df.groupby('Model').agg({\n",
    "            'Runtime_Minutes': ['sum', 'mean'],\n",
    "            'Runtime_Seconds': ['sum', 'mean']\n",
    "        }).round(3)\n",
    "        \n",
    "        summary_stats.columns = ['Total_Minutes', 'Avg_Minutes', 'Total_Seconds', 'Avg_Seconds']\n",
    "        summary_stats.reset_index(inplace=True)\n",
    "        \n",
    "        summary_path = os.path.join(self.results_path, \"runtime_summary.csv\")\n",
    "        summary_stats.to_csv(summary_path, index=False)\n",
    "        \n",
    "        print(f\"\\nüìä Runtime Summary:\")\n",
    "        print(summary_stats.to_string(index=False))\n",
    "        print(f\"‚úÖ Runtime data saved to: {runtime_path}\")\n",
    "        print(f\"‚úÖ Runtime summary saved to: {summary_path}\")\n",
    "\n",
    "# Initialize runtime tracker\n",
    "runtime_tracker = RuntimeTracker(RESULTS_PATH)\n",
    "\n",
    "# =============================================================================\n",
    "# --- Your Custom CNN Models ---\n",
    "# =============================================================================\n",
    "@register_keras_serializable()\n",
    "class ParkinsonDetectorModel(Model):\n",
    "    \"\"\"Your end-to-end CNN model for original features.\"\"\"\n",
    "    def __init__(self, input_shape, **kwargs):\n",
    "        super(ParkinsonDetectorModel, self).__init__(**kwargs)\n",
    "        self.input_shape_config = input_shape\n",
    "        self.reshape_in = Reshape((input_shape[0], input_shape[1], 1))\n",
    "        self.conv1a = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv1b = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.pool1 = MaxPooling2D(5)\n",
    "        self.drop1 = Dropout(DROPOUT_RATE)\n",
    "        self.conv2a = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv2b = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same', name=\"last_conv_layer\")\n",
    "        self.pool2 = MaxPooling2D(5)\n",
    "        self.drop2 = Dropout(DROPOUT_RATE)\n",
    "        self.flatten_cnn = Flatten()\n",
    "        self.attention = MultiHeadAttention(num_heads=2, key_dim=64)\n",
    "        self.flatten_att = Flatten()\n",
    "        self.lstm1 = LSTM(128, return_sequences=True)\n",
    "        self.lstm2 = LSTM(128, return_sequences=False)\n",
    "        self.drop_lstm = Dropout(DROPOUT_RATE)\n",
    "        self.concat = Concatenate()\n",
    "        self.dense_bottleneck = Dense(128, activation='relu', name='bottleneck_features')\n",
    "        self.dense_output = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, extract_features=False, grad_cam=False):\n",
    "        x = self.reshape_in(inputs)\n",
    "        x = self.conv1a(x); x = self.conv1b(x); x = self.pool1(x); x = self.drop1(x, training=False)\n",
    "        x = self.conv2a(x)\n",
    "        last_conv_output = self.conv2b(x)\n",
    "        x = self.pool2(last_conv_output); x = self.drop2(x, training=False)\n",
    "        cnn_flat = self.flatten_cnn(x)\n",
    "        shape = tf.shape(x); sequence = tf.reshape(x, [-1, shape[1] * shape[2], shape[3]])\n",
    "        att_out = self.attention(query=sequence, key=sequence, value=sequence); att_flat = self.flatten_att(att_out)\n",
    "        lstm_seq = self.lstm1(sequence); lstm_out = self.lstm2(lstm_seq); lstm_out = self.drop_lstm(lstm_out, training=False)\n",
    "        concatenated = self.concat([cnn_flat, att_flat, lstm_out]); bottleneck = self.dense_bottleneck(concatenated)\n",
    "        final_output = self.dense_output(bottleneck)\n",
    "        if grad_cam: return final_output, last_conv_output\n",
    "        if extract_features: return bottleneck\n",
    "        return final_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ParkinsonDetectorModel, self).get_config()\n",
    "        config.update({\"input_shape\": self.input_shape_config}); return config\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "@register_keras_serializable()\n",
    "class ParkinsonDetectorModelNCA(Model):\n",
    "    \"\"\"A modified version of your model for the small NCA input.\"\"\"\n",
    "    def __init__(self, input_shape, **kwargs):\n",
    "        super(ParkinsonDetectorModelNCA, self).__init__(**kwargs)\n",
    "        self.input_shape_config = input_shape\n",
    "        self.reshape_in = Reshape((input_shape[0], input_shape[1], 1))\n",
    "        self.conv1a = Conv2D(64, 3, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv1b = Conv2D(64, 3, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.pool1 = MaxPooling2D(2)\n",
    "        self.drop1 = Dropout(DROPOUT_RATE)\n",
    "        self.conv2a = Conv2D(64, 3, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv2b = Conv2D(64, 3, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same', name=\"last_conv_layer\")\n",
    "        self.pool2 = MaxPooling2D(2)\n",
    "        self.drop2 = Dropout(DROPOUT_RATE)\n",
    "        self.flatten_cnn = Flatten()\n",
    "        self.attention = MultiHeadAttention(num_heads=2, key_dim=64)\n",
    "        self.flatten_att = Flatten()\n",
    "        self.lstm1 = LSTM(128, return_sequences=True)\n",
    "        self.lstm2 = LSTM(128, return_sequences=False)\n",
    "        self.drop_lstm = Dropout(DROPOUT_RATE)\n",
    "        self.concat = Concatenate()\n",
    "        self.dense_bottleneck = Dense(128, activation='relu', name='bottleneck_features')\n",
    "        self.dense_output = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, extract_features=False, grad_cam=False):\n",
    "        x = self.reshape_in(inputs)\n",
    "        x = self.conv1a(x); x = self.conv1b(x); x = self.pool1(x); x = self.drop1(x, training=False)\n",
    "        x = self.conv2a(x)\n",
    "        last_conv_output = self.conv2b(x)\n",
    "        x = self.pool2(last_conv_output); x = self.drop2(x, training=False)\n",
    "        cnn_flat = self.flatten_cnn(x)\n",
    "        shape = tf.shape(x); sequence = tf.reshape(x, [-1, shape[1] * shape[2], shape[3]])\n",
    "        att_out = self.attention(query=sequence, key=sequence, value=sequence); att_flat = self.flatten_att(att_out)\n",
    "        lstm_seq = self.lstm1(sequence); lstm_out = self.lstm2(lstm_seq); lstm_out = self.drop_lstm(lstm_out, training=False)\n",
    "        concatenated = self.concat([cnn_flat, att_flat, lstm_out]); bottleneck = self.dense_bottleneck(concatenated)\n",
    "        final_output = self.dense_output(bottleneck)\n",
    "        if grad_cam: return final_output, last_conv_output\n",
    "        if extract_features: return bottleneck\n",
    "        return final_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ParkinsonDetectorModelNCA, self).get_config()\n",
    "        config.update({\"input_shape\": self.input_shape_config}); return config\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "# =============================================================================\n",
    "# --- Data Loading & Helper Functions ---\n",
    "# =============================================================================\n",
    "def load_single_dataset(config):\n",
    "    dataset_name = config['dataset']\n",
    "    mode = config['mode']\n",
    "    feature_mode = config['feature_mode']\n",
    "    path = os.path.join(dataset_name, \"data\", f\"features_{mode}_{feature_mode}.npz\")\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"WARNING: Data file not found at {path}. Skipping.\"); return None\n",
    "    print(f\"--- Loading data from {path} ---\")\n",
    "    with np.load(path) as data:\n",
    "        mel = data['mel_spectrogram']; mfccs = data['mfcc']\n",
    "        X = np.concatenate((mel, mfccs), axis=1)\n",
    "        labels = data['labels']\n",
    "        mel_bins = mel.shape[1]\n",
    "    print(f\"Loaded {dataset_name} successfully. Shape: {X.shape}\"); return X, labels, mel_bins\n",
    "\n",
    "def load_and_combine_data(configs):\n",
    "    all_X, all_y, mel_bins = [], [], -1\n",
    "    for config in configs:\n",
    "        data = load_single_dataset(config)\n",
    "        if data: \n",
    "            all_X.append(data[0]); all_y.append(data[1])\n",
    "            if mel_bins == -1: mel_bins = data[2]\n",
    "    if not all_X: raise ValueError(\"No data could be loaded for this configuration.\")\n",
    "    combined_X = np.concatenate(all_X, axis=0); combined_y = np.concatenate(all_y, axis=0)\n",
    "    print(f\"\\n--- All data combined. Final shape: X={combined_X.shape}, y={combined_y.shape} ---\")\n",
    "    return combined_X, combined_y, mel_bins\n",
    "\n",
    "# =============================================================================\n",
    "# --- Plotting and Evaluation Functions ---\n",
    "# =============================================================================\n",
    "def plot_and_save_history(history, model_name, path):\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.to_csv(os.path.join(path, f\"{model_name}_history.csv\"))\n",
    "    plt.style.use('seaborn-v0_8-whitegrid'); fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    axes[0].plot(history_df['loss'], label='Train Loss'); axes[0].plot(history_df['val_loss'], label='Val Loss', linestyle='--')\n",
    "    axes[0].set_title(f'{model_name} - Model Loss'); axes[0].set_xlabel('Epoch'); axes[0].legend()\n",
    "    axes[1].plot(history_df['accuracy'], label='Train Acc'); axes[1].plot(history_df['val_accuracy'], label='Val Acc', linestyle='--')\n",
    "    axes[1].set_title(f'{model_name} - Model Accuracy'); axes[1].set_xlabel('Epoch'); axes[1].legend()\n",
    "    auc_keys = [k for k in history_df.columns if 'auc' in k and 'val' not in k]\n",
    "    if auc_keys:\n",
    "        auc_key = auc_keys[-1]; val_auc_key = 'val_' + auc_key\n",
    "        axes[2].plot(history_df[auc_key], label='Train AUC'); axes[2].plot(history_df[val_auc_key], label='Val AUC', linestyle='--')\n",
    "        axes[2].set_title(f'{model_name} - Model AUC'); axes[2].set_xlabel('Epoch'); axes[2].legend()\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(path, f\"{model_name}_history.png\"), dpi=300); plt.close()\n",
    "    print(f\"‚úÖ Saved training history and plot for {model_name}.\")\n",
    "\n",
    "def calculate_and_save_metrics(y_true, y_pred, y_pred_proba, model_name, results_path, plots_path):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.shape == (2,2) else (0,0,0,0)\n",
    "    cm_df = pd.DataFrame(cm, index=['True Healthy', 'True Parkinson'], columns=['Pred Healthy', 'Pred Parkinson'])\n",
    "    cm_df.to_csv(os.path.join(plots_path, f\"{model_name}_confusion_matrix.csv\"))\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc_score = auc(fpr, tpr)\n",
    "    \n",
    "    metrics_data = {\n",
    "        'Model': model_name, 'Accuracy': accuracy_score(y_true, y_pred), 'AUC': roc_auc_score,\n",
    "        'F1-Score': f1_score(y_true, y_pred, zero_division=0), 'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'Sensitivity (Recall)': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(results_path, \"evaluation_metrics.csv\")\n",
    "    summary_df = pd.read_csv(summary_path) if os.path.exists(summary_path) else pd.DataFrame()\n",
    "    summary_df = pd.concat([summary_df, pd.DataFrame([metrics_data])], ignore_index=True)\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    \n",
    "    plot_and_save_confusion_matrix(cm, model_name, plots_path)\n",
    "    plot_and_save_roc_curve(y_true, y_pred_proba, model_name, plots_path)\n",
    "    print(f\"‚úÖ Saved all metrics and plots for {model_name}.\")\n",
    "    return metrics_data['Accuracy']\n",
    "\n",
    "def plot_and_save_confusion_matrix(cm, model_name, path):\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] if cm.sum(axis=1)[:, np.newaxis].all() > 0 else cm\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6)); class_names = ['Healthy', 'Parkinson']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], xticklabels=class_names, yticklabels=class_names)\n",
    "    axes[0].set_title(f'{model_name}\\nConfusion Matrix (Counts)'); axes[0].set_xlabel('Predicted'); axes[0].set_ylabel('True')\n",
    "    sns.heatmap(cm_percent, annot=True, fmt='.2%', cmap='Blues', ax=axes[1], xticklabels=class_names, yticklabels=class_names)\n",
    "    axes[1].set_title(f'{model_name}\\nConfusion Matrix (Percentages)'); axes[1].set_xlabel('Predicted'); axes[1].set_ylabel('True')\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(path, f\"{model_name}_confusion_matrix.png\"), dpi=300); plt.close()\n",
    "\n",
    "def plot_and_save_roc_curve(y_true, y_pred_proba, model_name, path):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba); roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(8, 6)); plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:0.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--'); plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{model_name} - ROC Curve'); plt.legend(loc=\"lower right\"); plt.grid(True)\n",
    "    plt.savefig(os.path.join(path, f\"{model_name}_roc_curve.png\"), dpi=300); plt.close()\n",
    "\n",
    "# =============================================================================\n",
    "# --- Explainability Functions ---\n",
    "# =============================================================================\n",
    "def run_full_shap_analysis(model, X_train, X_test, y_test, output_path, mel_bins, num_samples=50):\n",
    "    print(\"\\n--- Running SHAP Analysis ---\"); os.makedirs(output_path, exist_ok=True)\n",
    "    inputs = tf.keras.Input(shape=model.input_shape_config); outputs = model(inputs)\n",
    "    functional_model = Model(inputs, outputs)\n",
    "    idx = np.random.choice(len(X_test), min(num_samples, len(X_test)), replace=False)\n",
    "    test_samples, y_true_samples = X_test[idx], y_test[idx]\n",
    "    explainer = shap.GradientExplainer(functional_model, X_train[:50])\n",
    "    shap_values = explainer.shap_values(test_samples)\n",
    "    if isinstance(shap_values, list): shap_values = shap_values[0]\n",
    "    mean_abs_shap = np.mean(np.abs(shap_values.reshape(shap_values.shape[0], -1)), axis=0)\n",
    "    top_idx = np.argsort(mean_abs_shap)[::-1][:20]\n",
    "    coords = [np.unravel_index(i, (shap_values.shape[1], shap_values.shape[2])) for i in top_idx]\n",
    "    labels = [f\"Mel F{f} @ T{t}\" if f < mel_bins else f\"MFCC F{f-mel_bins} @ T{t}\" for t, f in coords]\n",
    "    plt.figure(figsize=(12, 6)); plt.bar(range(20), mean_abs_shap[top_idx])\n",
    "    plt.xticks(range(20), labels, rotation=45, ha=\"right\"); plt.title(\"Top-20 Global SHAP Features\")\n",
    "    plt.ylabel(\"Mean |SHAP value|\"); plt.tight_layout(); plt.savefig(os.path.join(output_path, \"shap_global_bar.png\"), dpi=300); plt.close()\n",
    "    print(\"-> Saved SHAP global bar plot.\")\n",
    "    hc_mask, pd_mask = (y_true_samples == 0), (y_true_samples == 1)\n",
    "    if np.any(hc_mask):\n",
    "        hc_mean = shap_values[hc_mask].mean(axis=0).squeeze()\n",
    "        plt.figure(); plt.imshow(hc_mean, cmap=\"bwr\", aspect=\"auto\"); cbar = plt.colorbar(); cbar.set_label(\"Mean SHAP Value\")\n",
    "        plt.title(\"Average SHAP - Healthy\"); plt.savefig(os.path.join(output_path, \"shap_summary_healthy.png\"), dpi=300); plt.close()\n",
    "    if np.any(pd_mask):\n",
    "        pd_mean = shap_values[pd_mask].mean(axis=0).squeeze()\n",
    "        plt.figure(); plt.imshow(pd_mean, cmap=\"bwr\", aspect=\"auto\"); cbar = plt.colorbar(); cbar.set_label(\"Mean SHAP Value\")\n",
    "        plt.title(\"Average SHAP - Parkinson's\"); plt.savefig(os.path.join(output_path, \"shap_summary_parkinson.png\"), dpi=300); plt.close()\n",
    "    print(\"-> Saved SHAP class heatmaps.\"); print(\"--- SHAP Analysis Complete ---\")\n",
    "\n",
    "def run_gradcam_analysis(model, X_test, y_test, output_path, num_samples=30):\n",
    "    print(\"\\n--- Running Grad-CAM Analysis ---\"); os.makedirs(output_path, exist_ok=True)\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int).flatten()\n",
    "    tp_idx = np.where((y_test == 1) & (y_pred == 1))[0]; tn_idx = np.where((y_test == 0) & (y_pred == 0))[0]\n",
    "    def get_avg_heatmap(indices):\n",
    "        heatmaps = []\n",
    "        if len(indices) == 0: return np.zeros(X_test.shape[1:3])\n",
    "        for i in tqdm(indices, desc=\"Grad-CAM Progress\", leave=False):\n",
    "            img_array = X_test[i:i+1]\n",
    "            with tf.GradientTape() as tape:\n",
    "                final_preds, last_conv_output = model(img_array, grad_cam=True)\n",
    "                tape.watch(last_conv_output); loss = final_preds[0]\n",
    "            grads = tape.gradient(loss, last_conv_output)\n",
    "            pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "            heatmap = last_conv_output[0] @ pooled_grads[..., tf.newaxis]; heatmap = tf.squeeze(heatmap)\n",
    "            heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + 1e-10)\n",
    "            heatmaps.append(heatmap.numpy())\n",
    "        return np.mean(heatmaps, axis=0)\n",
    "    avg_tp_heatmap = get_avg_heatmap(np.random.choice(tp_idx, min(num_samples, len(tp_idx)), replace=False))\n",
    "    avg_tn_heatmap = get_avg_heatmap(np.random.choice(tn_idx, min(num_samples, len(tn_idx)), replace=False))\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    im1 = axes[0].imshow(avg_tp_heatmap, cmap='jet', aspect='auto'); axes[0].set_title(f'Avg Grad-CAM for Parkinson\\'s (TP)'); cbar1 = fig.colorbar(im1, ax=axes[0]); cbar1.set_label(\"Activation Intensity\")\n",
    "    im2 = axes[1].imshow(avg_tn_heatmap, cmap='jet', aspect='auto'); axes[1].set_title(f'Avg Grad-CAM for Healthy (TN)'); cbar2 = fig.colorbar(im2, ax=axes[1]); cbar2.set_label(\"Activation Intensity\")\n",
    "    plt.suptitle(\"Average Model Attention by Class\"); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, \"gradcam_average_comparison.png\"), dpi=300); plt.close()\n",
    "    print(\"-> Saved average Grad-CAM comparison.\"); print(\"--- Grad-CAM Analysis Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865fa138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üöÄ STARTING TRAINING PHASE | SAVING TO: trained_on_Italian\n",
      "================================================================================\n",
      "--- Loading data from Italian\\data\\features_A_ALL.npz ---\n",
      "Loaded Italian successfully. Shape: (440, 60, 94)\n",
      "\n",
      "--- All data combined. Final shape: X=(440, 60, 94), y=(440,) ---\n",
      "\n",
      "Training data prepared. Train: (352, 60, 94), Validation: (88, 60, 94)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRAINING MODEL 1: k-NN (Baseline)\n",
      "--------------------------------------------------------------------------------\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "‚è±Ô∏è Model_1_kNN - Training: 0.22 minutes (13.00 seconds)\n",
      "‚úÖ Best k-NN pipeline trained and saved.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRAINING MODEL 2: CNN (End-to-End)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434ms/step - accuracy: 0.5354 - auc: 0.5633 - loss: 4.0569\n",
      "Epoch 1: val_auc improved from None to 0.83988, saving model to runs\\trained_on_Italian\\model_2_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 627ms/step - accuracy: 0.6051 - auc: 0.6664 - loss: 3.2626 - val_accuracy: 0.6705 - val_auc: 0.8399 - val_loss: 2.5471\n",
      "Epoch 2/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424ms/step - accuracy: 0.7197 - auc: 0.8177 - loss: 2.4014\n",
      "Epoch 2: val_auc improved from 0.83988 to 0.91839, saving model to runs\\trained_on_Italian\\model_2_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 496ms/step - accuracy: 0.7670 - auc: 0.8588 - loss: 2.2939 - val_accuracy: 0.8523 - val_auc: 0.9184 - val_loss: 2.1940\n",
      "Epoch 3/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425ms/step - accuracy: 0.8584 - auc: 0.9448 - loss: 2.0779\n",
      "Epoch 3: val_auc improved from 0.91839 to 0.94706, saving model to runs\\trained_on_Italian\\model_2_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 489ms/step - accuracy: 0.8494 - auc: 0.9272 - loss: 2.0606 - val_accuracy: 0.8977 - val_auc: 0.9471 - val_loss: 1.9969\n",
      "Epoch 4/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433ms/step - accuracy: 0.8687 - auc: 0.9451 - loss: 1.9524\n",
      "Epoch 4: val_auc improved from 0.94706 to 0.94938, saving model to runs\\trained_on_Italian\\model_2_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 499ms/step - accuracy: 0.8551 - auc: 0.9365 - loss: 1.9370 - val_accuracy: 0.8864 - val_auc: 0.9494 - val_loss: 1.8559\n",
      "Epoch 5/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 0.9159 - auc: 0.9819 - loss: 1.7446\n",
      "Epoch 5: val_auc improved from 0.94938 to 0.98915, saving model to runs\\trained_on_Italian\\model_2_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 483ms/step - accuracy: 0.9290 - auc: 0.9866 - loss: 1.6984 - val_accuracy: 0.9318 - val_auc: 0.9892 - val_loss: 1.6529\n",
      "Epoch 6/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9372 - auc: 0.9858 - loss: 1.6095\n",
      "Epoch 6: val_auc improved from 0.98915 to 0.99613, saving model to runs\\trained_on_Italian\\model_2_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 480ms/step - accuracy: 0.9119 - auc: 0.9621 - loss: 1.6915 - val_accuracy: 0.9432 - val_auc: 0.9961 - val_loss: 1.5092\n",
      "Epoch 7/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step - accuracy: 0.9177 - auc: 0.9816 - loss: 1.5668\n",
      "Epoch 7: val_auc did not improve from 0.99613\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 487ms/step - accuracy: 0.9233 - auc: 0.9762 - loss: 1.5430 - val_accuracy: 0.9205 - val_auc: 0.9915 - val_loss: 1.4816\n",
      "Epoch 8/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 427ms/step - accuracy: 0.9520 - auc: 0.9962 - loss: 1.4187\n",
      "Epoch 8: val_auc improved from 0.99613 to 0.99897, saving model to runs\\trained_on_Italian\\model_2_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 497ms/step - accuracy: 0.9574 - auc: 0.9933 - loss: 1.3975 - val_accuracy: 0.9659 - val_auc: 0.9990 - val_loss: 1.3255\n",
      "Epoch 9/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448ms/step - accuracy: 0.9788 - auc: 0.9995 - loss: 1.2858\n",
      "Epoch 9: val_auc did not improve from 0.99897\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 490ms/step - accuracy: 0.9830 - auc: 0.9996 - loss: 1.2646 - val_accuracy: 0.9545 - val_auc: 0.9974 - val_loss: 1.2581\n",
      "Epoch 10/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412ms/step - accuracy: 0.9887 - auc: 0.9998 - loss: 1.1890\n",
      "Epoch 10: val_auc improved from 0.99897 to 1.00000, saving model to runs\\trained_on_Italian\\model_2_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 478ms/step - accuracy: 0.9915 - auc: 0.9998 - loss: 1.1705 - val_accuracy: 0.9886 - val_auc: 1.0000 - val_loss: 1.1531\n",
      "Epoch 11/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 427ms/step - accuracy: 0.9843 - auc: 0.9995 - loss: 1.1272\n",
      "Epoch 11: val_auc improved from 1.00000 to 1.00000, saving model to runs\\trained_on_Italian\\model_2_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 495ms/step - accuracy: 0.9716 - auc: 0.9985 - loss: 1.1331 - val_accuracy: 0.9886 - val_auc: 1.0000 - val_loss: 1.0873\n",
      "Epoch 12/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 449ms/step - accuracy: 0.9826 - auc: 0.9993 - loss: 1.0838\n",
      "Epoch 12: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 492ms/step - accuracy: 0.9886 - auc: 0.9995 - loss: 1.0606 - val_accuracy: 0.9659 - val_auc: 1.0000 - val_loss: 1.0820\n",
      "Epoch 13/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440ms/step - accuracy: 0.9888 - auc: 0.9999 - loss: 1.0263\n",
      "Epoch 13: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 492ms/step - accuracy: 0.9943 - auc: 0.9998 - loss: 0.9923 - val_accuracy: 0.9886 - val_auc: 1.0000 - val_loss: 0.9683\n",
      "Epoch 14/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431ms/step - accuracy: 0.9945 - auc: 1.0000 - loss: 0.9443\n",
      "Epoch 14: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 474ms/step - accuracy: 0.9943 - auc: 0.9999 - loss: 0.9329 - val_accuracy: 0.9886 - val_auc: 1.0000 - val_loss: 0.9127\n",
      "Epoch 15/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418ms/step - accuracy: 0.9988 - auc: 1.0000 - loss: 0.8877\n",
      "Epoch 15: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 469ms/step - accuracy: 0.9972 - auc: 1.0000 - loss: 0.8791 - val_accuracy: 0.9545 - val_auc: 1.0000 - val_loss: 0.9094\n",
      "Epoch 16/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422ms/step - accuracy: 0.9893 - auc: 0.9999 - loss: 0.8588\n",
      "Epoch 16: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 473ms/step - accuracy: 0.9943 - auc: 0.9998 - loss: 0.8382 - val_accuracy: 0.9659 - val_auc: 0.9995 - val_loss: 0.8587\n",
      "Epoch 17/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434ms/step - accuracy: 0.9940 - auc: 1.0000 - loss: 0.8004\n",
      "Epoch 17: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 478ms/step - accuracy: 0.9943 - auc: 1.0000 - loss: 0.7878 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.7744\n",
      "Epoch 18/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440ms/step - accuracy: 0.9979 - auc: 1.0000 - loss: 0.7509\n",
      "Epoch 18: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 492ms/step - accuracy: 0.9972 - auc: 1.0000 - loss: 0.7416 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.7271\n",
      "Epoch 19/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 430ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.7093\n",
      "Epoch 19: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 482ms/step - accuracy: 0.9972 - auc: 1.0000 - loss: 0.7007 - val_accuracy: 0.9886 - val_auc: 1.0000 - val_loss: 0.6881\n",
      "Epoch 20/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 417ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.6698\n",
      "Epoch 20: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 468ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.6625 - val_accuracy: 0.9886 - val_auc: 1.0000 - val_loss: 0.6517\n",
      "Epoch 21/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411ms/step - accuracy: 0.9973 - auc: 1.0000 - loss: 0.6382\n",
      "Epoch 21: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 461ms/step - accuracy: 0.9972 - auc: 1.0000 - loss: 0.6323 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.6319\n",
      "Epoch 22/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.6044\n",
      "Epoch 22: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 493ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.5969 - val_accuracy: 0.9886 - val_auc: 0.9995 - val_loss: 0.6183\n",
      "Epoch 23/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419ms/step - accuracy: 0.9875 - auc: 0.9996 - loss: 0.6030\n",
      "Epoch 23: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 470ms/step - accuracy: 0.9915 - auc: 0.9998 - loss: 0.5830 - val_accuracy: 0.9886 - val_auc: 0.9995 - val_loss: 0.5861\n",
      "Epoch 24/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452ms/step - accuracy: 0.9908 - auc: 0.9992 - loss: 0.5704\n",
      "Epoch 24: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 496ms/step - accuracy: 0.9659 - auc: 0.9930 - loss: 0.6379 - val_accuracy: 0.9545 - val_auc: 0.9985 - val_loss: 0.6581\n",
      "Epoch 25/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - accuracy: 0.9625 - auc: 0.9940 - loss: 0.6267\n",
      "Epoch 25: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 461ms/step - accuracy: 0.9602 - auc: 0.9899 - loss: 0.6530 - val_accuracy: 0.9432 - val_auc: 0.9822 - val_loss: 0.6672\n",
      "Epoch 26/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 0.9050 - auc: 0.9652 - loss: 1.1825\n",
      "Epoch 26: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 472ms/step - accuracy: 0.8750 - auc: 0.9523 - loss: 1.1143 - val_accuracy: 0.9091 - val_auc: 0.9716 - val_loss: 0.8112\n",
      "Epoch 27/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425ms/step - accuracy: 0.9011 - auc: 0.9738 - loss: 0.7663\n",
      "Epoch 27: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 468ms/step - accuracy: 0.9205 - auc: 0.9792 - loss: 0.7422 - val_accuracy: 0.9205 - val_auc: 0.9708 - val_loss: 0.8674\n",
      "Epoch 28/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - accuracy: 0.9578 - auc: 0.9938 - loss: 0.6721\n",
      "Epoch 28: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 467ms/step - accuracy: 0.9489 - auc: 0.9902 - loss: 0.6944 - val_accuracy: 0.9773 - val_auc: 0.9974 - val_loss: 0.6499\n",
      "Epoch 29/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - accuracy: 0.9837 - auc: 0.9956 - loss: 0.6287\n",
      "Epoch 29: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 487ms/step - accuracy: 0.9773 - auc: 0.9948 - loss: 0.6319 - val_accuracy: 0.9773 - val_auc: 0.9979 - val_loss: 0.6494\n",
      "Epoch 30/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 417ms/step - accuracy: 0.9860 - auc: 0.9991 - loss: 0.5927\n",
      "Epoch 30: val_auc did not improve from 1.00000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 468ms/step - accuracy: 0.9801 - auc: 0.9983 - loss: 0.5987 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.5580\n",
      "‚úÖ Saved training history and plot for model_2_cnn.\n",
      "‚è±Ô∏è Model_2_CNN - Training: 2.90 minutes (174.27 seconds)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRAINING MODEL 3: CNN Feature Extractor + k-NN\n",
      "--------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:From c:\\Users\\bahar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 166ms/step\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "‚è±Ô∏è Model_3_CNN_kNN - Training: 0.18 minutes (10.66 seconds)\n",
      "‚úÖ Best CNN+k-NN pipeline trained and saved.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRAINING MODEL 4: NCA pre-processing + CNN\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/30\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5155 - auc: 0.5463 - loss: 2.6431\n",
      "Epoch 1: val_auc improved from None to 0.86080, saving model to runs\\trained_on_Italian\\model_4_nca_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 176ms/step - accuracy: 0.5312 - auc: 0.5751 - loss: 2.5254 - val_accuracy: 0.7045 - val_auc: 0.8608 - val_loss: 2.2887\n",
      "Epoch 2/30\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6940 - auc: 0.7937 - loss: 2.2152\n",
      "Epoch 2: val_auc improved from 0.86080 to 0.93647, saving model to runs\\trained_on_Italian\\model_4_nca_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.7557 - auc: 0.8327 - loss: 2.1101 - val_accuracy: 0.8182 - val_auc: 0.9365 - val_loss: 1.9050\n",
      "Epoch 3/30\n",
      "\u001b[1m 8/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9291 - auc: 0.9715 - loss: 1.7414\n",
      "Epoch 3: val_auc improved from 0.93647 to 0.95300, saving model to runs\\trained_on_Italian\\model_4_nca_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9318 - auc: 0.9766 - loss: 1.6113 - val_accuracy: 0.8864 - val_auc: 0.9530 - val_loss: 1.5082\n",
      "Epoch 4/30\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9679 - auc: 0.9898 - loss: 1.3357\n",
      "Epoch 4: val_auc improved from 0.95300 to 0.97882, saving model to runs\\trained_on_Italian\\model_4_nca_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9574 - auc: 0.9829 - loss: 1.3051 - val_accuracy: 0.9318 - val_auc: 0.9788 - val_loss: 1.2881\n",
      "Epoch 5/30\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9820 - auc: 0.9997 - loss: 1.1065\n",
      "Epoch 5: val_auc improved from 0.97882 to 0.98657, saving model to runs\\trained_on_Italian\\model_4_nca_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9858 - auc: 0.9995 - loss: 1.0696 - val_accuracy: 0.9205 - val_auc: 0.9866 - val_loss: 1.1210\n",
      "Epoch 6/30\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9793 - auc: 1.0000 - loss: 0.9695\n",
      "Epoch 6: val_auc improved from 0.98657 to 0.99096, saving model to runs\\trained_on_Italian\\model_4_nca_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9858 - auc: 0.9998 - loss: 0.9316 - val_accuracy: 0.9545 - val_auc: 0.9910 - val_loss: 0.9873\n",
      "Epoch 7/30\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9974 - auc: 1.0000 - loss: 0.8459\n",
      "Epoch 7: val_auc improved from 0.99096 to 0.99199, saving model to runs\\trained_on_Italian\\model_4_nca_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9972 - auc: 1.0000 - loss: 0.8150 - val_accuracy: 0.9432 - val_auc: 0.9920 - val_loss: 0.9141\n",
      "Epoch 8/30\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9967 - auc: 1.0000 - loss: 0.7417\n",
      "Epoch 8: val_auc did not improve from 0.99199\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9943 - auc: 1.0000 - loss: 0.7224 - val_accuracy: 0.9545 - val_auc: 0.9912 - val_loss: 0.7827\n",
      "Epoch 9/30\n",
      "\u001b[1m 8/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9983 - auc: 1.0000 - loss: 0.6634\n",
      "Epoch 9: val_auc did not improve from 0.99199\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9972 - auc: 1.0000 - loss: 0.6344 - val_accuracy: 0.9545 - val_auc: 0.9920 - val_loss: 0.7171\n",
      "Epoch 10/30\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.5778\n",
      "Epoch 10: val_auc did not improve from 0.99199\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.5593 - val_accuracy: 0.9545 - val_auc: 0.9920 - val_loss: 0.6424\n",
      "Epoch 11/30\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.5106\n",
      "Epoch 11: val_auc improved from 0.99199 to 0.99277, saving model to runs\\trained_on_Italian\\model_4_nca_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.4956 - val_accuracy: 0.9545 - val_auc: 0.9928 - val_loss: 0.5771\n",
      "Epoch 12/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.4506\n",
      "Epoch 12: val_auc did not improve from 0.99277\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.4404 - val_accuracy: 0.9545 - val_auc: 0.9923 - val_loss: 0.5364\n",
      "Epoch 13/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.3999\n",
      "Epoch 13: val_auc did not improve from 0.99277\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.3926 - val_accuracy: 0.9545 - val_auc: 0.9917 - val_loss: 0.4962\n",
      "Epoch 14/30\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.3626\n",
      "Epoch 14: val_auc did not improve from 0.99277\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.3506 - val_accuracy: 0.9545 - val_auc: 0.9923 - val_loss: 0.4719\n",
      "Epoch 15/30\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.3244\n",
      "Epoch 15: val_auc did not improve from 0.99277\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.3156 - val_accuracy: 0.9432 - val_auc: 0.9917 - val_loss: 0.4420\n",
      "Epoch 16/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.2881\n",
      "Epoch 16: val_auc did not improve from 0.99277\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.2825 - val_accuracy: 0.9545 - val_auc: 0.9907 - val_loss: 0.3918\n",
      "Epoch 17/30\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.2665\n",
      "Epoch 17: val_auc did not improve from 0.99277\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.2610 - val_accuracy: 0.9318 - val_auc: 0.9902 - val_loss: 0.3743\n",
      "Epoch 18/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.2451\n",
      "Epoch 18: val_auc did not improve from 0.99277\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.2404 - val_accuracy: 0.9545 - val_auc: 0.9915 - val_loss: 0.3776\n",
      "Epoch 19/30\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9977 - auc: 0.9999 - loss: 0.2314\n",
      "Epoch 19: val_auc improved from 0.99277 to 0.99535, saving model to runs\\trained_on_Italian\\model_4_nca_cnn.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9943 - auc: 0.9996 - loss: 0.2386 - val_accuracy: 0.9432 - val_auc: 0.9954 - val_loss: 0.3264\n",
      "Epoch 20/30\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9737 - auc: 0.9931 - loss: 0.2732\n",
      "Epoch 20: val_auc did not improve from 0.99535\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9631 - auc: 0.9862 - loss: 0.3190 - val_accuracy: 0.8750 - val_auc: 0.9845 - val_loss: 0.5635\n",
      "Epoch 21/30\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9608 - auc: 0.9977 - loss: 0.3194\n",
      "Epoch 21: val_auc did not improve from 0.99535\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9830 - auc: 0.9979 - loss: 0.2869 - val_accuracy: 0.9432 - val_auc: 0.9811 - val_loss: 0.4601\n",
      "Epoch 22/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9837 - auc: 0.9993 - loss: 0.2560\n",
      "Epoch 22: val_auc did not improve from 0.99535\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9858 - auc: 0.9959 - loss: 0.2751 - val_accuracy: 0.9432 - val_auc: 0.9801 - val_loss: 0.4181\n",
      "Epoch 23/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9980 - auc: 0.9998 - loss: 0.2493\n",
      "Epoch 23: val_auc did not improve from 0.99535\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9915 - auc: 0.9992 - loss: 0.2577 - val_accuracy: 0.9205 - val_auc: 0.9938 - val_loss: 0.4278\n",
      "Epoch 24/30\n",
      "\u001b[1m 9/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9892 - auc: 0.9996 - loss: 0.2495\n",
      "Epoch 24: val_auc did not improve from 0.99535\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9915 - auc: 0.9997 - loss: 0.2494 - val_accuracy: 0.9432 - val_auc: 0.9626 - val_loss: 0.5717\n",
      "Epoch 25/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.2261\n",
      "Epoch 25: val_auc did not improve from 0.99535\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.2183 - val_accuracy: 0.9773 - val_auc: 0.9737 - val_loss: 0.3757\n",
      "Epoch 26/30\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.2061\n",
      "Epoch 26: val_auc did not improve from 0.99535\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.2019 - val_accuracy: 0.9545 - val_auc: 0.9752 - val_loss: 0.3941\n",
      "Epoch 27/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.1906\n",
      "Epoch 27: val_auc did not improve from 0.99535\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.1873 - val_accuracy: 0.9545 - val_auc: 0.9747 - val_loss: 0.3730\n",
      "Epoch 28/30\n",
      "\u001b[1m10/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.1763\n",
      "Epoch 28: val_auc did not improve from 0.99535\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.1729 - val_accuracy: 0.9545 - val_auc: 0.9737 - val_loss: 0.3500\n",
      "Epoch 29/30\n",
      "\u001b[1m 9/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.1631\n",
      "Epoch 29: val_auc did not improve from 0.99535\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.1599 - val_accuracy: 0.9545 - val_auc: 0.9737 - val_loss: 0.3385\n",
      "Epoch 30/30\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.1507\n",
      "Epoch 30: val_auc did not improve from 0.99535\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.1482 - val_accuracy: 0.9545 - val_auc: 0.9726 - val_loss: 0.3216\n",
      "‚úÖ Saved training history and plot for model_4_nca_cnn.\n",
      "‚è±Ô∏è Model_4_NCA_CNN - Training: 0.45 minutes (26.97 seconds)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üî¨ TESTING ON UNSEEN DATASET: ({'dataset': 'Neurovoz', 'mode': 'A', 'feature_mode': 'ALL'},) with models from trained_on_Italian\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 150\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    149\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müî¨ TESTING ON UNSEEN DATASET: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTEST_CONFIG\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with models from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRUN_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     data = \u001b[43mload_single_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEST_CONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n\u001b[32m    152\u001b[39m         exit(\u001b[33m\"\u001b[39m\u001b[33mNo test data found. Aborting.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 216\u001b[39m, in \u001b[36mload_single_dataset\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_single_dataset\u001b[39m(config):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     dataset_name = \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    217\u001b[39m     mode = config[\u001b[33m'\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    218\u001b[39m     feature_mode = config[\u001b[33m'\u001b[39m\u001b[33mfeature_mode\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mTypeError\u001b[39m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# --- Main Execution ---\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # --- PHASE 1: TRAINING ---\n",
    "    RESULTS_PATH = os.path.join(\"runs\", RUN_ID)\n",
    "    PLOTS_PATH = os.path.join(RESULTS_PATH, \"plots\")\n",
    "    os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "    os.makedirs(PLOTS_PATH, exist_ok=True)\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üöÄ STARTING TRAINING PHASE | SAVING TO: {RUN_ID}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        X_train_full, y_train_full, mel_bins = load_and_combine_data(TRAIN_CONFIG)\n",
    "        with open(os.path.join(RESULTS_PATH, 'run_config.json'), 'w') as f:\n",
    "            json.dump({'mel_bins': mel_bins}, f)\n",
    "    except ValueError as e:\n",
    "        exit(f\"STOPPING: {e}\")\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full)\n",
    "    d1, d2 = X_train.shape[1], X_train.shape[2]\n",
    "    X_train_full_2d = X_train_full.reshape((X_train_full.shape[0], d1 * d2))\n",
    "    X_train_2d = X_train.reshape((X_train.shape[0], d1 * d2))\n",
    "    X_val_2d = X_val.reshape((X_val.shape[0], d1 * d2))\n",
    "    \n",
    "    print(f\"\\nTraining data prepared. Train: {X_train.shape}, Validation: {X_val.shape}\")\n",
    "    \n",
    "    # --- Train Model 1: k-NN (Baseline) ---\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TRAINING MODEL 1: k-NN (Baseline)\")\n",
    "    print(\"-\"*80)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pipeline_knn = Pipeline([('scaler', StandardScaler()), ('smote', SMOTE(random_state=42)), ('nca', NeighborhoodComponentsAnalysis(random_state=42, max_iter=200)), ('classifier', KNeighborsClassifier())])\n",
    "    param_dist_knn = {'nca__n_components': [10, 20, 30, 40], 'classifier__n_neighbors': [3, 5, 7], 'classifier__weights': ['distance'], 'classifier__metric': ['manhattan']}\n",
    "    search_knn = RandomizedSearchCV(pipeline_knn, param_dist_knn, n_iter=10, cv=3, scoring='accuracy', n_jobs=1, random_state=42, verbose=1)\n",
    "    search_knn.fit(X_train_full_2d, y_train_full)\n",
    "    joblib.dump(search_knn.best_estimator_, os.path.join(RESULTS_PATH, \"model_1_knn.joblib\"))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    runtime_tracker.track_runtime(\"Model_1_kNN\", \"Training\", start_time, end_time)\n",
    "    print(\"‚úÖ Best k-NN pipeline trained and saved.\")\n",
    "\n",
    "    # --- Train Model 2: CNN (End-to-End) ---\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TRAINING MODEL 2: CNN (End-to-End)\")\n",
    "    print(\"-\"*80)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    MODEL_CNN_PATH = os.path.join(RESULTS_PATH, \"model_2_cnn.keras\")\n",
    "    model_cnn = ParkinsonDetectorModel(input_shape=(d1, d2))\n",
    "    model_cnn.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "    history_cnn = model_cnn.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                                 callbacks=[ModelCheckpoint(MODEL_CNN_PATH, save_best_only=True, monitor='val_auc', mode='max', verbose=1)])\n",
    "    plot_and_save_history(history_cnn, \"model_2_cnn\", PLOTS_PATH)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    runtime_tracker.track_runtime(\"Model_2_CNN\", \"Training\", start_time, end_time)\n",
    "    \n",
    "    # --- Train Model 3: CNN + k-NN ---\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TRAINING MODEL 3: CNN Feature Extractor + k-NN\")\n",
    "    print(\"-\"*80)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_model_cnn_extractor = tf.keras.models.load_model(MODEL_CNN_PATH)\n",
    "    \n",
    "    # Create a functional model to extract features from the 'bottleneck_features' layer\n",
    "    # Fix: Create a new Input layer since the loaded model's input isn't defined yet.\n",
    "    cnn_input_tensor = tf.keras.Input(shape=(d1, d2))\n",
    "    cnn_output_tensor = best_model_cnn_extractor(cnn_input_tensor, extract_features=True)\n",
    "    feature_extractor_model = Model(\n",
    "        inputs=cnn_input_tensor,\n",
    "        outputs=cnn_output_tensor\n",
    "    )\n",
    "\n",
    "    # Use predict in batches on the full training data to avoid memory issues\n",
    "    X_train_features = feature_extractor_model.predict(X_train_full, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Fit the k-NN model on the extracted features\n",
    "    search_cnn_knn = RandomizedSearchCV(\n",
    "        pipeline_knn, \n",
    "        param_dist_knn, \n",
    "        n_iter=10, \n",
    "        cv=3, \n",
    "        scoring='accuracy', \n",
    "        n_jobs=1, \n",
    "        random_state=42, \n",
    "        verbose=1\n",
    "    )\n",
    "    search_cnn_knn.fit(X_train_features, y_train_full)\n",
    "    joblib.dump(search_cnn_knn.best_estimator_, os.path.join(RESULTS_PATH, \"model_3_cnn_knn.joblib\"))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    runtime_tracker.track_runtime(\"Model_3_CNN_kNN\", \"Training\", start_time, end_time)\n",
    "    print(\"‚úÖ Best CNN+k-NN pipeline trained and saved.\")\n",
    "    \n",
    "    # --- Train Model 4: NCA + CNN ---\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TRAINING MODEL 4: NCA pre-processing + CNN\")\n",
    "    print(\"-\"*80)\n",
    "    start_time = time.time()\n",
    "\n",
    "    NCA_COMPONENTS = 64\n",
    "    nca_img_dim = int(np.sqrt(NCA_COMPONENTS))\n",
    "\n",
    "    # Use a pipeline to handle scaling and NCA transformation on the fly\n",
    "    nca_preprocessor = Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('nca', NeighborhoodComponentsAnalysis(n_components=NCA_COMPONENTS, random_state=42, max_iter=200))\n",
    "    ])\n",
    "\n",
    "    # Fit and transform ONLY on the training split, not the full dataset\n",
    "    X_train_nca = nca_preprocessor.fit_transform(X_train_2d, y_train)\n",
    "    X_train_nca_3d = X_train_nca.reshape(-1, nca_img_dim, nca_img_dim)\n",
    "\n",
    "    # Transform the validation split using the fitted preprocessor\n",
    "    X_val_nca = nca_preprocessor.transform(X_val_2d)\n",
    "    X_val_nca_3d = X_val_nca.reshape(-1, nca_img_dim, nca_img_dim)\n",
    "\n",
    "    # Save the preprocessor to be used later for testing\n",
    "    joblib.dump(nca_preprocessor, os.path.join(RESULTS_PATH, \"model_4_nca_preprocessor.joblib\"))\n",
    "\n",
    "    MODEL_NCA_CNN_PATH = os.path.join(RESULTS_PATH, \"model_4_nca_cnn.keras\")\n",
    "    model_nca_cnn = ParkinsonDetectorModelNCA(input_shape=(nca_img_dim, nca_img_dim))\n",
    "    model_nca_cnn.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "    history_nca_cnn = model_nca_cnn.fit(\n",
    "        X_train_nca_3d, \n",
    "        y_train, \n",
    "        validation_data=(X_val_nca_3d, y_val), \n",
    "        epochs=EPOCHS, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[ModelCheckpoint(MODEL_NCA_CNN_PATH, save_best_only=True, monitor='val_auc', mode='max', verbose=1)]\n",
    "    )\n",
    "    plot_and_save_history(history_nca_cnn, \"model_4_nca_cnn\", PLOTS_PATH)\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime_tracker.track_runtime(\"Model_4_NCA_CNN\", \"Training\", start_time, end_time)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f915b71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "üî¨ TESTING ON UNSEEN DATASET: ({'dataset': 'Neurovoz', 'mode': 'A', 'feature_mode': 'ALL'},) with models from trained_on_Italian\n",
      "--- Loading data from Neurovoz\\data\\features_A_ALL.npz ---\n",
      "Loaded Neurovoz successfully. Shape: (1064, 60, 94)\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "EVALUATING MODEL 1: k-NN (Baseline)\n",
      "--------------------------------------------------------------------------------\n",
      "‚úÖ Saved all metrics and plots for model_1_knn.\n",
      "‚è±Ô∏è Model_1_kNN - Testing: 0.03 minutes (1.94 seconds)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "EVALUATING MODEL 2: CNN (End-to-End)\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 130ms/step\n",
      "‚úÖ Saved all metrics and plots for model_2_cnn.\n",
      "‚è±Ô∏è Model_2_CNN - Testing: 0.13 minutes (7.81 seconds)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "EVALUATING MODEL 3: CNN + k-NN\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 110ms/step\n",
      "‚úÖ Saved all metrics and plots for model_3_cnn_knn.\n",
      "‚è±Ô∏è Model_3_CNN_kNN - Testing: 0.11 minutes (6.41 seconds)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "EVALUATING MODEL 4: NCA + CNN\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
      "‚úÖ Saved all metrics and plots for model_4_nca_cnn.\n",
      "‚è±Ô∏è Model_4_NCA_CNN - Testing: 0.07 minutes (4.49 seconds)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üî¨ RUNNING EXPLAINABILITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "--- Explaining Model 2 (CNN) and Model 3 (CNN+k-NN) Feature Extractor ---\n",
      "\n",
      "--- Running SHAP Analysis ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bahar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor_12\n",
      "Received: inputs=['Tensor(shape=(50, 60, 94))']\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saved SHAP global bar plot.\n",
      "-> Saved SHAP class heatmaps.\n",
      "--- SHAP Analysis Complete ---\n",
      "\n",
      "--- Running Grad-CAM Analysis ---\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saved average Grad-CAM comparison.\n",
      "--- Grad-CAM Analysis Complete ---\n",
      "‚è±Ô∏è Model_2_CNN - Explainability: 1.64 minutes (98.63 seconds)\n",
      "\n",
      "--- Explaining Model 4 (NCA + CNN) ---\n",
      "\n",
      "--- Running SHAP Analysis ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bahar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor_14\n",
      "Received: inputs=['Tensor(shape=(50, 8, 8))']\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saved SHAP global bar plot.\n",
      "-> Saved SHAP class heatmaps.\n",
      "--- SHAP Analysis Complete ---\n",
      "\n",
      "--- Running Grad-CAM Analysis ---\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saved average Grad-CAM comparison.\n",
      "--- Grad-CAM Analysis Complete ---\n",
      "‚è±Ô∏è Model_4_NCA_CNN - Explainability: 0.33 minutes (19.68 seconds)\n",
      "\n",
      "üìä Runtime Summary:\n",
      "          Model  Total_Minutes  Avg_Minutes  Total_Seconds  Avg_Seconds\n",
      "    Model_1_kNN          0.249        0.124         14.936        7.468\n",
      "    Model_2_CNN          4.679        1.560        280.718       93.573\n",
      "Model_3_CNN_kNN          0.284        0.142         17.069        8.535\n",
      "Model_4_NCA_CNN          0.852        0.284         51.137       17.046\n",
      "‚úÖ Runtime data saved to: d:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\runs\\trained_on_Italian\\model_runtimes.csv\n",
      "‚úÖ Runtime summary saved to: d:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\runs\\trained_on_Italian\\runtime_summary.csv\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üèÜ FINAL SUMMARY - trained_on_Italian\n",
      "================================================================================\n",
      "             Model  Accuracy       AUC  F1-Score  Precision  Sensitivity (Recall)   TP   TN   FP   FN\n",
      "0      model_1_knn  0.516917  0.509967  0.608828   0.511509              0.751880  400  150  382  132\n",
      "1      model_2_cnn  0.486842  0.497023  0.642202   0.492958              0.921053  490   28  504   42\n",
      "2  model_3_cnn_knn  0.495301  0.497375  0.630928   0.497291              0.862782  459   68  464   73\n",
      "3  model_4_nca_cnn  0.490602  0.507328  0.632791   0.494703              0.877820  467   55  477   65\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üöÄ Best Performing Model: 'model_1_knn' with an accuracy of 0.5169\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "    # --- PHASE 2: TESTING AND EXPLAINABILITY ---\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    if TEST_CONFIG is None:\n",
    "        print(f\"üî¨ TESTING ON VALIDATION SPLIT FROM: {RUN_ID}\")\n",
    "        X_test, y_test = X_val, y_val\n",
    "    else:\n",
    "        print(f\"üî¨ TESTING ON UNSEEN DATASET: {TEST_CONFIG} with models from {RUN_ID}\")\n",
    "        data = load_single_dataset(TEST_CONFIG[0])\n",
    "        if data is None: \n",
    "            exit(\"No test data found. Aborting.\")\n",
    "        X_test, y_test, _ = data\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Correctly reshape test data for models that expect 2D input\n",
    "    d1, d2 = X_test.shape[1], X_test.shape[2]\n",
    "    X_test_2d = X_test.reshape((X_test.shape[0], d1 * d2))\n",
    "    \n",
    "    test_results = {}\n",
    "    \n",
    "    # --- Test Model 1: k-NN ---\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"EVALUATING MODEL 1: k-NN (Baseline)\")\n",
    "    print(\"-\"*80)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_1 = joblib.load(os.path.join(RESULTS_PATH, \"model_1_knn.joblib\"))\n",
    "    y_pred_1 = model_1.predict(X_test_2d)\n",
    "    y_pred_proba_1 = model_1.predict_proba(X_test_2d)[:, 1]\n",
    "    acc1 = calculate_and_save_metrics(y_test, y_pred_1, y_pred_proba_1, \"model_1_knn\", RESULTS_PATH, PLOTS_PATH)\n",
    "    test_results[\"1: k-NN (Baseline)\"] = acc1\n",
    "    \n",
    "    end_time = time.time()\n",
    "    runtime_tracker.track_runtime(\"Model_1_kNN\", \"Testing\", start_time, end_time)\n",
    "\n",
    "    # --- Test Model 2: CNN ---\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"EVALUATING MODEL 2: CNN (End-to-End)\")\n",
    "    print(\"-\"*80)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_2 = tf.keras.models.load_model(os.path.join(RESULTS_PATH, \"model_2_cnn.keras\"))\n",
    "    y_pred_proba_2 = model_2.predict(X_test)\n",
    "    y_pred_2 = (y_pred_proba_2 > 0.5).astype(\"int32\")\n",
    "    acc2 = calculate_and_save_metrics(y_test, y_pred_2, y_pred_proba_2, \"model_2_cnn\", RESULTS_PATH, PLOTS_PATH)\n",
    "    test_results[\"2: CNN (End-to-End)\"] = acc2\n",
    "    \n",
    "    end_time = time.time()\n",
    "    runtime_tracker.track_runtime(\"Model_2_CNN\", \"Testing\", start_time, end_time)\n",
    "    \n",
    "    # --- Test Model 3: CNN + k-NN ---\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"EVALUATING MODEL 3: CNN + k-NN\")\n",
    "    print(\"-\"*80)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_3_extractor = tf.keras.models.load_model(os.path.join(RESULTS_PATH, \"model_2_cnn.keras\"))\n",
    "    model_3_knn = joblib.load(os.path.join(RESULTS_PATH, \"model_3_cnn_knn.joblib\"))\n",
    "    # Use the feature extractor to get features from the test data\n",
    "    # Fix: Use the `feature_extractor_model` created earlier for consistency\n",
    "    X_test_features = feature_extractor_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "    y_pred_3 = model_3_knn.predict(X_test_features)\n",
    "    y_pred_proba_3 = model_3_knn.predict_proba(X_test_features)[:, 1]\n",
    "    acc3 = calculate_and_save_metrics(y_test, y_pred_3, y_pred_proba_3, \"model_3_cnn_knn\", RESULTS_PATH, PLOTS_PATH)\n",
    "    test_results[\"3: CNN + k-NN\"] = acc3\n",
    "    \n",
    "    end_time = time.time()\n",
    "    runtime_tracker.track_runtime(\"Model_3_CNN_kNN\", \"Testing\", start_time, end_time)\n",
    "    \n",
    "    # --- Test Model 4: NCA + CNN ---\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"EVALUATING MODEL 4: NCA + CNN\")\n",
    "    print(\"-\"*80)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_4_preprocessor = joblib.load(os.path.join(RESULTS_PATH, \"model_4_nca_preprocessor.joblib\"))\n",
    "    model_4_cnn = tf.keras.models.load_model(os.path.join(RESULTS_PATH, \"model_4_nca_cnn.keras\"))\n",
    "    \n",
    "    # Apply the preprocessor to the test data\n",
    "    X_test_nca = model_4_preprocessor.transform(X_test_2d)\n",
    "    nca_img_dim = int(np.sqrt(X_test_nca.shape[1]))\n",
    "    X_test_nca_3d = X_test_nca.reshape(-1, nca_img_dim, nca_img_dim)\n",
    "    \n",
    "    y_pred_proba_4 = model_4_cnn.predict(X_test_nca_3d)\n",
    "    y_pred_4 = (y_pred_proba_4 > 0.5).astype(\"int32\")\n",
    "    acc4 = calculate_and_save_metrics(y_test, y_pred_4, y_pred_proba_4, \"model_4_nca_cnn\", RESULTS_PATH, PLOTS_PATH)\n",
    "    test_results[\"4: NCA + CNN\"] = acc4\n",
    "    \n",
    "    end_time = time.time()\n",
    "    runtime_tracker.track_runtime(\"Model_4_NCA_CNN\", \"Testing\", start_time, end_time)\n",
    "    \n",
    "    # --- Explainability for CNN-based models ---\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"üî¨ RUNNING EXPLAINABILITY ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    explain_path = os.path.join(PLOTS_PATH, \"explainability\")\n",
    "    print(\"\\n--- Explaining Model 2 (CNN) and Model 3 (CNN+k-NN) Feature Extractor ---\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    run_full_shap_analysis(model_2, X_train, X_test, y_test, os.path.join(explain_path, \"model_2_cnn\"), mel_bins)\n",
    "    run_gradcam_analysis(model_2, X_test, y_test, os.path.join(explain_path, \"model_2_cnn\"))\n",
    "    end_time = time.time()\n",
    "    runtime_tracker.track_runtime(\"Model_2_CNN\", \"Explainability\", start_time, end_time)\n",
    "    \n",
    "    print(\"\\n--- Explaining Model 4 (NCA + CNN) ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Note: SHAP analysis for Model 4 should use the NCA-transformed training and test data\n",
    "    run_full_shap_analysis(model_4_cnn, X_train_nca_3d, X_test_nca_3d, y_test, os.path.join(explain_path, \"model_4_nca_cnn\"), mel_bins=0)\n",
    "    run_gradcam_analysis(model_4_cnn, X_test_nca_3d, y_test, os.path.join(explain_path, \"model_4_nca_cnn\"))\n",
    "    end_time = time.time()\n",
    "    runtime_tracker.track_runtime(\"Model_4_NCA_CNN\", \"Explainability\", start_time, end_time)\n",
    "    \n",
    "    # --- Save Runtime Summary ---\n",
    "    runtime_tracker.save_runtime_summary()\n",
    "    \n",
    "    # --- Final Summary ---\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(f\"üèÜ FINAL SUMMARY - {RUN_ID}\")\n",
    "    print(\"=\"*80)\n",
    "    summary_df = pd.read_csv(os.path.join(RESULTS_PATH, \"evaluation_metrics.csv\"))\n",
    "    print(summary_df.to_string())\n",
    "    best_model_row = summary_df.loc[summary_df['Accuracy'].idxmax()]\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"\\nüöÄ Best Performing Model: '{best_model_row['Model']}' with an accuracy of {best_model_row['Accuracy']:.4f}\")\n",
    "    print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
