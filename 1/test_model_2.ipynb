{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ae15c24db8dec",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-10T22:54:38.716322Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Dropout, Flatten,\n",
    "                                     Dense, LSTM, MultiHeadAttention, Concatenate, Reshape)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# =============================================================================\n",
    "# --- Configuration ---\n",
    "# =============================================================================\n",
    "# DATASET = \"MPOWER_DATASET\"\n",
    "MODE = \"ALL_VALIDS\"\n",
    "FEATURE_MODE = \"BASIC\"\n",
    "MODEL_NAME = \"feature_extractor_cnn_att_lstm\" # MODIFIED: Model name updated\n",
    "# ------------------------------------\n",
    "\n",
    "# Path Setup\n",
    "dataset = \"Italian\"\n",
    "FEATURES_FILE_PATH = os.path.join(os.getcwd(), dataset, \"data\", f\"features_{MODE}_{FEATURE_MODE}.npz\")\n",
    "RESULTS_PATH = os.path.join(os.getcwd(), dataset, f\"results_{MODE}_{FEATURE_MODE}\")\n",
    "MODEL_PATH = os.path.join(RESULTS_PATH, MODEL_NAME)\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "HISTORY_SAVE_PATH = os.path.join(MODEL_PATH, \"history.csv\")\n",
    "BEST_EXTRACTOR_PATH = os.path.join(MODEL_PATH, \"best_feature_extractor.keras\") # MODIFIED: Path name updated\n",
    "KNN_MODEL_PATH = os.path.join(MODEL_PATH, \"knn_classifier.joblib\") # NEW: Path for k-NN model\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT_RATE = 0.5\n",
    "L2_STRENGTH = 0.01\n",
    "\n",
    "# =============================================================================\n",
    "# --- Your Custom CNN Models ---\n",
    "# =============================================================================\n",
    "@register_keras_serializable()\n",
    "class ParkinsonDetectorModel(Model):\n",
    "    \"\"\"Your end-to-end CNN model for original features.\"\"\"\n",
    "    def __init__(self, input_shape, **kwargs):\n",
    "        super(ParkinsonDetectorModel, self).__init__(**kwargs)\n",
    "        self.input_shape_config = input_shape\n",
    "        self.reshape_in = Reshape((input_shape[0], input_shape[1], 1))\n",
    "        self.conv1a = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv1b = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.pool1 = MaxPooling2D(5)\n",
    "        self.drop1 = Dropout(DROPOUT_RATE)\n",
    "        self.conv2a = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv2b = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.pool2 = MaxPooling2D(5)\n",
    "        self.drop2 = Dropout(DROPOUT_RATE)\n",
    "        self.flatten_cnn = Flatten()\n",
    "        self.attention = MultiHeadAttention(num_heads=2, key_dim=64)\n",
    "        self.flatten_att = Flatten()\n",
    "        self.lstm1 = LSTM(128, return_sequences=True)\n",
    "        self.lstm2 = LSTM(128, return_sequences=False)\n",
    "        self.drop_lstm = Dropout(DROPOUT_RATE)\n",
    "        self.concat = Concatenate()\n",
    "        self.dense_bottleneck = Dense(128, activation='relu', name='bottleneck_features')\n",
    "        self.dense_output = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, extract_features=False):\n",
    "        x = self.reshape_in(inputs)\n",
    "        x = self.conv1a(x)\n",
    "        x = self.conv1b(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.conv2a(x)\n",
    "        x = self.conv2b(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.drop2(x)\n",
    "        cnn_flat = self.flatten_cnn(x)\n",
    "        shape = tf.shape(x)\n",
    "        sequence = tf.reshape(x, [-1, shape[1] * shape[2], shape[3]])\n",
    "        att_out = self.attention(query=sequence, key=sequence, value=sequence)\n",
    "        att_flat = self.flatten_att(att_out)\n",
    "        lstm_seq = self.lstm1(sequence)\n",
    "        lstm_out = self.lstm2(lstm_seq)\n",
    "        lstm_out = self.drop_lstm(lstm_out)\n",
    "        concatenated = self.concat([cnn_flat, att_flat, lstm_out])\n",
    "        bottleneck = self.dense_bottleneck(concatenated)\n",
    "        if extract_features:\n",
    "            return bottleneck\n",
    "        return self.dense_output(bottleneck)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ParkinsonDetectorModel, self).get_config()\n",
    "        config.update({\"input_shape\": self.input_shape_config})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "@register_keras_serializable()\n",
    "class ParkinsonDetectorModelNCA(Model):\n",
    "    \"\"\"A modified version of your model for the small 8x8 NCA input.\"\"\"\n",
    "    def __init__(self, input_shape, **kwargs):\n",
    "        super(ParkinsonDetectorModelNCA, self).__init__(**kwargs)\n",
    "        self.input_shape_config = input_shape\n",
    "        self.reshape_in = Reshape((input_shape[0], input_shape[1], 1))\n",
    "        self.conv1a = Conv2D(64, 3, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv1b = Conv2D(64, 3, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.pool1 = MaxPooling2D(2)\n",
    "        self.drop1 = Dropout(DROPOUT_RATE)\n",
    "        self.conv2a = Conv2D(64, 3, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv2b = Conv2D(64, 3, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.pool2 = MaxPooling2D(2)\n",
    "        self.drop2 = Dropout(DROPOUT_RATE)\n",
    "        self.flatten_cnn = Flatten()\n",
    "        self.attention = MultiHeadAttention(num_heads=2, key_dim=64)\n",
    "        self.flatten_att = Flatten()\n",
    "        self.lstm1 = LSTM(128, return_sequences=True)\n",
    "        self.lstm2 = LSTM(128, return_sequences=False)\n",
    "        self.drop_lstm = Dropout(DROPOUT_RATE)\n",
    "        self.concat = Concatenate()\n",
    "        self.dense_bottleneck = Dense(128, activation='relu', name='bottleneck_features')\n",
    "        self.dense_output = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, extract_features=False):\n",
    "        x = self.reshape_in(inputs)\n",
    "        x = self.conv1a(x)\n",
    "        x = self.conv1b(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.conv2a(x)\n",
    "        x = self.conv2b(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.drop2(x)\n",
    "        cnn_flat = self.flatten_cnn(x)\n",
    "        shape = tf.shape(x)\n",
    "        sequence = tf.reshape(x, [-1, shape[1] * shape[2], shape[3]])\n",
    "        att_out = self.attention(query=sequence, key=sequence, value=sequence)\n",
    "        att_flat = self.flatten_att(att_out)\n",
    "        lstm_seq = self.lstm1(sequence)\n",
    "        lstm_out = self.lstm2(lstm_seq)\n",
    "        lstm_out = self.drop_lstm(lstm_out)\n",
    "        concatenated = self.concat([cnn_flat, att_flat, lstm_out])\n",
    "        bottleneck = self.dense_bottleneck(concatenated)\n",
    "        if extract_features:\n",
    "            return bottleneck\n",
    "        return self.dense_output(bottleneck)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ParkinsonDetectorModelNCA, self).get_config()\n",
    "        config.update({\"input_shape\": self.input_shape_config})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "# =============================================================================\n",
    "# --- Helper Functions ---\n",
    "# =============================================================================\n",
    "def load_data(path: str) -> tuple:\n",
    "    \"\"\"Loads features and labels from the .npz file.\"\"\"\n",
    "    print(f\"--- Loading data from {path} ---\")\n",
    "    with np.load(path) as data:\n",
    "        mel_spectrograms = data['mel_spectrogram']\n",
    "        mfccs = data['mfcc']\n",
    "        labels = data['labels']\n",
    "        X = np.concatenate((mel_spectrograms, mfccs), axis=1)\n",
    "    print(\"Data loaded successfully.\")\n",
    "    return X, labels\n",
    "\n",
    "def evaluate_cnn_and_report(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluates a compiled Keras model and prints a classification report.\"\"\"\n",
    "    print(f\"\\n--- Evaluating Model: {model_name} ---\")\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Loss: {loss:.4f}\")\n",
    "\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(\"int32\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    return accuracy # MODIFIED: Return the accuracy\n",
    "\n",
    "def tune_and_evaluate_knn_pipeline(X_train, y_train, X_test, y_test, results_path, model_name):\n",
    "    \"\"\"Finds the best k-NN pipeline using SMOTE and NCA.\"\"\"\n",
    "    print(f\"\\n--- Tuning and Evaluating k-NN Pipeline for: {model_name} ---\")\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('nca', NeighborhoodComponentsAnalysis(random_state=42, max_iter=200)),\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ])\n",
    "    param_dist = {\n",
    "        'nca__n_components': [10, 20, 30, 40, 50],\n",
    "        'classifier__n_neighbors': [5, 7, 9, 11, 13],\n",
    "        'classifier__weights': ['distance'],\n",
    "        'classifier__metric': ['manhattan']\n",
    "    }\n",
    "    search = RandomizedSearchCV(pipeline, param_dist, n_iter=15, cv=5, scoring='accuracy', n_jobs=1, random_state=42, verbose=1)\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"\\n--- Results for {model_name} ---\")\n",
    "    print(f\"Best cross-validation accuracy: {search.best_score_:.4f}\")\n",
    "    y_pred = search.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "    pipeline_path = os.path.join(results_path, f\"{model_name}_pipeline.joblib\")\n",
    "    joblib.dump(search.best_estimator_, pipeline_path)\n",
    "    print(f\"✅ Best k-NN pipeline saved to: {pipeline_path}\")\n",
    "    return test_accuracy # MODIFIED: Return the accuracy\n",
    "\n",
    "# =============================================================================\n",
    "# --- Main Execution ---\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "    # MODIFIED: Dictionary to store final results\n",
    "    results_summary = {}\n",
    "\n",
    "    X, y = load_data(FEATURES_FILE_PATH)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    n_samples_train, d1, d2 = X_train.shape\n",
    "    X_train_2d = X_train.reshape((n_samples_train, d1 * d2))\n",
    "    X_test_2d = X_test.reshape((X_test.shape[0], d1 * d2))\n",
    "    print(f\"Data split and reshaped. Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # --- Experiment 1: k-NN (Baseline) ---\n",
    "    # =========================================================================\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"RUNNING EXPERIMENT 1: k-NN (Baseline on original features)\")\n",
    "    print(\"=\"*80)\n",
    "    acc1 = tune_and_evaluate_knn_pipeline(X_train_2d, y_train, X_test_2d, y_test, RESULTS_PATH, \"baseline_knn\")\n",
    "    results_summary[\"1: k-NN (Baseline)\"] = acc1\n",
    "\n",
    "    # =========================================================================\n",
    "    # --- Experiment 2: CNN (End-to-End) ---\n",
    "    # =========================================================================\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"RUNNING EXPERIMENT 2: CNN (End-to-End)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    MODEL_CNN_PATH = os.path.join(RESULTS_PATH, \"cnn_model.keras\")\n",
    "    model_cnn = ParkinsonDetectorModel(input_shape=(d1, d2))\n",
    "    model_cnn.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(\"\\n--- Training CNN model ---\")\n",
    "    model_cnn.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                  callbacks=[ModelCheckpoint(MODEL_CNN_PATH, save_best_only=True, monitor='val_accuracy', mode='max')],\n",
    "                  verbose=1)\n",
    "\n",
    "    best_model_cnn = tf.keras.models.load_model(MODEL_CNN_PATH)\n",
    "    acc2 = evaluate_cnn_and_report(best_model_cnn, X_test, y_test, \"CNN_End_to_End\")\n",
    "    results_summary[\"2: CNN (End-to-End)\"] = acc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96602fb251bddfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # =========================================================================\n",
    "    # --- Experiment 3: CNN + k-NN ---\n",
    "    # =========================================================================\n",
    "    # --- Experiment 3: CNN + k-NN ---\n",
    "    print(\"RUNNING EXPERIMENT 3: CNN Feature Extractor + k-NN\")\n",
    "    # ...\n",
    "    # Call the model to get features as Tensors\n",
    "    X_train_features_tensor = best_model_cnn(X_train, extract_features=True)\n",
    "    X_test_features_tensor = best_model_cnn(X_test, extract_features=True)\n",
    "\n",
    "    # FIX: Convert Tensors to NumPy arrays for scikit-learn\n",
    "    X_train_features = X_train_features_tensor.numpy()\n",
    "    X_test_features = X_test_features_tensor.numpy()\n",
    "\n",
    "    print(f\"Extracted feature shapes: Train={X_train_features.shape}, Test={X_test_features.shape}\")\n",
    "\n",
    "    tune_and_evaluate_knn_pipeline(\n",
    "        X_train_features, y_train, X_test_features, y_test, RESULTS_PATH, \"cnn_plus_knn\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c54cb684c95fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # =========================================================================\n",
    "    # --- Experiment 4: NCA + CNN (no k-NN) ---\n",
    "    # =========================================================================\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"RUNNING EXPERIMENT 4: NCA pre-processing + CNN (End-to-End)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(\"\\n--- Applying NCA to original 2D features ---\")\n",
    "    NCA_COMPONENTS = 64\n",
    "    nca_pipeline = Pipeline([('scaler', StandardScaler()), ('nca', NeighborhoodComponentsAnalysis(n_components=NCA_COMPONENTS, random_state=42, max_iter=200))])\n",
    "    X_train_nca = nca_pipeline.fit_transform(X_train_2d, y_train)\n",
    "    X_test_nca = nca_pipeline.transform(X_test_2d)\n",
    "\n",
    "    nca_img_dim = int(np.sqrt(NCA_COMPONENTS))\n",
    "    X_train_nca_3d = X_train_nca.reshape(-1, nca_img_dim, nca_img_dim)\n",
    "    X_test_nca_3d = X_test_nca.reshape(-1, nca_img_dim, nca_img_dim)\n",
    "\n",
    "    MODEL_NCA_CNN_PATH = os.path.join(RESULTS_PATH, \"nca_cnn_model.keras\")\n",
    "    model_nca_cnn = ParkinsonDetectorModelNCA(input_shape=(nca_img_dim, nca_img_dim))\n",
    "    model_nca_cnn.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(\"\\n--- Training NCA+CNN model ---\")\n",
    "    model_nca_cnn.fit(X_train_nca_3d, y_train, validation_data=(X_test_nca_3d, y_test), epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                      callbacks=[ModelCheckpoint(MODEL_NCA_CNN_PATH, save_best_only=True, monitor='val_accuracy', mode='max')],\n",
    "                      verbose=1)\n",
    "\n",
    "    best_model_nca_cnn = tf.keras.models.load_model(MODEL_NCA_CNN_PATH)\n",
    "    acc4 = evaluate_cnn_and_report(best_model_nca_cnn, X_test_nca_3d, y_test, \"NCA_plus_CNN\")\n",
    "    results_summary[\"4: NCA + CNN\"] = acc4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0901a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # =========================================================================\n",
    "    # --- FINAL RESULTS SUMMARY ---\n",
    "    # =========================================================================\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"✅ ALL EXPERIMENTS COMPLETE! FINAL RESULTS SUMMARY:\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    if not results_summary:\n",
    "        print(\"No results were recorded.\")\n",
    "    else:\n",
    "        # Find the best model\n",
    "        best_model_name = max(results_summary, key=results_summary.get)\n",
    "        best_accuracy = results_summary[best_model_name]\n",
    "\n",
    "        print(f\"{'Model':<40} | {'Final Test Accuracy':<20}\")\n",
    "        print(\"-\" * 65)\n",
    "        for name, acc in results_summary.items():\n",
    "            print(f\"{name:<40} | {acc:<20.4f}\")\n",
    "\n",
    "        print(\"-\" * 65)\n",
    "        print(f\"\\n🏆 Best Performing Model: '{best_model_name}' with an accuracy of {best_accuracy:.4f}\")\n",
    "\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
