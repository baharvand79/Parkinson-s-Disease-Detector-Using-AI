{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T07:13:56.205746Z",
     "start_time": "2025-09-13T07:13:45.641043Z"
    }
   },
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Dropout, Flatten,\n",
    "                                     Dense, LSTM, MultiHeadAttention, Concatenate, Reshape)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import ttest_ind\n",
    "import pandas as pd\n",
    "import shap\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.patches as mpatches\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Reshape\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# =============================================================================\n",
    "# --- Configuration ---\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. Core Paths ---\n",
    "ITALIAN_DATASET = \"ITALIAN_DATASET\"\n",
    "UAMS_DATASET = \"UAMS_DATASET\"\n",
    "NEUROVOZ_DATASET = \"NEUROVOZ_DATASET\"\n",
    "MPOWER_DATASET = \"MPOWER_DATASET\"\n",
    "SYNTHETIC_DATASET = \"SYNTHETIC_DATASET\"\n",
    "\n",
    "MODE_ALL_VALIDS = \"ALL_VALIDS\"\n",
    "MODE_A = \"A\"\n",
    "\n",
    "FEATURE_MODE_BASIC = \"BASIC\"        # mel_spectrogram, mfcc, spectrogram\n",
    "FEATURE_MODE_ALL = \"ALL\"            # basic + fsc\n",
    "FEATURE_MODE_DEFAULT = \"DEFAULT\"\n",
    "\n",
    "MODEL_NAME = \"cnn_att_lstm\"\n",
    "\n",
    "# /////////// SELCET HERE \\\\\\\\\\\\\\\\\\\\\\\n",
    "# ----------------------------------\n",
    "DATASET = UAMS_DATASET\n",
    "MODE = MODE_A\n",
    "FEATURE_MODE = FEATURE_MODE_DEFAULT\n",
    "# ----------------------------------"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T07:13:56.219955Z",
     "start_time": "2025-09-13T07:13:56.212937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = \"\"\n",
    "if DATASET == NEUROVOZ_DATASET:\n",
    "    dataset = \"Neurovoz\"\n",
    "elif DATASET == UAMS_DATASET:\n",
    "    dataset = \"UAMS\"\n",
    "elif DATASET == MPOWER_DATASET:\n",
    "    dataset = \"mPower\"\n",
    "elif DATASET == SYNTHETIC_DATASET:\n",
    "    dataset = \"Synthetic\"\n",
    "elif DATASET == ITALIAN_DATASET:\n",
    "    dataset = \"Italian\"\n",
    "\n",
    "# Path Setup\n",
    "FEATURES_FILE_PATH = os.path.join(os.getcwd(), dataset, \"data\", f\"features_{MODE}_{FEATURE_MODE}.npz\")\n",
    "\n",
    "MODEL_PATH = os.path.join(os.getcwd(), dataset, f\"results_{MODE}_{FEATURE_MODE}\", MODEL_NAME)\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "EVALUATION_FILE_PATH = os.path.join(MODEL_PATH, \"evaluation.csv\")\n",
    "HISTORY_SAVE_PATH = os.path.join(MODEL_PATH, \"history.csv\")\n",
    "BEST_MODEL_PATH = os.path.join(MODEL_PATH, \"best_model.keras\")\n",
    "\n",
    "SHAP_OUTPUT_PATH = os.path.join(MODEL_PATH, \"shap_analysis\")\n",
    "GRADCAM_OUTPUT_PATH = os.path.join(MODEL_PATH, \"gradcam_analysis\")\n",
    "os.makedirs(SHAP_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(GRADCAM_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT_RATE = 0.5\n",
    "L2_STRENGTH = 0.01\n",
    "\n",
    "# Model Checkpoint Callback\n",
    "checkpoint_cb = ModelCheckpoint(BEST_MODEL_PATH, monitor='val_auc', mode='max', save_best_only=True, verbose=1)"
   ],
   "id": "da1cd86c3e2012d2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T07:13:56.238852Z",
     "start_time": "2025-09-13T07:13:56.226605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(feature_file_path):\n",
    "\n",
    "    print(f\"--- Loading data from {feature_file_path} ---\")\n",
    "    with np.load(feature_file_path) as data:\n",
    "        labels = data['labels']\n",
    "        mel_spectrogram = data['mel_spectrogram']\n",
    "        mfcc = data['mfcc']\n",
    "        X = np.concatenate((mel_spectrogram, mfcc), axis=-1)\n",
    "        return X, labels\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# --- Model Architecture ---\n",
    "# =============================================================================\n",
    "\n",
    "@register_keras_serializable()\n",
    "class ParkinsonDetectorModel(Model):\n",
    "    def __init__(self, input_shape, **kwargs):\n",
    "        super(ParkinsonDetectorModel, self).__init__(**kwargs)\n",
    "        self.input_shape_config = input_shape\n",
    "\n",
    "        self.reshape_in = Reshape((input_shape[0], input_shape[1], 1))\n",
    "        self.conv1a = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv1b = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.pool1 = MaxPooling2D(5)\n",
    "        self.drop1 = Dropout(DROPOUT_RATE)\n",
    "        self.conv2a = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv2b = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same', name='last_conv_layer')\n",
    "        self.pool2 = MaxPooling2D(5, name='cnn_output')\n",
    "        self.drop2 = Dropout(DROPOUT_RATE)\n",
    "        self.flatten_cnn = Flatten()\n",
    "        self.attention = MultiHeadAttention(num_heads=2, key_dim=64, name='attention_output')\n",
    "        self.flatten_att = Flatten()\n",
    "        self.lstm1 = LSTM(128, return_sequences=True)\n",
    "        self.lstm2 = LSTM(128, return_sequences=False, name='lstm_output')\n",
    "        self.drop_lstm = Dropout(DROPOUT_RATE)\n",
    "        self.concat = Concatenate()\n",
    "        self.dense_bottleneck = Dense(128, activation='relu', name='bottleneck_features')\n",
    "        self.dense_output = Dense(1, activation='sigmoid')\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.reshape_in(inputs)\n",
    "        x = self.conv1a(x)\n",
    "        x = self.conv1b(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x, training=training)\n",
    "        x = self.conv2a(x)\n",
    "        x = self.conv2b(x)\n",
    "        cnn_branch_output = self.pool2(x)\n",
    "        x = self.drop2(cnn_branch_output, training=training)\n",
    "\n",
    "        cnn_flat = self.flatten_cnn(x)\n",
    "\n",
    "        shape = tf.shape(x)\n",
    "        sequence = tf.reshape(x, [-1, shape[1] * shape[2], shape[3]])\n",
    "\n",
    "        att_branch_output = self.attention(query=sequence, key=sequence, value=sequence)\n",
    "        att_flat = self.flatten_att(att_branch_output)\n",
    "\n",
    "        lstm_seq = self.lstm1(sequence)\n",
    "        lstm_branch_output = self.lstm2(lstm_seq)\n",
    "        lstm_out = self.drop_lstm(lstm_branch_output, training=training)\n",
    "\n",
    "        concatenated = self.concat([cnn_flat, att_flat, lstm_out])\n",
    "        bottleneck = self.dense_bottleneck(concatenated)\n",
    "        final_output = self.dense_output(bottleneck)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ParkinsonDetectorModel, self).get_config()\n",
    "        config.update({\"input_shape\": self.input_shape_config})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "def build_model(input_shape: tuple) -> Model:\n",
    "    \"\"\"Builds the hybrid model by wrapping the custom class in a Functional API model.\"\"\"\n",
    "    print(\"--- Building the model ---\")\n",
    "    inputs = Input(shape=input_shape)\n",
    "    parkinson_detector = ParkinsonDetectorModel(input_shape=input_shape)\n",
    "    outputs = parkinson_detector(inputs)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    print(\"Model built successfully.\")\n",
    "    return model\n",
    "\n",
    "# =============================================================================\n",
    "# --- Model Performance ---\n",
    "# =============================================================================\n",
    "def save_metrics_to_csv(y_true, y_pred_proba, filename=\"classification_report.csv\", threshold=0.5):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_binary = (np.array(y_pred_proba) > threshold).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    total_samples = cm.sum()\n",
    "    if total_samples == 0:\n",
    "        tn_percent, fp_percent, fn_percent, tp_percent = 0, 0, 0, 0\n",
    "    else:\n",
    "        tn_percent = (tn / total_samples) * 100\n",
    "        fp_percent = (fp / total_samples) * 100\n",
    "        fn_percent = (fn / total_samples) * 100\n",
    "        tp_percent = (tp / total_samples) * 100\n",
    "\n",
    "    precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
    "    sensitivity = recall\n",
    "\n",
    "    report_data = {\n",
    "        'Metric': [\n",
    "            'True Positive (TP)',\n",
    "            'True Negative (TN)',\n",
    "            'False Positive (FP)',\n",
    "            'False Negative (FN)',\n",
    "            'Precision',\n",
    "            'Recall (Sensitivity)',\n",
    "            'F1-Score'\n",
    "        ],\n",
    "        'Value': [\n",
    "            f\"{tp} ({tp_percent:.2f}%)\",\n",
    "            f\"{tn} ({tn_percent:.2f}%)\",\n",
    "            f\"{fp} ({fp_percent:.2f}%)\",\n",
    "            f\"{fn} ({fn_percent:.2f}%)\",\n",
    "            f\"{precision:.4f}\",\n",
    "            f\"{recall:.4f}\",\n",
    "            f\"{f1:.4f}\"\n",
    "        ]\n",
    "    }\n",
    "    df = pd.DataFrame(report_data)\n",
    "\n",
    "    try:\n",
    "        df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"The evaluation results is stored: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erorr while saving the evaluation report: {e}\")\n",
    "# =============================================================================\n",
    "# --- Model Explainability (SHAP & Grad-CAM) ---\n",
    "# =============================================================================\n"
   ],
   "id": "e0cb581eacb4685",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T07:13:56.280121Z",
     "start_time": "2025-09-13T07:13:56.246329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_feature_map_info(npz_file_path):\n",
    "    \"\"\"\n",
    "    Dynamically generates feature layout information (color mask, names, colors)\n",
    "    by inspecting the contents of an .npz file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(npz_file_path):\n",
    "        print(f\"Error: File not found at '{npz_file_path}'\")\n",
    "        return None\n",
    "\n",
    "    feature_layout = {}\n",
    "    print(f\"Inspecting file for feature map info: {npz_file_path}\")\n",
    "\n",
    "    with np.load(npz_file_path) as data:\n",
    "        ignore_keys = {'labels', 'sex', 'age', 'X', 'y'}\n",
    "        feature_keys = [key for key in data.files if key not in ignore_keys]\n",
    "\n",
    "        if not feature_keys:\n",
    "            print(\"Error: No feature arrays found in the .npz file.\")\n",
    "            return None\n",
    "\n",
    "        for key in sorted(feature_keys):\n",
    "            arr_shape = data[key].shape\n",
    "\n",
    "            if len(arr_shape) == 1:\n",
    "                num_rows = 1\n",
    "            elif len(arr_shape) >= 2:\n",
    "                num_rows = arr_shape[1]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            feature_layout[key] = num_rows\n",
    "\n",
    "    colors = plt.get_cmap('Paired', len(feature_layout))\n",
    "    feature_names = list(feature_layout.keys())\n",
    "    total_rows = sum(feature_layout.values())\n",
    "    time_steps_for_visual = total_rows\n",
    "\n",
    "    color_mask = np.zeros((total_rows, time_steps_for_visual), dtype=int)\n",
    "\n",
    "    current_row = 0\n",
    "    for i, (name, num_rows) in enumerate(feature_layout.items()):\n",
    "        color_mask[current_row : current_row + num_rows, :] = i\n",
    "        current_row += num_rows\n",
    "\n",
    "    legend_patches = [mpatches.Patch(color=colors(i), label=f\"{name} ({feature_layout[name]} rows)\")\n",
    "                      for i, name in enumerate(feature_names)]\n",
    "\n",
    "    return {\n",
    "        'color_mask': color_mask,\n",
    "        'feature_layout': feature_layout,\n",
    "        'feature_names': feature_names,\n",
    "        'colors': colors,\n",
    "        'legend_patches': legend_patches,\n",
    "        'total_rows': total_rows\n",
    "    }\n",
    "\n",
    "def run_full_shap_analysis(model, X_train, X_test, y_test, output_path, features_npz_path, samples_per_class = 50, top_n=20):\n",
    "    \"\"\"\n",
    "    Run SHAP analysis with balanced sample selection.\n",
    "    Generates plots with two side-by-side, perfectly SQUARE and equal-sized subplots.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running Full SHAP Analysis ---\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    feature_map_info = generate_feature_map_info(features_npz_path)\n",
    "    if feature_map_info is None: return\n",
    "\n",
    "    legend_patches = feature_map_info['legend_patches']\n",
    "    total_rows = feature_map_info['total_rows']\n",
    "\n",
    "    # Balanced sample selection logic...\n",
    "    healthy_indices = np.where(y_test == 0)[0]\n",
    "    parkinson_indices = np.where(y_test == 1)[0]\n",
    "    num_healthy_to_select = min(samples_per_class, len(healthy_indices))\n",
    "    num_parkinson_to_select = min(samples_per_class, len(parkinson_indices))\n",
    "    print(f\"Attempting to select {num_healthy_to_select} healthy and {num_parkinson_to_select} Parkinson's samples.\")\n",
    "    selected_healthy_indices = np.random.choice(healthy_indices, num_healthy_to_select, replace=False)\n",
    "    selected_parkinson_indices = np.random.choice(parkinson_indices, num_parkinson_to_select, replace=False)\n",
    "    final_indices = np.concatenate([selected_healthy_indices, selected_parkinson_indices])\n",
    "    np.random.shuffle(final_indices)\n",
    "    test_samples = X_test[final_indices]\n",
    "    y_true_samples = y_test[final_indices]\n",
    "\n",
    "    print(f\"Calculating SHAP values for {len(test_samples)} balanced samples...\")\n",
    "    explainer = shap.GradientExplainer(model, X_train[:50].astype(np.float32))\n",
    "    shap_values_list = []\n",
    "    for sample in tqdm(test_samples, desc=\"SHAP Progress\"):\n",
    "        sample_batch = np.expand_dims(sample, axis=0).astype(np.float32)\n",
    "        sv = explainer.shap_values(sample_batch)\n",
    "        if isinstance(sv, list): sv = sv[0]\n",
    "        shap_values_list.append(sv)\n",
    "    shap_values = np.vstack(shap_values_list)\n",
    "    print(f\"\\nSHAP values shape: {shap_values.shape}, Test samples shape: {test_samples.shape}\")\n",
    "    if shap_values.shape[1] != total_rows:\n",
    "        print(f\"⚠️ Warning: SHAP values feature rows ({shap_values.shape[1]}) do not match total_rows from NPZ ({total_rows}).\")\n",
    "    actual_data_time_steps = shap_values.shape[2]\n",
    "\n",
    "    # ... (Global Top-N Pixel Bar Plot code remains the same) ...\n",
    "    flat_shap = shap_values.reshape(shap_values.shape[0], -1)\n",
    "    mean_abs = np.mean(np.abs(flat_shap), axis=0)\n",
    "    top_idx = np.argsort(mean_abs)[::-1][:top_n]\n",
    "    coords = [np.unravel_index(i, (shap_values.shape[1], shap_values.shape[2])) for i in top_idx]\n",
    "    labels = [f\"T{t} F{f}\" for t, f in coords]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(top_idx)), mean_abs[top_idx])\n",
    "    plt.xticks(range(len(top_idx)), labels, rotation=45, ha=\"right\")\n",
    "    plt.title(f\"Top-{top_n} Global SHAP Features (time × frequency bins)\")\n",
    "    plt.xlabel(\"Time Bin × Frequency Bin\")\n",
    "    plt.ylabel(\"Mean |SHAP value|\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, \"shap_global_bar.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"-> Saved 'shap_global_bar.png'\")\n",
    "\n",
    "    def plot_aligned_heatmap(heatmap_data, title, filename_suffix, cmap, label, vmin=None, vmax=None):\n",
    "        \"\"\"\n",
    "        Plots the SHAP heatmap and Feature Map in two perfectly square, side-by-side subplots.\n",
    "        \"\"\"\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "        ax_shap, ax_feature_map = axes[0], axes[1]\n",
    "\n",
    "        # --- SHAP Heatmap (Left Subplot) ---\n",
    "        img = ax_shap.imshow(heatmap_data, cmap=cmap, aspect='auto', interpolation='nearest', vmin=vmin, vmax=vmax)\n",
    "        ax_shap.set_title(title, fontsize=12)\n",
    "        ax_shap.set_xlabel(f\"Time Steps (Actual: {actual_data_time_steps})\", fontsize=10)\n",
    "        ax_shap.set_ylabel(f\"Feature Rows ({total_rows})\", fontsize=10)\n",
    "\n",
    "        # This prevents the colorbar from shrinking the main plot.\n",
    "        divider = make_axes_locatable(ax_shap)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "        fig.colorbar(img, cax=cax, label=label)\n",
    "\n",
    "        # --- Feature Map (Right Subplot) ---\n",
    "        aligned_color_mask = np.zeros((total_rows, actual_data_time_steps), dtype=int)\n",
    "        current_row = 0\n",
    "        for i, (name, num_rows) in enumerate(feature_map_info['feature_layout'].items()):\n",
    "            original_idx = feature_map_info['feature_names'].index(name)\n",
    "            aligned_color_mask[current_row : current_row + num_rows, :] = original_idx\n",
    "            current_row += num_rows\n",
    "\n",
    "        ax_feature_map.imshow(aligned_color_mask, cmap=feature_map_info['colors'], aspect='auto', interpolation='nearest')\n",
    "        ax_feature_map.set_title(\"Feature Map\", fontsize=12)\n",
    "        ax_feature_map.set_xlabel(f\"Time Steps (Visual: {actual_data_time_steps})\", fontsize=10)\n",
    "        # The y-axis label is shared, so we can turn it off for the second plot for a cleaner look\n",
    "        ax_feature_map.tick_params(axis='y', labelleft=False)\n",
    "\n",
    "        ax_feature_map.legend(handles=legend_patches, loc='upper left', bbox_to_anchor=(1.02, 1), borderaxespad=0., fontsize=8)\n",
    "\n",
    "        fig.suptitle(f\"SHAP Analysis: {title}\", fontsize=16, fontweight='bold')\n",
    "\n",
    "        # Use standard tight_layout; figsize and dedicated colorbar axis handle the rest.\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95]) # rect adjusts for suptitle\n",
    "\n",
    "        plt.savefig(os.path.join(output_path, f\"shap_aligned_{filename_suffix}.png\"), dpi=300)\n",
    "        plt.close(fig)\n",
    "        print(f\"-> Saved 'shap_aligned_{filename_suffix}.png'\")\n",
    "\n",
    "    hc_mask, pd_mask = (y_true_samples == 0), (y_true_samples == 1)\n",
    "    # ... (The rest of the function that calls plot_aligned_heatmap remains the same) ...\n",
    "    if np.any(hc_mask):\n",
    "        hc_mean = shap_values[hc_mask].mean(axis=0).squeeze()\n",
    "        plot_aligned_heatmap(hc_mean, \"Average SHAP Heatmap - Healthy\", \"summary_healthy\", \"bwr\", \"Mean SHAP Value\")\n",
    "    if np.any(pd_mask):\n",
    "        pd_mean = shap_values[pd_mask].mean(axis=0).squeeze()\n",
    "        plot_aligned_heatmap(pd_mean, \"Average SHAP Heatmap - Parkinson\", \"summary_parkinson\", \"bwr\", \"Mean SHAP Value\")\n",
    "    if np.any(hc_mask) and np.any(pd_mask):\n",
    "        diff_map = pd_mean - hc_mean\n",
    "        max_abs_diff = np.max(np.abs(diff_map))\n",
    "        plot_aligned_heatmap(diff_map, \"SHAP Difference Heatmap (Parkinson - Healthy)\", \"difference\", \"seismic\", \"Δ SHAP (PD - HC)\", vmin=-max_abs_diff, vmax=max_abs_diff)\n",
    "    if np.any(hc_mask) and np.any(pd_mask) and np.sum(hc_mask) > 1 and np.sum(pd_mask) > 1:\n",
    "        hc_vals = shap_values[hc_mask].reshape(np.sum(hc_mask), -1)\n",
    "        pd_vals = shap_values[pd_mask].reshape(np.sum(pd_mask), -1)\n",
    "        t_stat, p_vals = ttest_ind(pd_vals, hc_vals, axis=0, equal_var=False)\n",
    "        p_map = p_vals.reshape(shap_values.shape[1], shap_values.shape[2])\n",
    "        plot_aligned_heatmap(p_map, \"Statistical Significance Map (PD vs HC)\", \"significance\", \"viridis_r\", \"p-value (t-test)\", vmin=0, vmax=0.05)\n",
    "    else:\n",
    "        print(\"Skipping significance map: Not enough samples for t-test.\")\n",
    "\n",
    "    print(\"\\n--- SHAP Analysis Complete ---\")\n",
    "\n",
    "def run_gradcam_analysis(model, X_test, y_test, output_path, FEATURES_FILE_PATH, num_samples = 50):\n",
    "    \"\"\"\n",
    "    Run Grad-CAM analysis and plots the average heatmaps alongside an\n",
    "    architectural feature map derived from the model's layers.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running Grad-CAM Analysis with Architectural Feature Map ---\")\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    parkinson_detector = None\n",
    "    for layer in model.layers:\n",
    "        if 'ParkinsonDetectorModel' in str(type(layer)):\n",
    "            parkinson_detector = layer\n",
    "            break\n",
    "\n",
    "    if parkinson_detector is None:\n",
    "        print(\"ParkinsonDetectorModel not found in the model layers.\")\n",
    "        return\n",
    "\n",
    "    print(\"Determining feature sizes from model architecture...\")\n",
    "\n",
    "    input_tensor = model.inputs[0]\n",
    "\n",
    "    # Manually pass the input through the layers of the custom model\n",
    "    x = parkinson_detector.reshape_in(input_tensor)\n",
    "    x = parkinson_detector.conv1a(x)\n",
    "    x = parkinson_detector.conv1b(x)\n",
    "    x = parkinson_detector.pool1(x)\n",
    "    x = parkinson_detector.drop1(x, training=False)\n",
    "    x = parkinson_detector.conv2a(x)\n",
    "    x = parkinson_detector.conv2b(x)\n",
    "    cnn_branch_output = parkinson_detector.pool2(x)\n",
    "\n",
    "    _, h, w, c = cnn_branch_output.shape\n",
    "    target_shape = (h * w, c)\n",
    "\n",
    "    sequence = Reshape(target_shape)(cnn_branch_output)\n",
    "\n",
    "    att_branch_output = parkinson_detector.attention(query=sequence, key=sequence, value=sequence)\n",
    "\n",
    "    lstm_seq = parkinson_detector.lstm1(sequence)\n",
    "    lstm_branch_output = parkinson_detector.lstm2(lstm_seq)\n",
    "\n",
    "    feature_size_model = Model(\n",
    "        inputs=model.inputs,\n",
    "        outputs=[cnn_branch_output, att_branch_output, lstm_branch_output]\n",
    "    )\n",
    "\n",
    "    cnn_out_shape, att_out_shape, lstm_out_shape = feature_size_model.predict(X_test[:1], verbose=0)\n",
    "\n",
    "    len_cnn_features = cnn_out_shape.shape[1] * cnn_out_shape.shape[2] * cnn_out_shape.shape[3]\n",
    "    len_att_features = att_out_shape.shape[1] * att_out_shape.shape[2]\n",
    "    len_lstm_features = lstm_out_shape.shape[1]\n",
    "\n",
    "    total_features = len_cnn_features + len_att_features + len_lstm_features\n",
    "    print(f\"  - CNN features: {len_cnn_features}\")\n",
    "    print(f\"  - Attention features: {len_att_features}\")\n",
    "    print(f\"  - LSTM features: {len_lstm_features}\")\n",
    "    print(f\"  - Total concatenated features: {total_features}\")\n",
    "\n",
    "    last_conv_layer = parkinson_detector.conv2b\n",
    "    print(f\"Using last conv layer: {last_conv_layer.name}\")\n",
    "\n",
    "    def get_conv_and_output(inputs):\n",
    "        x = parkinson_detector.reshape_in(inputs)\n",
    "        x = parkinson_detector.conv1a(x)\n",
    "        x = parkinson_detector.conv1b(x)\n",
    "        x = parkinson_detector.pool1(x)\n",
    "        x = parkinson_detector.drop1(x, training=False)\n",
    "        x = parkinson_detector.conv2a(x)\n",
    "        conv_output = parkinson_detector.conv2b(x)\n",
    "        x = parkinson_detector.pool2(conv_output)\n",
    "        x = parkinson_detector.drop2(x, training=False)\n",
    "        cnn_flat = parkinson_detector.flatten_cnn(x)\n",
    "        shape = tf.shape(x)\n",
    "        sequence = tf.reshape(x, [-1, shape[1] * shape[2], shape[3]])\n",
    "        att_out = parkinson_detector.attention(query=sequence, key=sequence, value=sequence)\n",
    "        att_flat = parkinson_detector.flatten_att(att_out)\n",
    "        lstm_seq = parkinson_detector.lstm1(sequence)\n",
    "        lstm_out = parkinson_detector.lstm2(lstm_seq)\n",
    "        lstm_out = parkinson_detector.drop_lstm(lstm_out, training=False)\n",
    "        concatenated = parkinson_detector.concat([cnn_flat, att_flat, lstm_out])\n",
    "        bottleneck = parkinson_detector.dense_bottleneck(concatenated)\n",
    "        final_output = parkinson_detector.dense_output(bottleneck)\n",
    "        return conv_output, final_output\n",
    "\n",
    "    y_pred_probs = model.predict(X_test, verbose=0)\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "    parkinson_indices = np.where(y_test == 1)[0]\n",
    "    healthy_indices = np.where(y_test == 0)[0]\n",
    "\n",
    "    selected_pd_indices = list(np.random.choice(parkinson_indices, min(num_samples, len(parkinson_indices)), replace=False))\n",
    "    selected_hc_indices = list(np.random.choice(healthy_indices, min(num_samples, len(healthy_indices)), replace=False))\n",
    "\n",
    "\n",
    "    print(f\"Selected {len(selected_pd_indices)} TP and {len(selected_hc_indices)} TN samples for Grad-CAM.\")\n",
    "\n",
    "    # --- Calculating heatmaps for True Positives ---\n",
    "    tp_heatmaps = []\n",
    "    for i in selected_pd_indices:\n",
    "        img = X_test[i:i+1]\n",
    "        with tf.GradientTape() as tape:\n",
    "            img_tensor = tf.cast(img, tf.float32)\n",
    "            tape.watch(img_tensor)\n",
    "            conv_outputs, preds = get_conv_and_output(img_tensor)\n",
    "            loss = preds[:, 0]\n",
    "        grads = tape.gradient(loss, conv_outputs)\n",
    "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "        conv_outputs_np = conv_outputs[0].numpy()\n",
    "        pooled_grads_np = pooled_grads.numpy()\n",
    "        heatmap = np.zeros(conv_outputs_np.shape[:-1])\n",
    "        for j in range(conv_outputs_np.shape[-1]):\n",
    "            heatmap += pooled_grads_np[j] * conv_outputs_np[:, :, j]\n",
    "        heatmap = np.maximum(heatmap, 0)\n",
    "        heatmap /= (heatmap.max() + 1e-10)\n",
    "        tp_heatmaps.append(heatmap)\n",
    "\n",
    "    # --- Calculating heatmaps for True Negatives ---\n",
    "    tn_heatmaps = []\n",
    "    for i in selected_hc_indices:\n",
    "        img = X_test[i:i+1]\n",
    "        with tf.GradientTape() as tape:\n",
    "            img_tensor = tf.cast(img, tf.float32)\n",
    "            tape.watch(img_tensor)\n",
    "            conv_outputs, preds = get_conv_and_output(img_tensor)\n",
    "            loss = preds[:, 0]\n",
    "        grads = tape.gradient(loss, conv_outputs)\n",
    "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "        conv_outputs_np = conv_outputs[0].numpy()\n",
    "        pooled_grads_np = pooled_grads.numpy()\n",
    "        heatmap = np.zeros(conv_outputs_np.shape[:-1])\n",
    "        for j in range(conv_outputs_np.shape[-1]):\n",
    "            heatmap += pooled_grads_np[j] * conv_outputs_np[:, :, j]\n",
    "        heatmap = np.maximum(heatmap, 0)\n",
    "        heatmap /= (heatmap.max() + 1e-10)\n",
    "        tn_heatmaps.append(heatmap)\n",
    "\n",
    "    avg_pd_heatmap = np.mean(tp_heatmaps, axis=0) if tp_heatmaps else np.zeros((X_test.shape[1], X_test.shape[2]))\n",
    "    avg_hc_heatmap = np.mean(tn_heatmaps, axis=0) if tn_heatmaps else np.zeros((X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "    # --- Get original input dimensions ---\n",
    "    original_feature_rows, original_time_steps = X_test.shape[1], X_test.shape[2]\n",
    "\n",
    "    # --- Resize heatmaps to original input dimensions ---\n",
    "    upscaled_avg_pd_heatmap = resize(avg_pd_heatmap,\n",
    "                                     (original_feature_rows, original_time_steps),\n",
    "                                     order=3, mode='reflect', anti_aliasing=True)\n",
    "    upscaled_avg_hc_heatmap = resize(avg_hc_heatmap,\n",
    "                                     (original_feature_rows, original_time_steps),\n",
    "                                     order=3, mode='reflect', anti_aliasing=True)\n",
    "\n",
    "    # --- Select a representative sample for the background ---\n",
    "    sample_input_pd = X_test[selected_pd_indices[0]] if selected_pd_indices else None\n",
    "    sample_input_hc = X_test[selected_hc_indices[0]] if selected_hc_indices else None\n",
    "\n",
    "    def normalize_for_display(img_data):\n",
    "        if img_data is None: return None\n",
    "        img_data = np.squeeze(img_data) # Remove channel dims\n",
    "        return (img_data - img_data.min()) / (img_data.max() - img_data.min() + 1e-10)\n",
    "\n",
    "    normalized_input_pd = normalize_for_display(sample_input_pd)\n",
    "    normalized_input_hc = normalize_for_display(sample_input_hc)\n",
    "\n",
    "    # --- Plotting: All 5 plots in a single row ---\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(25, 6),\n",
    "                             gridspec_kw={'width_ratios': [1, 1, 1, 1, 1]})  # <-- all equal now\n",
    "    fig.suptitle(\"Grad-CAM: Model Attention, Input, and Feature Layout\", fontsize=18, fontweight='bold')\n",
    "\n",
    "    # --- Parkinson's - Original Input ---\n",
    "    ax_pd_input = axes[0]\n",
    "    if normalized_input_pd is not None:\n",
    "        ax_pd_input.imshow(normalized_input_pd, cmap='gray', aspect='auto', origin='lower')\n",
    "        ax_pd_input.set_title(f'Parkinson\\'s Input\\n(Sample {selected_pd_indices[0] if selected_pd_indices else \"\"})')\n",
    "        ax_pd_input.set_xlabel(\"Time Steps\")\n",
    "        ax_pd_input.set_ylabel(\"Feature Rows\")\n",
    "    else:\n",
    "        ax_pd_input.text(0.5, 0.5, 'No PD samples', ha='center', va='center', transform=ax_pd_input.transAxes)\n",
    "\n",
    "    # --- Parkinson's - Upscaled Heatmap ---\n",
    "    ax_pd_heatmap = axes[1]\n",
    "    im_pd_hm = ax_pd_heatmap.imshow(upscaled_avg_pd_heatmap, cmap='jet', aspect='auto', origin='lower')\n",
    "    ax_pd_heatmap.set_title(f'Avg. PD Attention\\n({len(selected_pd_indices)} samples)')\n",
    "    ax_pd_heatmap.set_xlabel(\"Time Steps\")\n",
    "    ax_pd_heatmap.set_yticklabels([])\n",
    "    divider_pd = make_axes_locatable(ax_pd_heatmap)\n",
    "    cax_pd = divider_pd.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(im_pd_hm, cax=cax_pd)\n",
    "\n",
    "    # --- Healthy - Original Input ---\n",
    "    ax_hc_input = axes[2]\n",
    "    if normalized_input_hc is not None:\n",
    "        ax_hc_input.imshow(normalized_input_hc, cmap='gray', aspect='auto', origin='lower')\n",
    "        ax_hc_input.set_title(f'Healthy Input\\n(Sample {selected_hc_indices[0] if selected_hc_indices else \"\"})')\n",
    "        ax_hc_input.set_xlabel(\"Time Steps\")\n",
    "        ax_hc_input.set_yticklabels([])\n",
    "    else:\n",
    "        ax_hc_input.text(0.5, 0.5, 'No HC samples', ha='center', va='center', transform=ax_hc_input.transAxes)\n",
    "\n",
    "    # --- Healthy - Upscaled Heatmap ---\n",
    "    ax_hc_heatmap = axes[3]\n",
    "    im_hc_hm = ax_hc_heatmap.imshow(upscaled_avg_hc_heatmap, cmap='jet', aspect='auto', origin='lower')\n",
    "    ax_hc_heatmap.set_title(f'Avg. Healthy Attention\\n({len(selected_hc_indices)} samples)')\n",
    "    ax_hc_heatmap.set_xlabel(\"Time Steps\")\n",
    "    ax_hc_heatmap.set_yticklabels([])\n",
    "    divider_hc = make_axes_locatable(ax_hc_heatmap)\n",
    "    cax_hc = divider_hc.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(im_hc_hm, cax=cax_hc)\n",
    "\n",
    "\n",
    "     # --- Feature Map as the 5th plot ---\n",
    "    ax_feature_map = axes[4]\n",
    "    feature_map_info = generate_feature_map_info(FEATURES_FILE_PATH)\n",
    "    if feature_map_info:\n",
    "        legend_colors = [patch.get_facecolor() for patch in feature_map_info['legend_patches']]\n",
    "        cmap = ListedColormap(legend_colors)\n",
    "\n",
    "        # --- 1. Transpose the data ---\n",
    "        transposed_data = feature_map_info['color_mask'].T\n",
    "\n",
    "        im_feat = ax_feature_map.imshow(\n",
    "            transposed_data,  # Use the transposed data here\n",
    "            cmap=cmap,\n",
    "            aspect='auto',\n",
    "            interpolation='nearest',\n",
    "            origin='lower'\n",
    "        )\n",
    "        ax_feature_map.set_title(\"Feature\\nLayout\")\n",
    "\n",
    "        # --- 2. Swap the axis labels ---\n",
    "        ax_feature_map.set_ylabel(\"Time Steps\") # The old x-label is now the y-label\n",
    "        ax_feature_map.set_xlabel(\"Features\")   # Add a new label for the x-axis\n",
    "        ax_feature_map.set_xticklabels([])      # The old y-tick setting is now for the x-axis\n",
    "\n",
    "        # --- Legend OUTSIDE the entire figure ---\n",
    "        fig.legend(\n",
    "            handles=feature_map_info['legend_patches'],\n",
    "            loc='center left',\n",
    "            bbox_to_anchor=(0.93, 0.5),\n",
    "            borderaxespad=0.\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        ax_feature_map.set_visible(False)\n",
    "\n",
    "    # --- Final Save ---\n",
    "    plt.tight_layout(rect=[0, 0, 0.93, 0.93])  # leave space on right for legend\n",
    "\n",
    "    save_path = os.path.join(output_path, \"gradcam_full_comparison_row.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")  # ensure legend is not cut\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved full Grad-CAM comparison to {save_path}\")\n"
   ],
   "id": "b1406f4bc7124f9a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T07:15:52.597557Z",
     "start_time": "2025-09-13T07:13:56.286768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- Main Execution ---\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    X, y= load_data(FEATURES_FILE_PATH)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(f\"\\nData split into training ({len(y_train)}) and testing ({len(y_test)}) sets.\")\n",
    "\n",
    "    model = build_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    model.summary()\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "    print(\"\\n--- Starting model training ---\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[checkpoint_cb],\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"--- Model training finished ---\")\n",
    "\n",
    "    pd.DataFrame(history.history).to_csv(HISTORY_SAVE_PATH, index_label='epoch')\n",
    "    print(f\"\\nTraining history saved to '{HISTORY_SAVE_PATH}'\")\n",
    "\n"
   ],
   "id": "2845d82bc4a4775",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading data from D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\data\\features_A_DEFAULT.npz ---\n",
      "\n",
      "Data split into training (262) and testing (66) sets.\n",
      "--- Building the model ---\n",
      "WARNING:tensorflow:From C:\\Users\\bahar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Model built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"functional\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001B[38;5;33mInputLayer\u001B[0m)        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m30\u001B[0m, \u001B[38;5;34m188\u001B[0m)        │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ parkinson_detector_model        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │       \u001B[38;5;34m704,001\u001B[0m │\n",
       "│ (\u001B[38;5;33mParkinsonDetectorModel\u001B[0m)        │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">188</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ parkinson_detector_model        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">704,001</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ParkinsonDetectorModel</span>)        │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m704,001\u001B[0m (2.69 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">704,001</span> (2.69 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m704,001\u001B[0m (2.69 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">704,001</span> (2.69 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting model training ---\n",
      "Epoch 1/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 340ms/step - accuracy: 0.5484 - auc: 0.5569 - loss: 10.3337\n",
      "Epoch 1: val_auc improved from None to 0.54959, saving model to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\cnn_att_lstm\\best_model.keras\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 506ms/step - accuracy: 0.5611 - auc: 0.5636 - loss: 7.6239 - val_accuracy: 0.5000 - val_auc: 0.5496 - val_loss: 3.1277\n",
      "Epoch 2/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 321ms/step - accuracy: 0.5712 - auc: 0.6288 - loss: 3.2314\n",
      "Epoch 2: val_auc improved from 0.54959 to 0.67034, saving model to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\cnn_att_lstm\\best_model.keras\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 363ms/step - accuracy: 0.5763 - auc: 0.5972 - loss: 3.0581 - val_accuracy: 0.5758 - val_auc: 0.6703 - val_loss: 2.7463\n",
      "Epoch 3/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 359ms/step - accuracy: 0.5610 - auc: 0.6077 - loss: 2.8475\n",
      "Epoch 3: val_auc did not improve from 0.67034\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 389ms/step - accuracy: 0.6069 - auc: 0.6226 - loss: 2.8190 - val_accuracy: 0.5303 - val_auc: 0.6373 - val_loss: 2.6591\n",
      "Epoch 4/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 310ms/step - accuracy: 0.6236 - auc: 0.6713 - loss: 2.7798\n",
      "Epoch 4: val_auc improved from 0.67034 to 0.75895, saving model to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\cnn_att_lstm\\best_model.keras\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 356ms/step - accuracy: 0.6183 - auc: 0.6518 - loss: 2.7574 - val_accuracy: 0.6212 - val_auc: 0.7590 - val_loss: 2.5834\n",
      "Epoch 5/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 294ms/step - accuracy: 0.6196 - auc: 0.6389 - loss: 2.6665\n",
      "Epoch 5: val_auc improved from 0.75895 to 0.77181, saving model to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\cnn_att_lstm\\best_model.keras\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 335ms/step - accuracy: 0.6221 - auc: 0.6458 - loss: 2.6574 - val_accuracy: 0.6364 - val_auc: 0.7718 - val_loss: 2.5566\n",
      "Epoch 6/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 279ms/step - accuracy: 0.6089 - auc: 0.6571 - loss: 2.6468\n",
      "Epoch 6: val_auc did not improve from 0.77181\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 306ms/step - accuracy: 0.5840 - auc: 0.6298 - loss: 2.7077 - val_accuracy: 0.5909 - val_auc: 0.7199 - val_loss: 2.5544\n",
      "Epoch 7/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 277ms/step - accuracy: 0.5997 - auc: 0.6383 - loss: 2.6087\n",
      "Epoch 7: val_auc did not improve from 0.77181\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 308ms/step - accuracy: 0.6069 - auc: 0.6269 - loss: 2.5924 - val_accuracy: 0.5909 - val_auc: 0.7548 - val_loss: 2.5314\n",
      "Epoch 8/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 281ms/step - accuracy: 0.5937 - auc: 0.5951 - loss: 2.5690\n",
      "Epoch 8: val_auc improved from 0.77181 to 0.78880, saving model to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\cnn_att_lstm\\best_model.keras\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 324ms/step - accuracy: 0.5954 - auc: 0.6118 - loss: 2.5558 - val_accuracy: 0.7273 - val_auc: 0.7888 - val_loss: 2.5074\n",
      "Epoch 9/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 287ms/step - accuracy: 0.6437 - auc: 0.6565 - loss: 2.4941\n",
      "Epoch 9: val_auc did not improve from 0.78880\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 315ms/step - accuracy: 0.6183 - auc: 0.6341 - loss: 2.5031 - val_accuracy: 0.6212 - val_auc: 0.7865 - val_loss: 2.4830\n",
      "Epoch 10/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 297ms/step - accuracy: 0.6468 - auc: 0.6736 - loss: 2.4963\n",
      "Epoch 10: val_auc did not improve from 0.78880\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 331ms/step - accuracy: 0.6641 - auc: 0.6716 - loss: 2.4859 - val_accuracy: 0.6818 - val_auc: 0.7231 - val_loss: 2.4609\n",
      "Epoch 11/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 301ms/step - accuracy: 0.6856 - auc: 0.7249 - loss: 2.4344\n",
      "Epoch 11: val_auc did not improve from 0.78880\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 332ms/step - accuracy: 0.6832 - auc: 0.7058 - loss: 2.4391 - val_accuracy: 0.6818 - val_auc: 0.7346 - val_loss: 2.4133\n",
      "Epoch 12/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 313ms/step - accuracy: 0.6155 - auc: 0.6900 - loss: 2.4571\n",
      "Epoch 12: val_auc did not improve from 0.78880\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 347ms/step - accuracy: 0.6107 - auc: 0.6945 - loss: 2.4506 - val_accuracy: 0.6364 - val_auc: 0.7507 - val_loss: 2.3748\n",
      "Epoch 13/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 308ms/step - accuracy: 0.6763 - auc: 0.7620 - loss: 2.3518\n",
      "Epoch 13: val_auc did not improve from 0.78880\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 340ms/step - accuracy: 0.6527 - auc: 0.7293 - loss: 2.3840 - val_accuracy: 0.6212 - val_auc: 0.7415 - val_loss: 2.3563\n",
      "Epoch 14/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 647ms/step - accuracy: 0.6502 - auc: 0.6930 - loss: 2.3765\n",
      "Epoch 14: val_auc did not improve from 0.78880\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 693ms/step - accuracy: 0.6489 - auc: 0.7131 - loss: 2.3701 - val_accuracy: 0.6818 - val_auc: 0.7796 - val_loss: 2.3248\n",
      "Epoch 15/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 623ms/step - accuracy: 0.6926 - auc: 0.7492 - loss: 2.2865\n",
      "Epoch 15: val_auc did not improve from 0.78880\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 663ms/step - accuracy: 0.6718 - auc: 0.7246 - loss: 2.2941 - val_accuracy: 0.6515 - val_auc: 0.7567 - val_loss: 2.3045\n",
      "Epoch 16/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 516ms/step - accuracy: 0.6737 - auc: 0.7368 - loss: 2.3008\n",
      "Epoch 16: val_auc did not improve from 0.78880\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 548ms/step - accuracy: 0.6565 - auc: 0.7294 - loss: 2.2989 - val_accuracy: 0.6667 - val_auc: 0.7856 - val_loss: 2.2569\n",
      "Epoch 17/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 314ms/step - accuracy: 0.7223 - auc: 0.8200 - loss: 2.2057\n",
      "Epoch 17: val_auc did not improve from 0.78880\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 352ms/step - accuracy: 0.7023 - auc: 0.7840 - loss: 2.2183 - val_accuracy: 0.7424 - val_auc: 0.7883 - val_loss: 2.2043\n",
      "Epoch 18/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 358ms/step - accuracy: 0.7354 - auc: 0.8161 - loss: 2.1592\n",
      "Epoch 18: val_auc improved from 0.78880 to 0.81405, saving model to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\cnn_att_lstm\\best_model.keras\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 410ms/step - accuracy: 0.7290 - auc: 0.8025 - loss: 2.1565 - val_accuracy: 0.7424 - val_auc: 0.8140 - val_loss: 2.1513\n",
      "Epoch 19/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 307ms/step - accuracy: 0.7382 - auc: 0.8218 - loss: 2.1232\n",
      "Epoch 19: val_auc did not improve from 0.81405\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 343ms/step - accuracy: 0.7176 - auc: 0.8029 - loss: 2.1315 - val_accuracy: 0.6970 - val_auc: 0.8058 - val_loss: 2.1324\n",
      "Epoch 20/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 330ms/step - accuracy: 0.7874 - auc: 0.8376 - loss: 2.0850\n",
      "Epoch 20: val_auc did not improve from 0.81405\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 368ms/step - accuracy: 0.7443 - auc: 0.8031 - loss: 2.1027 - val_accuracy: 0.6818 - val_auc: 0.7567 - val_loss: 2.1538\n",
      "Epoch 21/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 396ms/step - accuracy: 0.7146 - auc: 0.7823 - loss: 2.1163\n",
      "Epoch 21: val_auc did not improve from 0.81405\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 588ms/step - accuracy: 0.7252 - auc: 0.7929 - loss: 2.1030 - val_accuracy: 0.6818 - val_auc: 0.7916 - val_loss: 2.0720\n",
      "Epoch 22/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 410ms/step - accuracy: 0.7986 - auc: 0.8703 - loss: 1.9950\n",
      "Epoch 22: val_auc did not improve from 0.81405\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 444ms/step - accuracy: 0.8053 - auc: 0.8803 - loss: 1.9869 - val_accuracy: 0.6667 - val_auc: 0.7259 - val_loss: 2.1799\n",
      "Epoch 23/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 375ms/step - accuracy: 0.7510 - auc: 0.8254 - loss: 2.0361\n",
      "Epoch 23: val_auc did not improve from 0.81405\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 409ms/step - accuracy: 0.7634 - auc: 0.8238 - loss: 2.0120 - val_accuracy: 0.6970 - val_auc: 0.7792 - val_loss: 2.1141\n",
      "Epoch 24/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 374ms/step - accuracy: 0.7768 - auc: 0.8562 - loss: 1.9775\n",
      "Epoch 24: val_auc improved from 0.81405 to 0.83104, saving model to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\cnn_att_lstm\\best_model.keras\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 425ms/step - accuracy: 0.7443 - auc: 0.8123 - loss: 2.0006 - val_accuracy: 0.7273 - val_auc: 0.8310 - val_loss: 1.9690\n",
      "Epoch 25/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 380ms/step - accuracy: 0.8039 - auc: 0.8830 - loss: 1.9161\n",
      "Epoch 25: val_auc improved from 0.83104 to 0.85859, saving model to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\cnn_att_lstm\\best_model.keras\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 445ms/step - accuracy: 0.7519 - auc: 0.8305 - loss: 1.9916 - val_accuracy: 0.6970 - val_auc: 0.8586 - val_loss: 1.9702\n",
      "Epoch 26/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 330ms/step - accuracy: 0.7826 - auc: 0.9317 - loss: 1.8630\n",
      "Epoch 26: val_auc did not improve from 0.85859\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 363ms/step - accuracy: 0.7786 - auc: 0.8862 - loss: 1.8895 - val_accuracy: 0.7576 - val_auc: 0.8260 - val_loss: 1.9420\n",
      "Epoch 27/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 305ms/step - accuracy: 0.7798 - auc: 0.8551 - loss: 1.9124\n",
      "Epoch 27: val_auc did not improve from 0.85859\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 339ms/step - accuracy: 0.7863 - auc: 0.8600 - loss: 1.8959 - val_accuracy: 0.7576 - val_auc: 0.8352 - val_loss: 1.9017\n",
      "Epoch 28/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 305ms/step - accuracy: 0.8158 - auc: 0.9213 - loss: 1.7791\n",
      "Epoch 28: val_auc improved from 0.85859 to 0.85905, saving model to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\cnn_att_lstm\\best_model.keras\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 356ms/step - accuracy: 0.8206 - auc: 0.9257 - loss: 1.7555 - val_accuracy: 0.7576 - val_auc: 0.8590 - val_loss: 1.8453\n",
      "Epoch 29/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 322ms/step - accuracy: 0.8483 - auc: 0.9133 - loss: 1.7418\n",
      "Epoch 29: val_auc did not improve from 0.85905\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 357ms/step - accuracy: 0.8397 - auc: 0.9103 - loss: 1.7376 - val_accuracy: 0.7273 - val_auc: 0.8264 - val_loss: 1.8658\n",
      "Epoch 30/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 328ms/step - accuracy: 0.8041 - auc: 0.9086 - loss: 1.7303\n",
      "Epoch 30: val_auc did not improve from 0.85905\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 365ms/step - accuracy: 0.8015 - auc: 0.8996 - loss: 1.7369 - val_accuracy: 0.7273 - val_auc: 0.8439 - val_loss: 1.8147\n",
      "--- Model training finished ---\n",
      "\n",
      "Training history saved to 'D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\cnn_att_lstm\\history.csv'\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T07:15:55.902249Z",
     "start_time": "2025-09-13T07:15:54.652226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    print(\"\\n--- Start evaluating modrl ---\")\n",
    "    y_pred_probabilities = model.predict(X_test)\n",
    "    save_metrics_to_csv(y_test, y_pred_probabilities, EVALUATION_FILE_PATH)\n"
   ],
   "id": "13d0277c7b67f77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Start evaluating modrl ---\n",
      "\u001B[1m3/3\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 275ms/step\n",
      "The evaluation results is stored: D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\cnn_att_lstm\\evaluation.csv\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T07:17:20.760812Z",
     "start_time": "2025-09-13T07:15:55.909298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    if os.path.exists(BEST_MODEL_PATH):\n",
    "        print(\"\\n--- Loading best saved model for explainability analysis ---\")\n",
    "        best_model = load_model(BEST_MODEL_PATH, custom_objects={'ParkinsonDetectorModel': ParkinsonDetectorModel})\n",
    "\n",
    "        run_full_shap_analysis(model, X_train, X_test, y_test, SHAP_OUTPUT_PATH, FEATURES_FILE_PATH, 50, 20)\n"
   ],
   "id": "2ea7ae69825793f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading best saved model for explainability analysis ---\n",
      "\n",
      "--- Running Full SHAP Analysis ---\n",
      "Inspecting file for feature map info: D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\data\\features_A_DEFAULT.npz\n",
      "Attempting to select 33 healthy and 33 Parkinson's samples.\n",
      "Calculating SHAP values for 66 balanced samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SHAP Progress:   0%|          | 0/66 [00:00<?, ?it/s]C:\\Users\\bahar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor\n",
      "Received: inputs=['Tensor(shape=(1, 30, 188))']\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\bahar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor\n",
      "Received: inputs=['Tensor(shape=(50, 30, 188))']\n",
      "  warnings.warn(msg)\n",
      "SHAP Progress: 100%|██████████| 66/66 [01:19<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SHAP values shape: (66, 30, 188, 1), Test samples shape: (66, 30, 188)\n",
      "⚠️ Warning: SHAP values feature rows (30) do not match total_rows from NPZ (60).\n",
      "-> Saved 'shap_global_bar.png'\n",
      "-> Saved 'shap_aligned_summary_healthy.png'\n",
      "-> Saved 'shap_aligned_summary_parkinson.png'\n",
      "-> Saved 'shap_aligned_difference.png'\n",
      "-> Saved 'shap_aligned_significance.png'\n",
      "\n",
      "--- SHAP Analysis Complete ---\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T07:17:31.251505Z",
     "start_time": "2025-09-13T07:17:20.811485Z"
    }
   },
   "cell_type": "code",
   "source": "    run_gradcam_analysis(best_model, X_test, y_test, GRADCAM_OUTPUT_PATH, FEATURES_FILE_PATH, 50)",
   "id": "bbbcc6503553228c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Grad-CAM Analysis with Architectural Feature Map ---\n",
      "Determining feature sizes from model architecture...\n",
      "  - CNN features: 448\n",
      "  - Attention features: 448\n",
      "  - LSTM features: 128\n",
      "  - Total concatenated features: 1024\n",
      "Using last conv layer: last_conv_layer\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001EDB6E3AF20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Selected 33 TP and 33 TN samples for Grad-CAM.\n",
      "Inspecting file for feature map info: D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\data\\features_A_DEFAULT.npz\n",
      "Saved full Grad-CAM comparison to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\cnn_att_lstm\\gradcam_analysis\\gradcam_full_comparison_row.png\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T07:17:31.272528Z",
     "start_time": "2025-09-13T07:17:31.268477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    else:\n",
    "            print(\"\\nCould not find best model file. Skipping SHAP and Grad-CAM analysis.\")"
   ],
   "id": "6ebb1aae4d9b58e",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1879175016.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[31m    \u001B[39m\u001B[31melse:\u001B[39m\n    ^\n\u001B[31mSyntaxError\u001B[39m\u001B[31m:\u001B[39m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T07:17:31.289148800Z",
     "start_time": "2025-09-12T19:49:12.339007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "# --- 1. Audio and Feature Extraction Parameters ---\n",
    "# These parameters must match the ones used during model training.\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION_S = 3\n",
    "AUDIO_SAMPLES = SAMPLE_RATE * DURATION_S\n",
    "N_MELS = 30\n",
    "N_MFCC = 30\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "\n",
    "EXPECTED_FRAMES = int(np.ceil(AUDIO_SAMPLES / HOP_LENGTH))\n",
    "\n",
    "# --- 2. Paths to Model and Data ---\n",
    "model_path = BEST_MODEL_PATH\n",
    "folder_a_path = r\"D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\mPower\\data\\original_A_DEFAULT\\healthy_control\"\n",
    "folder_b_path = r\"D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\mPower\\data\\original_A_DEFAULT\\parkinson_patient\"\n",
    "\n",
    "# --- 3. Load the Model ---\n",
    "try:\n",
    "    model = keras.models.load_model(model_path)\n",
    "    print(\"Model loaded successfully! ✅\")\n",
    "    model.summary()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "def process_and_predict(file_path, model):\n",
    "    \"\"\"\n",
    "    Loads a single WAV file, extracts both Mel Spectrogram and MFCC features,\n",
    "    and makes a prediction using the model.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "\n",
    "        # Extract Mel spectrogram\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
    "        mel_spectrogram = librosa.util.fix_length(mel_spectrogram, size=EXPECTED_FRAMES, axis=1)\n",
    "\n",
    "        y_preemp = librosa.effects.preemphasis(audio, coef=0.5)\n",
    "        # Match time dimension\n",
    "        mfccs = librosa.feature.mfcc(y=y_preemp, sr=sr, n_mfcc=N_MFCC, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        mfccs = librosa.util.fix_length(mfccs, size=EXPECTED_FRAMES, axis=1)\n",
    "\n",
    "        # Stack them: (n_mfcc + n_mel, frames)\n",
    "        input_data = np.concatenate((mel_spectrogram, mfccs), axis=1)\n",
    "        input_data = np.expand_dims(input_data, axis=0)\n",
    "        # Make prediction\n",
    "        prediction = model.predict(input_data, verbose=0)\n",
    "        predicted_label = 1 if prediction[0][0] > 0.5 else 0\n",
    "\n",
    "        return predicted_label\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_files_in_folder(folder_path, model, true_label):\n",
    "    \"\"\"\n",
    "    Iterates through a folder, tests each WAV file, and collects true and predicted labels.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return [], []\n",
    "\n",
    "    file_list = [f for f in os.listdir(folder_path) if f.endswith('.wav')]\n",
    "    print(f\"Found {len(file_list)} WAV files in {folder_path}. Processing all of them...\")\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        predicted_label = process_and_predict(file_path, model)\n",
    "        if predicted_label is not None:\n",
    "            true_labels.append(true_label)\n",
    "            predicted_labels.append(predicted_label)\n",
    "\n",
    "    return predicted_labels, true_labels\n",
    "\n",
    "# --- 4. Main Evaluation Logic ---\n",
    "print(\"\\n--- Starting Model Evaluation ---\")\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Test 'healthy_control' folder (Label 0)\n",
    "print(\"\\nProcessing 'healthy_control' folder (Label 0)...\")\n",
    "true_a, pred_a = test_files_in_folder(folder_a_path, model, true_label=0)\n",
    "y_true.extend(true_a)\n",
    "y_pred.extend(pred_a)\n",
    "\n",
    "# Test 'parkinson_patient' folder (Label 1)\n",
    "print(\"\\nProcessing 'parkinson_patient' folder (Label 1)...\")\n",
    "true_b, pred_b = test_files_in_folder(folder_b_path, model, true_label=1)\n",
    "y_true.extend(true_b)\n",
    "y_pred.extend(pred_b)\n",
    "\n",
    "print(\"\\nEvaluation complete. ✅\")\n",
    "\n",
    "# --- 5. Generate and Display Confusion Matrix ---\n",
    "if len(y_true) > 0 and len(y_pred) > 0:\n",
    "    print(\"\\n--- Generating Confusion Matrix ---\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Calculate key metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall (Sensitivity): {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1_score:.2f}\")\n",
    "\n",
    "    # Plot the matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted Healthy', 'Predicted Parkinson'],\n",
    "                yticklabels=['Actual Healthy', 'Actual Parkinson'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Not enough data to generate the confusion matrix. Please check your file paths and data.\")"
   ],
   "id": "564657b4582b5535",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully! ✅\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"functional\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001B[38;5;33mInputLayer\u001B[0m)        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m30\u001B[0m, \u001B[38;5;34m188\u001B[0m)        │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ parkinson_detector_model        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │       \u001B[38;5;34m704,001\u001B[0m │\n",
       "│ (\u001B[38;5;33mParkinsonDetectorModel\u001B[0m)        │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">188</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ parkinson_detector_model        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">704,001</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ParkinsonDetectorModel</span>)        │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m2,112,005\u001B[0m (8.06 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,112,005</span> (8.06 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m704,001\u001B[0m (2.69 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">704,001</span> (2.69 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Optimizer params: \u001B[0m\u001B[38;5;34m1,408,004\u001B[0m (5.37 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,408,004</span> (5.37 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Evaluation ---\n",
      "\n",
      "Processing 'healthy_control' folder (Label 0)...\n",
      "Found 229 WAV files in D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\mPower\\data\\original_A_DEFAULT\\healthy_control. Processing all of them...\n",
      "\n",
      "Processing 'parkinson_patient' folder (Label 1)...\n",
      "Found 260 WAV files in D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\mPower\\data\\original_A_DEFAULT\\parkinson_patient. Processing all of them...\n",
      "\n",
      "Evaluation complete. ✅\n",
      "\n",
      "--- Generating Confusion Matrix ---\n",
      "Accuracy: 0.54\n",
      "Precision: 0.62\n",
      "Recall (Sensitivity): 0.56\n",
      "F1-Score: 0.59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
