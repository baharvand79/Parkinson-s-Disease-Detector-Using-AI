{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T08:22:24.022139Z",
     "start_time": "2025-09-14T08:14:23.481827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Dropout, Flatten,\n",
    "                                     Dense, LSTM, MultiHeadAttention, Concatenate, Reshape)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (confusion_matrix, precision_score, recall_score, f1_score,\n",
    "                           roc_curve, roc_auc_score, precision_recall_curve,\n",
    "                           average_precision_score, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import ttest_ind\n",
    "import pandas as pd\n",
    "import shap\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid, make_axes_locatable\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.cm as cm\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# =============================================================================\n",
    "# --- Configuration ---\n",
    "# =============================================================================\n",
    "\n",
    "# Dataset and mode configuration\n",
    "ITALIAN_DATASET = \"ITALIAN_DATASET\"\n",
    "UAMS_DATASET = \"UAMS_DATASET\"\n",
    "NEUROVOZ_DATASET = \"NEUROVOZ_DATASET\"\n",
    "MPOWER_DATASET = \"MPOWER_DATASET\"\n",
    "SYNTHETIC_DATASET = \"SYNTHETIC_DATASET\"\n",
    "\n",
    "MODE_ALL_VALIDS = \"ALL_VALIDS\"\n",
    "MODE_A = \"A\"\n",
    "\n",
    "FEATURE_MODE_BASIC = \"BASIC\"\n",
    "FEATURE_MODE_ALL = \"ALL\"\n",
    "FEATURE_MODE_DEFAULT = \"DEFAULT\"\n",
    "\n",
    "MODEL_NAME = \"nca_smote_seq\"\n",
    "\n",
    "# Select configuration\n",
    "DATASET = UAMS_DATASET\n",
    "MODE = MODE_A\n",
    "FEATURE_MODE = FEATURE_MODE_DEFAULT\n",
    "\n",
    "# Map dataset names\n",
    "dataset_mapping = {\n",
    "    NEUROVOZ_DATASET: \"Neurovoz\",\n",
    "    UAMS_DATASET: \"UAMS\",\n",
    "    MPOWER_DATASET: \"mPower\",\n",
    "    SYNTHETIC_DATASET: \"Synthetic\",\n",
    "    ITALIAN_DATASET: \"Italian\"\n",
    "}\n",
    "\n",
    "dataset = dataset_mapping[DATASET]\n",
    "\n",
    "# Path Setup\n",
    "FEATURES_FILE_PATH = os.path.join(os.getcwd(), dataset, \"data\", f\"features_{MODE}_{FEATURE_MODE}.npz\")\n",
    "MODEL_PATH = os.path.join(os.getcwd(), dataset, f\"results_{MODE}_{FEATURE_MODE}\", MODEL_NAME)\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "# File paths\n",
    "EVALUATION_FILE_PATH = os.path.join(MODEL_PATH, \"evaluation.csv\")\n",
    "HISTORY_SAVE_PATH = os.path.join(MODEL_PATH, \"history.csv\")\n",
    "BEST_MODEL_PATH = os.path.join(MODEL_PATH, \"best_model.keras\")\n",
    "PLOTS_PATH = os.path.join(MODEL_PATH, \"plots\")\n",
    "SHAP_OUTPUT_PATH = os.path.join(MODEL_PATH, \"shap_analysis\")\n",
    "GRADCAM_OUTPUT_PATH = os.path.join(MODEL_PATH, \"gradcam_analysis\")\n",
    "ANALYSIS_PATH = os.path.join(MODEL_PATH, \"comprehensive_analysis\")\n",
    "\n",
    "# Create directories\n",
    "for path in [PLOTS_PATH, SHAP_OUTPUT_PATH, GRADCAM_OUTPUT_PATH, ANALYSIS_PATH]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT_RATE = 0.5\n",
    "L2_STRENGTH = 0.01\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(BEST_MODEL_PATH, monitor='val_auc', mode='max',\n",
    "                               save_best_only=True, verbose=1)\n",
    "early_stop_cb = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# =============================================================================\n",
    "# --- Data Loading ---\n",
    "# =============================================================================\n",
    "\n",
    "def load_data(feature_file_path):\n",
    "    \"\"\"Load and prepare data from feature file\"\"\"\n",
    "    print(f\"--- Loading data from {feature_file_path} ---\")\n",
    "\n",
    "    if not os.path.exists(feature_file_path):\n",
    "        print(f\"Creating dummy data for demonstration...\")\n",
    "        # Create dummy data for demonstration\n",
    "        n_samples = 1000\n",
    "        n_features = 60  # mel_spectrogram (30) + mfcc (30)\n",
    "        n_timesteps = 94\n",
    "\n",
    "        X = np.random.randn(n_samples, n_features, n_timesteps)\n",
    "        y = np.random.randint(0, 2, n_samples)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    with np.load(feature_file_path) as data:\n",
    "        labels = data['labels']\n",
    "        mel_spectrogram = data['mel_spectrogram']\n",
    "        mfcc = data['mfcc']\n",
    "        X = np.concatenate((mel_spectrogram, mfcc), axis=-1)\n",
    "        return X, labels\n",
    "\n",
    "# =============================================================================\n",
    "# --- Model Architecture ---\n",
    "# =============================================================================\n",
    "\n",
    "@register_keras_serializable()\n",
    "class ParkinsonDetectorModel(Model):\n",
    "    def __init__(self, input_shape, **kwargs):\n",
    "        super(ParkinsonDetectorModel, self).__init__(**kwargs)\n",
    "        self.input_shape_config = input_shape\n",
    "\n",
    "        # CNN layers\n",
    "        self.reshape_in = Reshape((input_shape[0], input_shape[1], 1))\n",
    "        self.conv1a = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv1b = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.pool1 = MaxPooling2D(5)\n",
    "        self.drop1 = Dropout(DROPOUT_RATE)\n",
    "        self.conv2a = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv2b = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same', name='last_conv_layer')\n",
    "        self.pool2 = MaxPooling2D(5, name='cnn_output')\n",
    "        self.drop2 = Dropout(DROPOUT_RATE)\n",
    "\n",
    "        # Sequential flow: CNN -> LSTM -> Attention -> Dense\n",
    "        self.lstm1 = LSTM(128, return_sequences=True)\n",
    "        self.lstm2 = LSTM(128, return_sequences=True, name='lstm_output')  # Keep sequences for attention\n",
    "        self.drop_lstm = Dropout(DROPOUT_RATE)\n",
    "\n",
    "        self.attention = MultiHeadAttention(num_heads=2, key_dim=64, name='attention_output')\n",
    "        self.flatten_final = Flatten()\n",
    "\n",
    "        self.dense_bottleneck = Dense(128, activation='relu', name='bottleneck_features')\n",
    "        self.dense_output = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # CNN branch\n",
    "        x = self.reshape_in(inputs)\n",
    "        x = self.conv1a(x)\n",
    "        x = self.conv1b(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x, training=training)\n",
    "        x = self.conv2a(x)\n",
    "        x = self.conv2b(x)\n",
    "        cnn_output = self.pool2(x)\n",
    "        x = self.drop2(cnn_output, training=training)\n",
    "\n",
    "        # Reshape for LSTM input (batch_size, timesteps, features)\n",
    "        shape = tf.shape(x)\n",
    "        sequence = tf.reshape(x, [-1, shape[1] * shape[2], shape[3]])\n",
    "\n",
    "        # LSTM processing\n",
    "        lstm_out = self.lstm1(sequence)\n",
    "        lstm_out = self.lstm2(lstm_out)\n",
    "        lstm_out = self.drop_lstm(lstm_out, training=training)\n",
    "\n",
    "        # Multi-Head Attention processing\n",
    "        attention_out = self.attention(query=lstm_out, key=lstm_out, value=lstm_out)\n",
    "\n",
    "        # Flatten and final dense layers\n",
    "        flattened = self.flatten_final(attention_out)\n",
    "        bottleneck = self.dense_bottleneck(flattened)\n",
    "        output = self.dense_output(bottleneck)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"input_shape_config\": self.input_shape_config\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "def build_model(input_shape: tuple) -> Model:\n",
    "    \"\"\"Build the hybrid model\"\"\"\n",
    "    print(\"--- Building the model ---\")\n",
    "    inputs = Input(shape=input_shape)\n",
    "    parkinson_detector = ParkinsonDetectorModel(input_shape=input_shape)\n",
    "    outputs = parkinson_detector(inputs)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    print(\"Model built successfully.\")\n",
    "    return model\n",
    "\n",
    "# =============================================================================\n",
    "# --- Comprehensive Plotting Functions ---\n",
    "# =============================================================================\n",
    "\n",
    "def plot_training_history(history_df, save_path):\n",
    "    \"\"\"Plot comprehensive training history\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Training History Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Loss\n",
    "    axes[0, 0].plot(history_df.index, history_df['loss'], label='Training Loss', linewidth=2)\n",
    "    axes[0, 0].plot(history_df.index, history_df['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    axes[0, 0].set_title('Model Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Accuracy\n",
    "    axes[0, 1].plot(history_df.index, history_df['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    axes[0, 1].plot(history_df.index, history_df['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    axes[0, 1].set_title('Model Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # AUC\n",
    "    axes[1, 0].plot(history_df.index, history_df['auc'], label='Training AUC', linewidth=2)\n",
    "    axes[1, 0].plot(history_df.index, history_df['val_auc'], label='Validation AUC', linewidth=2)\n",
    "    axes[1, 0].set_title('Model AUC')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('AUC')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Learning curves comparison\n",
    "    axes[1, 1].plot(history_df.index, history_df['loss'], label='Train Loss', alpha=0.7)\n",
    "    axes[1, 1].plot(history_df.index, history_df['val_loss'], label='Val Loss', alpha=0.7)\n",
    "    ax2 = axes[1, 1].twinx()\n",
    "    ax2.plot(history_df.index, history_df['accuracy'], label='Train Acc', linestyle='--', alpha=0.7)\n",
    "    ax2.plot(history_df.index, history_df['val_accuracy'], label='Val Acc', linestyle='--', alpha=0.7)\n",
    "    axes[1, 1].set_title('Loss vs Accuracy')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss', color='blue')\n",
    "    ax2.set_ylabel('Accuracy', color='red')\n",
    "    axes[1, 1].legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Training history plot saved to {save_path}\")\n",
    "\n",
    "def plot_model_performance(y_true, y_pred_proba, save_dir):\n",
    "    \"\"\"Generate comprehensive performance plots\"\"\"\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    gs = gridspec.GridSpec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # 1. Confusion Matrix\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "    ax1.set_title('Confusion Matrix')\n",
    "    ax1.set_xlabel('Predicted')\n",
    "    ax1.set_ylabel('Actual')\n",
    "\n",
    "    # 2. ROC Curve\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    auc_score = roc_auc_score(y_true, y_pred_proba)\n",
    "    ax2.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.3f})', linewidth=2)\n",
    "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.set_title('ROC Curve')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Precision-Recall Curve\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    avg_precision = average_precision_score(y_true, y_pred_proba)\n",
    "    ax3.plot(recall, precision, label=f'PR Curve (AP = {avg_precision:.3f})', linewidth=2)\n",
    "    ax3.set_xlabel('Recall')\n",
    "    ax3.set_ylabel('Precision')\n",
    "    ax3.set_title('Precision-Recall Curve')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Prediction Distribution\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    ax4.hist(y_pred_proba[y_true == 0], bins=30, alpha=0.7, label='Healthy', density=True)\n",
    "    ax4.hist(y_pred_proba[y_true == 1], bins=30, alpha=0.7, label='Parkinson', density=True)\n",
    "    ax4.axvline(x=0.5, color='red', linestyle='--', label='Threshold')\n",
    "    ax4.set_xlabel('Prediction Probability')\n",
    "    ax4.set_ylabel('Density')\n",
    "    ax4.set_title('Prediction Distribution')\n",
    "    ax4.legend()\n",
    "\n",
    "    # 5. Threshold Analysis\n",
    "    ax5 = fig.add_subplot(gs[1, 1])\n",
    "    thresholds = np.linspace(0.1, 0.9, 50)\n",
    "    precisions, recalls, f1s = [], [], []\n",
    "\n",
    "    for thresh in thresholds:\n",
    "        y_pred_thresh = (y_pred_proba > thresh).astype(int)\n",
    "        precisions.append(precision_score(y_true, y_pred_thresh, zero_division=0))\n",
    "        recalls.append(recall_score(y_true, y_pred_thresh, zero_division=0))\n",
    "        f1s.append(f1_score(y_true, y_pred_thresh, zero_division=0))\n",
    "\n",
    "    ax5.plot(thresholds, precisions, label='Precision', linewidth=2)\n",
    "    ax5.plot(thresholds, recalls, label='Recall', linewidth=2)\n",
    "    ax5.plot(thresholds, f1s, label='F1-Score', linewidth=2)\n",
    "    ax5.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='Default Threshold')\n",
    "    ax5.set_xlabel('Threshold')\n",
    "    ax5.set_ylabel('Score')\n",
    "    ax5.set_title('Threshold Analysis')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "\n",
    "    # 6. Class Distribution\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    class_counts = np.bincount(y_true)\n",
    "    ax6.bar(['Healthy', 'Parkinson'], class_counts, color=['lightblue', 'lightcoral'])\n",
    "    ax6.set_title('Class Distribution')\n",
    "    ax6.set_ylabel('Count')\n",
    "    for i, count in enumerate(class_counts):\n",
    "        ax6.text(i, count + 0.01 * max(class_counts), str(count), ha='center')\n",
    "\n",
    "    # 7. Error Analysis\n",
    "    ax7 = fig.add_subplot(gs[2, :])\n",
    "    errors = np.abs(y_true - y_pred_proba.flatten())\n",
    "    ax7.scatter(range(len(errors)), errors, alpha=0.6,\n",
    "               c=['red' if y_true[i] != y_pred[i] else 'blue' for i in range(len(y_true))],\n",
    "               s=20)\n",
    "    ax7.set_xlabel('Sample Index')\n",
    "    ax7.set_ylabel('Prediction Error')\n",
    "    ax7.set_title('Prediction Errors (Red: Misclassified, Blue: Correct)')\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle('Comprehensive Model Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    plt.savefig(os.path.join(save_dir, 'comprehensive_performance.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Comprehensive performance plot saved to {save_dir}\")\n",
    "\n",
    "def save_metrics_to_csv(y_true, y_pred_proba, filename, threshold=0.5):\n",
    "    \"\"\"Save detailed metrics to CSV\"\"\"\n",
    "    y_pred_binary = (np.array(y_pred_proba) > threshold).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    total_samples = cm.sum()\n",
    "    tn_percent = (tn / total_samples) * 100 if total_samples > 0 else 0\n",
    "    fp_percent = (fp / total_samples) * 100 if total_samples > 0 else 0\n",
    "    fn_percent = (fn / total_samples) * 100 if total_samples > 0 else 0\n",
    "    tp_percent = (tp / total_samples) * 100 if total_samples > 0 else 0\n",
    "\n",
    "    precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "    report_data = {\n",
    "        'Metric': [\n",
    "            'True Positive (TP)', 'True Negative (TN)', 'False Positive (FP)', 'False Negative (FN)',\n",
    "            'Precision', 'Recall (Sensitivity)', 'Specificity', 'F1-Score', 'AUC', 'Accuracy'\n",
    "        ],\n",
    "        'Value': [\n",
    "            f\"{tp} ({tp_percent:.2f}%)\", f\"{tn} ({tn_percent:.2f}%)\",\n",
    "            f\"{fp} ({fp_percent:.2f}%)\", f\"{fn} ({fn_percent:.2f}%)\",\n",
    "            f\"{precision:.4f}\", f\"{recall:.4f}\", f\"{specificity:.4f}\",\n",
    "            f\"{f1:.4f}\", f\"{auc:.4f}\", f\"{(tp + tn) / total_samples:.4f}\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(report_data)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Evaluation results saved to {filename}\")\n",
    "\n",
    "# =============================================================================\n",
    "# --- Feature Analysis ---\n",
    "# =============================================================================\n",
    "\n",
    "def generate_feature_map_info(X_shape):\n",
    "    \"\"\"Generate feature layout information\"\"\"\n",
    "    n_features, n_timesteps = X_shape[1], X_shape[2]\n",
    "\n",
    "    # Assume first half is mel spectrogram, second half is MFCC\n",
    "    mel_features = n_features // 2\n",
    "    mfcc_features = n_features - mel_features\n",
    "\n",
    "    feature_layout = {\n",
    "        'mel_spectrogram': mel_features,\n",
    "        'mfcc': mfcc_features\n",
    "    }\n",
    "\n",
    "    colors = plt.get_cmap('Paired', len(feature_layout))\n",
    "    feature_names = list(feature_layout.keys())\n",
    "\n",
    "    color_mask = np.zeros((n_features, n_timesteps), dtype=int)\n",
    "\n",
    "    current_row = 0\n",
    "    for i, (name, num_rows) in enumerate(feature_layout.items()):\n",
    "        color_mask[current_row:current_row + num_rows, :] = i\n",
    "        current_row += num_rows\n",
    "\n",
    "    legend_patches = [mpatches.Patch(color=colors(i), label=f\"{name} ({feature_layout[name]} rows)\")\n",
    "                      for i, name in enumerate(feature_names)]\n",
    "\n",
    "    return {\n",
    "        'color_mask': color_mask,\n",
    "        'feature_layout': feature_layout,\n",
    "        'feature_names': feature_names,\n",
    "        'colors': colors,\n",
    "        'legend_patches': legend_patches,\n",
    "        'total_rows': n_features\n",
    "    }\n",
    "\n",
    "def plot_feature_analysis(X, y, save_dir):\n",
    "    \"\"\"Generate feature analysis plots\"\"\"\n",
    "    print(\"Generating feature analysis plots...\")\n",
    "\n",
    "    # Feature statistics by class\n",
    "    healthy_features = X[y == 0]\n",
    "    parkinson_features = X[y == 1]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # 1. Feature means by class\n",
    "    healthy_mean = np.mean(healthy_features, axis=(0, 2))\n",
    "    parkinson_mean = np.mean(parkinson_features, axis=(0, 2))\n",
    "\n",
    "    axes[0, 0].plot(healthy_mean, label='Healthy', linewidth=2)\n",
    "    axes[0, 0].plot(parkinson_mean, label='Parkinson', linewidth=2)\n",
    "    axes[0, 0].set_title('Average Feature Values by Class')\n",
    "    axes[0, 0].set_xlabel('Feature Index')\n",
    "    axes[0, 0].set_ylabel('Average Value')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Feature variance by class\n",
    "    healthy_var = np.var(healthy_features, axis=(0, 2))\n",
    "    parkinson_var = np.var(parkinson_features, axis=(0, 2))\n",
    "\n",
    "    axes[0, 1].plot(healthy_var, label='Healthy', linewidth=2)\n",
    "    axes[0, 1].plot(parkinson_var, label='Parkinson', linewidth=2)\n",
    "    axes[0, 1].set_title('Feature Variance by Class')\n",
    "    axes[0, 1].set_xlabel('Feature Index')\n",
    "    axes[0, 1].set_ylabel('Variance')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Sample spectrograms\n",
    "    if len(healthy_features) > 0:\n",
    "        axes[1, 0].imshow(healthy_features[0], aspect='auto', cmap='viridis')\n",
    "        axes[1, 0].set_title('Sample Healthy Spectrogram')\n",
    "        axes[1, 0].set_xlabel('Time Steps')\n",
    "        axes[1, 0].set_ylabel('Features')\n",
    "\n",
    "    if len(parkinson_features) > 0:\n",
    "        axes[1, 1].imshow(parkinson_features[0], aspect='auto', cmap='viridis')\n",
    "        axes[1, 1].set_title('Sample Parkinson Spectrogram')\n",
    "        axes[1, 1].set_xlabel('Time Steps')\n",
    "        axes[1, 1].set_ylabel('Features')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'feature_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Feature correlation analysis\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Flatten features for correlation\n",
    "    healthy_flat = healthy_features.reshape(len(healthy_features), -1)\n",
    "    parkinson_flat = parkinson_features.reshape(len(parkinson_features), -1)\n",
    "\n",
    "    # Sample features for correlation (too many features for full correlation matrix)\n",
    "    n_sample_features = min(50, healthy_flat.shape[1])\n",
    "    sample_indices = np.random.choice(healthy_flat.shape[1], n_sample_features, replace=False)\n",
    "\n",
    "    healthy_corr = np.corrcoef(healthy_flat[:, sample_indices].T)\n",
    "    parkinson_corr = np.corrcoef(parkinson_flat[:, sample_indices].T)\n",
    "\n",
    "    im1 = axes[0].imshow(healthy_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    axes[0].set_title('Healthy Feature Correlations')\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "    im2 = axes[1].imshow(parkinson_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    axes[1].set_title('Parkinson Feature Correlations')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'feature_correlations.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Feature analysis plots saved to {save_dir}\")\n",
    "\n",
    "# =============================================================================\n",
    "# --- SHAP Analysis ---\n",
    "# =============================================================================\n",
    "\n",
    "def run_full_shap_analysis(model, X_train, X_test, y_test, output_path,\n",
    "                          samples_per_class=50, top_n=20):\n",
    "    \"\"\"Run comprehensive SHAP analysis\"\"\"\n",
    "    print(\"\\n--- Running Full SHAP Analysis ---\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    feature_map_info = generate_feature_map_info(X_test.shape)\n",
    "\n",
    "    # Balanced sample selection\n",
    "    healthy_indices = np.where(y_test == 0)[0]\n",
    "    parkinson_indices = np.where(y_test == 1)[0]\n",
    "\n",
    "    num_healthy = min(samples_per_class, len(healthy_indices))\n",
    "    num_parkinson = min(samples_per_class, len(parkinson_indices))\n",
    "\n",
    "    selected_healthy = np.random.choice(healthy_indices, num_healthy, replace=False) if num_healthy > 0 else np.array([])\n",
    "    selected_parkinson = np.random.choice(parkinson_indices, num_parkinson, replace=False) if num_parkinson > 0 else np.array([])\n",
    "\n",
    "    final_indices = np.concatenate([selected_healthy, selected_parkinson]).astype(int)\n",
    "    np.random.shuffle(final_indices)\n",
    "\n",
    "    test_samples = X_test[final_indices]\n",
    "    y_true_samples = y_test[final_indices]\n",
    "\n",
    "    print(f\"Calculating SHAP values for {len(test_samples)} balanced samples...\")\n",
    "\n",
    "    # Use a smaller background set for faster computation\n",
    "    background = X_train[:min(50, len(X_train))].astype(np.float32)\n",
    "    explainer = shap.GradientExplainer(model, background)\n",
    "\n",
    "    shap_values_list = []\n",
    "    for sample in tqdm(test_samples, desc=\"SHAP Progress\"):\n",
    "        sample_batch = np.expand_dims(sample, axis=0).astype(np.float32)\n",
    "        sv = explainer.shap_values(sample_batch)\n",
    "        if isinstance(sv, list):\n",
    "            sv = sv[0]\n",
    "        shap_values_list.append(sv.squeeze())\n",
    "\n",
    "    shap_values = np.array(shap_values_list)\n",
    "\n",
    "    # Global importance analysis\n",
    "    flat_shap = shap_values.reshape(len(shap_values), -1)\n",
    "    mean_abs_shap = np.mean(np.abs(flat_shap), axis=0)\n",
    "    top_indices = np.argsort(mean_abs_shap)[::-1][:top_n]\n",
    "\n",
    "    # Create coordinates\n",
    "    coords = [np.unravel_index(i, shap_values.shape[1:]) for i in top_indices]\n",
    "\n",
    "    # Map to feature names\n",
    "    def get_feature_name(row_idx, feature_layout):\n",
    "        cum = 0\n",
    "        for name, nrows in feature_layout.items():\n",
    "            if row_idx < cum + nrows:\n",
    "                return name\n",
    "            cum += nrows\n",
    "        return \"Unknown\"\n",
    "\n",
    "    labels = []\n",
    "    for row_idx, time_idx in coords:\n",
    "        fname = get_feature_name(row_idx, feature_map_info['feature_layout'])\n",
    "        labels.append(f\"{fname}_T{time_idx}\")\n",
    "\n",
    "    # Plot global importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.bar(range(len(top_indices)), mean_abs_shap[top_indices])\n",
    "    plt.xticks(range(len(top_indices)), labels, rotation=45, ha='right')\n",
    "    plt.title(f'Top-{top_n} Most Important Features (Global SHAP)')\n",
    "    plt.xlabel('Feature (Type_TimeStep)')\n",
    "    plt.ylabel('Mean |SHAP Value|')\n",
    "\n",
    "    # Color bars by feature type\n",
    "    colors = ['lightblue' if 'mel' in label else 'lightcoral' for label in labels]\n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_color(color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, 'shap_global_importance.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Class-specific analysis\n",
    "    hc_mask = (y_true_samples == 0)\n",
    "    pd_mask = (y_true_samples == 1)\n",
    "\n",
    "    if np.any(hc_mask) and np.any(pd_mask):\n",
    "        hc_shap = shap_values[hc_mask].mean(axis=0)\n",
    "        pd_shap = shap_values[pd_mask].mean(axis=0)\n",
    "        diff_shap = pd_shap - hc_shap\n",
    "\n",
    "        # Create comprehensive SHAP visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "        # Healthy average\n",
    "        im1 = axes[0, 0].imshow(hc_shap, cmap='RdBu_r', aspect='auto')\n",
    "        axes[0, 0].set_title('Average SHAP - Healthy')\n",
    "        axes[0, 0].set_xlabel('Time Steps')\n",
    "        axes[0, 0].set_ylabel('Features')\n",
    "        plt.colorbar(im1, ax=axes[0, 0])\n",
    "\n",
    "        # Parkinson average\n",
    "        im2 = axes[0, 1].imshow(pd_shap, cmap='RdBu_r', aspect='auto')\n",
    "        axes[0, 1].set_title('Average SHAP - Parkinson')\n",
    "        axes[0, 1].set_xlabel('Time Steps')\n",
    "        axes[0, 1].set_ylabel('Features')\n",
    "        plt.colorbar(im2, ax=axes[0, 1])\n",
    "\n",
    "        # Difference\n",
    "        max_diff = np.max(np.abs(diff_shap))\n",
    "        im3 = axes[1, 0].imshow(diff_shap, cmap='seismic', aspect='auto',\n",
    "                               vmin=-max_diff, vmax=max_diff)\n",
    "        axes[1, 0].set_title('SHAP Difference (Parkinson - Healthy)')\n",
    "        axes[1, 0].set_xlabel('Time Steps')\n",
    "        axes[1, 0].set_ylabel('Features')\n",
    "        plt.colorbar(im3, ax=axes[1, 0])\n",
    "\n",
    "        # Feature map\n",
    "        axes[1, 1].imshow(feature_map_info['color_mask'],\n",
    "                         cmap=feature_map_info['colors'], aspect='auto')\n",
    "        axes[1, 1].set_title('Feature Map')\n",
    "        axes[1, 1].set_xlabel('Time Steps')\n",
    "        axes[1, 1].set_ylabel('Features')\n",
    "        axes[1, 1].legend(handles=feature_map_info['legend_patches'],\n",
    "                         loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_path, 'shap_class_comparison.png'),\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    # Save SHAP summary statistics\n",
    "    shap_stats = {\n",
    "        'total_samples_analyzed': len(test_samples),\n",
    "        'healthy_samples': np.sum(hc_mask) if np.any(hc_mask) else 0,\n",
    "        'parkinson_samples': np.sum(pd_mask) if np.any(pd_mask) else 0,\n",
    "        'mean_abs_shap_healthy': np.mean(np.abs(hc_shap)) if np.any(hc_mask) else 0,\n",
    "        'mean_abs_shap_parkinson': np.mean(np.abs(pd_shap)) if np.any(pd_mask) else 0,\n",
    "        'max_shap_difference': np.max(np.abs(diff_shap)) if np.any(hc_mask) and np.any(pd_mask) else 0\n",
    "    }\n",
    "\n",
    "    pd.DataFrame([shap_stats]).to_csv(os.path.join(output_path, 'shap_statistics.csv'), index=False)\n",
    "\n",
    "    print(f\"SHAP analysis complete. Results saved to {output_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# --- Grad-CAM Analysis ---\n",
    "# =============================================================================\n",
    "\n",
    "def run_gradcam_analysis(model, X_test, y_test, output_path, num_samples=50):\n",
    "    \"\"\"Run Grad-CAM analysis\"\"\"\n",
    "    print(\"\\n--- Running Grad-CAM Analysis ---\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # Find ParkinsonDetectorModel layer\n",
    "    parkinson_detector = None\n",
    "    for layer in model.layers:\n",
    "        if 'ParkinsonDetectorModel' in str(type(layer)):\n",
    "            parkinson_detector = layer\n",
    "            break\n",
    "\n",
    "    if parkinson_detector is None:\n",
    "        print(\"ParkinsonDetectorModel not found. Skipping Grad-CAM analysis.\")\n",
    "        return\n",
    "\n",
    "    def get_conv_and_output(inputs):\n",
    "        x = parkinson_detector.reshape_in(inputs)\n",
    "        x = parkinson_detector.conv1a(x)\n",
    "        x = parkinson_detector.conv1b(x)\n",
    "        x = parkinson_detector.pool1(x)\n",
    "        x = parkinson_detector.drop1(x, training=False)\n",
    "        x = parkinson_detector.conv2a(x)\n",
    "        conv_output = parkinson_detector.conv2b(x)\n",
    "        x = parkinson_detector.pool2(conv_output)\n",
    "        x = parkinson_detector.drop2(x, training=False)\n",
    "        cnn_flat = parkinson_detector.flatten_cnn(x)\n",
    "\n",
    "        shape = tf.shape(x)\n",
    "        sequence = tf.reshape(x, [-1, shape[1] * shape[2], shape[3]])\n",
    "        att_out = parkinson_detector.attention(query=sequence, key=sequence, value=sequence)\n",
    "        att_flat = parkinson_detector.flatten_att(att_out)\n",
    "        lstm_seq = parkinson_detector.lstm1(sequence)\n",
    "        lstm_out = parkinson_detector.lstm2(lstm_seq)\n",
    "        lstm_out = parkinson_detector.drop_lstm(lstm_out, training=False)\n",
    "        concatenated = parkinson_detector.concat([cnn_flat, att_flat, lstm_out])\n",
    "        bottleneck = parkinson_detector.dense_bottleneck(concatenated)\n",
    "        final_output = parkinson_detector.dense_output(bottleneck)\n",
    "\n",
    "        return conv_output, final_output\n",
    "\n",
    "    # Sample selection\n",
    "    parkinson_indices = np.where(y_test == 1)[0]\n",
    "    healthy_indices = np.where(y_test == 0)[0]\n",
    "\n",
    "    n_samples = min(num_samples, min(len(parkinson_indices), len(healthy_indices)))\n",
    "\n",
    "    selected_pd = np.random.choice(parkinson_indices, n_samples, replace=False) if len(parkinson_indices) > 0 else []\n",
    "    selected_hc = np.random.choice(healthy_indices, n_samples, replace=False) if len(healthy_indices) > 0 else []\n",
    "\n",
    "    def calculate_gradcam_heatmaps(indices):\n",
    "        heatmaps = []\n",
    "        for i in tqdm(indices, desc=\"Computing Grad-CAM\"):\n",
    "            img = X_test[i:i+1]\n",
    "            with tf.GradientTape() as tape:\n",
    "                img_tensor = tf.cast(img, tf.float32)\n",
    "                tape.watch(img_tensor)\n",
    "                conv_outputs, preds = get_conv_and_output(img_tensor)\n",
    "                loss = preds[:, 0]\n",
    "\n",
    "            grads = tape.gradient(loss, conv_outputs)\n",
    "            pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "            conv_outputs_np = conv_outputs[0].numpy()\n",
    "            pooled_grads_np = pooled_grads.numpy()\n",
    "\n",
    "            heatmap = np.zeros(conv_outputs_np.shape[:-1])\n",
    "            for j in range(conv_outputs_np.shape[-1]):\n",
    "                heatmap += pooled_grads_np[j] * conv_outputs_np[:, :, j]\n",
    "\n",
    "            heatmap = np.maximum(heatmap, 0)\n",
    "            heatmap /= (heatmap.max() + 1e-10)\n",
    "            heatmaps.append(heatmap)\n",
    "\n",
    "        return heatmaps\n",
    "\n",
    "    # Calculate heatmaps\n",
    "    pd_heatmaps = calculate_gradcam_heatmaps(selected_pd) if len(selected_pd) > 0 else []\n",
    "    hc_heatmaps = calculate_gradcam_heatmaps(selected_hc) if len(selected_hc) > 0 else []\n",
    "\n",
    "    # Average heatmaps\n",
    "    avg_pd_heatmap = np.mean(pd_heatmaps, axis=0) if pd_heatmaps else np.zeros((X_test.shape[1], X_test.shape[2]))\n",
    "    avg_hc_heatmap = np.mean(hc_heatmaps, axis=0) if hc_heatmaps else np.zeros((X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "    # Resize to original input dimensions\n",
    "    original_height, original_width = X_test.shape[1], X_test.shape[2]\n",
    "\n",
    "    upscaled_pd = resize(avg_pd_heatmap, (original_height, original_width),\n",
    "                        order=3, mode='reflect', anti_aliasing=True)\n",
    "    upscaled_hc = resize(avg_hc_heatmap, (original_height, original_width),\n",
    "                        order=3, mode='reflect', anti_aliasing=True)\n",
    "\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "    # Sample inputs\n",
    "    if len(selected_pd) > 0:\n",
    "        sample_pd = X_test[selected_pd[0]]\n",
    "        axes[0, 0].imshow(sample_pd, cmap='viridis', aspect='auto')\n",
    "        axes[0, 0].set_title(f'Parkinson Sample {selected_pd[0]}')\n",
    "        axes[0, 0].set_xlabel('Time Steps')\n",
    "        axes[0, 0].set_ylabel('Features')\n",
    "\n",
    "    # Parkinson heatmap\n",
    "    im1 = axes[0, 1].imshow(upscaled_pd, cmap='jet', aspect='auto')\n",
    "    axes[0, 1].set_title(f'Avg. Parkinson Attention ({len(selected_pd)} samples)')\n",
    "    axes[0, 1].set_xlabel('Time Steps')\n",
    "    axes[0, 1].set_ylabel('Features')\n",
    "    plt.colorbar(im1, ax=axes[0, 1])\n",
    "\n",
    "    # Feature map\n",
    "    feature_map_info = generate_feature_map_info(X_test.shape)\n",
    "    axes[0, 2].imshow(feature_map_info['color_mask'], cmap=feature_map_info['colors'], aspect='auto')\n",
    "    axes[0, 2].set_title('Feature Layout')\n",
    "    axes[0, 2].set_xlabel('Time Steps')\n",
    "    axes[0, 2].set_ylabel('Features')\n",
    "    axes[0, 2].legend(handles=feature_map_info['legend_patches'],\n",
    "                     loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    # Healthy samples\n",
    "    if len(selected_hc) > 0:\n",
    "        sample_hc = X_test[selected_hc[0]]\n",
    "        axes[1, 0].imshow(sample_hc, cmap='viridis', aspect='auto')\n",
    "        axes[1, 0].set_title(f'Healthy Sample {selected_hc[0]}')\n",
    "        axes[1, 0].set_xlabel('Time Steps')\n",
    "        axes[1, 0].set_ylabel('Features')\n",
    "\n",
    "    # Healthy heatmap\n",
    "    im2 = axes[1, 1].imshow(upscaled_hc, cmap='jet', aspect='auto')\n",
    "    axes[1, 1].set_title(f'Avg. Healthy Attention ({len(selected_hc)} samples)')\n",
    "    axes[1, 1].set_xlabel('Time Steps')\n",
    "    axes[1, 1].set_ylabel('Features')\n",
    "    plt.colorbar(im2, ax=axes[1, 1])\n",
    "\n",
    "    # Difference heatmap\n",
    "    diff_heatmap = upscaled_pd - upscaled_hc\n",
    "    max_diff = np.max(np.abs(diff_heatmap))\n",
    "    im3 = axes[1, 2].imshow(diff_heatmap, cmap='seismic', aspect='auto',\n",
    "                           vmin=-max_diff, vmax=max_diff)\n",
    "    axes[1, 2].set_title('Attention Difference (PD - HC)')\n",
    "    axes[1, 2].set_xlabel('Time Steps')\n",
    "    axes[1, 2].set_ylabel('Features')\n",
    "    plt.colorbar(im3, ax=axes[1, 2])\n",
    "\n",
    "    plt.suptitle('Grad-CAM Analysis: Model Attention Patterns', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, 'gradcam_comprehensive.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Grad-CAM analysis complete. Results saved to {output_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# --- Cross-Validation Analysis ---\n",
    "# =============================================================================\n",
    "\n",
    "def run_cross_validation(X, y, n_folds=5):\n",
    "    \"\"\"Perform stratified cross-validation\"\"\"\n",
    "    print(f\"\\n--- Running {n_folds}-Fold Cross-Validation ---\")\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    cv_results = {\n",
    "        'fold': [], 'accuracy': [], 'precision': [], 'recall': [],\n",
    "        'f1': [], 'auc': [], 'specificity': []\n",
    "    }\n",
    "\n",
    "    fold_histories = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\nTraining fold {fold + 1}/{n_folds}\")\n",
    "\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Build model\n",
    "        model = build_model(input_shape=(X.shape[1], X.shape[2]))\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "        )\n",
    "\n",
    "        # Train with early stopping\n",
    "        early_stop = EarlyStopping(monitor='val_auc', patience=10, restore_best_weights=True)\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            validation_data=(X_val_fold, y_val_fold),\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        fold_histories.append(history.history)\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred_proba = model.predict(X_val_fold, verbose=0)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "        # Calculate metrics\n",
    "        cm = confusion_matrix(y_val_fold, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        cv_results['fold'].append(fold + 1)\n",
    "        cv_results['accuracy'].append((tp + tn) / (tp + tn + fp + fn))\n",
    "        cv_results['precision'].append(precision_score(y_val_fold, y_pred, zero_division=0))\n",
    "        cv_results['recall'].append(recall_score(y_val_fold, y_pred, zero_division=0))\n",
    "        cv_results['f1'].append(f1_score(y_val_fold, y_pred, zero_division=0))\n",
    "        cv_results['auc'].append(roc_auc_score(y_val_fold, y_pred_proba))\n",
    "        cv_results['specificity'].append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
    "\n",
    "    # Save CV results\n",
    "    cv_df = pd.DataFrame(cv_results)\n",
    "\n",
    "    # Add summary statistics\n",
    "    summary_stats = {}\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1', 'auc', 'specificity']:\n",
    "        summary_stats[f'{metric}_mean'] = cv_df[metric].mean()\n",
    "        summary_stats[f'{metric}_std'] = cv_df[metric].std()\n",
    "\n",
    "    summary_df = pd.DataFrame([summary_stats])\n",
    "\n",
    "    # Save results\n",
    "    cv_df.to_csv(os.path.join(ANALYSIS_PATH, 'cross_validation_results.csv'), index=False)\n",
    "    summary_df.to_csv(os.path.join(ANALYSIS_PATH, 'cross_validation_summary.csv'), index=False)\n",
    "\n",
    "    # Plot CV results\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc', 'specificity']\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i // 3, i % 3]\n",
    "        ax.bar(cv_df['fold'], cv_df[metric])\n",
    "        ax.axhline(y=cv_df[metric].mean(), color='red', linestyle='--',\n",
    "                  label=f'Mean: {cv_df[metric].mean():.3f}')\n",
    "        ax.set_title(f'{metric.upper()} across folds')\n",
    "        ax.set_xlabel('Fold')\n",
    "        ax.set_ylabel(metric.upper())\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ANALYSIS_PATH, 'cross_validation_metrics.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Cross-validation complete. Results saved to {ANALYSIS_PATH}\")\n",
    "    return cv_df, summary_df\n",
    "\n",
    "# =============================================================================\n",
    "# --- Model Architecture Visualization ---\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_model_architecture(model, save_path):\n",
    "    \"\"\"Create model architecture visualization\"\"\"\n",
    "    try:\n",
    "        tf.keras.utils.plot_model(\n",
    "            model,\n",
    "            to_file=save_path,\n",
    "            show_shapes=True,\n",
    "            show_layer_names=True,\n",
    "            rankdir='TB',\n",
    "            expand_nested=True,\n",
    "            dpi=96\n",
    "        )\n",
    "        print(f\"Model architecture diagram saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create model architecture diagram: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# --- Main Execution ---\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"PARKINSON'S DISEASE DETECTION - COMPREHENSIVE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Load data\n",
    "    X, y = load_data(FEATURES_FILE_PATH)\n",
    "    print(f\"Loaded data: {X.shape} features, {len(y)} labels\")\n",
    "    print(f\"Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(f\"\\nData split: Train {len(y_train)}, Test {len(y_test)}\")\n",
    "\n",
    "    # Generate feature analysis\n",
    "    plot_feature_analysis(X, y, PLOTS_PATH)\n",
    "\n",
    "    # Build and compile model\n",
    "    model = build_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    model.summary()\n",
    "\n",
    "    # Visualize model architecture\n",
    "    visualize_model_architecture(model, os.path.join(PLOTS_PATH, 'model_architecture.png'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[checkpoint_cb, early_stop_cb],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save training history\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.to_csv(HISTORY_SAVE_PATH, index_label='epoch')\n",
    "    print(f\"Training history saved to {HISTORY_SAVE_PATH}\")\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(history_df, os.path.join(PLOTS_PATH, 'training_history.png'))\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"\\n--- Evaluating Model ---\")\n",
    "    y_pred_probabilities = model.predict(X_test)\n",
    "\n",
    "    # Save metrics\n",
    "    save_metrics_to_csv(y_test, y_pred_probabilities, EVALUATION_FILE_PATH)\n",
    "\n",
    "    # Generate performance plots\n",
    "    plot_model_performance(y_test, y_pred_probabilities, PLOTS_PATH)\n",
    "\n",
    "    # Run cross-validation\n",
    "    cv_results, cv_summary = run_cross_validation(X, y, n_folds=5)"
   ],
   "id": "b3cb233ebb27285c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PARKINSON'S DISEASE DETECTION - COMPREHENSIVE ANALYSIS\n",
      "============================================================\n",
      "--- Loading data from D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\data\\features_A_DEFAULT.npz ---\n",
      "Loaded data: (328, 30, 188) features, 328 labels\n",
      "Class distribution: [164 164]\n",
      "\n",
      "Data split: Train 262, Test 66\n",
      "Generating feature analysis plots...\n",
      "Feature analysis plots saved to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\nca_smote_seq\\plots\n",
      "--- Building the model ---\n",
      "WARNING:tensorflow:From C:\\Users\\bahar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Model built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"functional\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m\n",
       "\n",
       " input_layer (\u001B[38;5;33mInputLayer\u001B[0m)         (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m30\u001B[0m, \u001B[38;5;34m188\u001B[0m)                     \u001B[38;5;34m0\u001B[0m \n",
       "\n",
       " parkinson_detector_model         (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)                     \u001B[38;5;34m720,449\u001B[0m \n",
       " (\u001B[38;5;33mParkinsonDetectorModel\u001B[0m)                                               \n",
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">188</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " parkinson_detector_model         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">720,449</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ParkinsonDetectorModel</span>)                                               \n",
       "\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m720,449\u001B[0m (2.75 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">720,449</span> (2.75 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m720,449\u001B[0m (2.75 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">720,449</span> (2.75 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install graphviz (see instructions at https://graphviz.gitlab.io/download/) for `plot_model` to work.\n",
      "Model architecture diagram saved to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\nca_smote_seq\\plots\\model_architecture.png\n",
      "\n",
      "--- Starting Model Training ---\n",
      "Epoch 1/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 252ms/step - accuracy: 0.5051 - auc: 0.4926 - loss: 2.4306\n",
      "Epoch 1: val_auc improved from None to 0.53949, saving model to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\nca_smote_seq\\best_model.keras\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 348ms/step - accuracy: 0.5038 - auc: 0.5006 - loss: 2.2751 - val_accuracy: 0.5455 - val_auc: 0.5395 - val_loss: 1.8359\n",
      "Epoch 2/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 252ms/step - accuracy: 0.4597 - auc: 0.5248 - loss: 1.7244\n",
      "Epoch 2: val_auc did not improve from 0.53949\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 278ms/step - accuracy: 0.4885 - auc: 0.5173 - loss: 1.6416 - val_accuracy: 0.5000 - val_auc: 0.5303 - val_loss: 1.4142\n",
      "Epoch 3/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 253ms/step - accuracy: 0.5147 - auc: 0.5195 - loss: 1.3574\n",
      "Epoch 3: val_auc improved from 0.53949 to 0.57530, saving model to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\nca_smote_seq\\best_model.keras\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 293ms/step - accuracy: 0.5000 - auc: 0.5144 - loss: 1.3153 - val_accuracy: 0.5152 - val_auc: 0.5753 - val_loss: 1.1898\n",
      "Epoch 4/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 259ms/step - accuracy: 0.5422 - auc: 0.5080 - loss: 1.1616\n",
      "Epoch 4: val_auc did not improve from 0.57530\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 289ms/step - accuracy: 0.5076 - auc: 0.5147 - loss: 1.1432 - val_accuracy: 0.4697 - val_auc: 0.5257 - val_loss: 1.0626\n",
      "Epoch 5/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 260ms/step - accuracy: 0.4966 - auc: 0.4603 - loss: 1.0451\n",
      "Epoch 5: val_auc did not improve from 0.57530\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 289ms/step - accuracy: 0.4962 - auc: 0.4433 - loss: 1.0314 - val_accuracy: 0.5000 - val_auc: 0.5124 - val_loss: 0.9781\n",
      "Epoch 6/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 266ms/step - accuracy: 0.4312 - auc: 0.4770 - loss: 0.9653\n",
      "Epoch 6: val_auc did not improve from 0.57530\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 293ms/step - accuracy: 0.4504 - auc: 0.4578 - loss: 0.9533 - val_accuracy: 0.5000 - val_auc: 0.4761 - val_loss: 0.9164\n",
      "Epoch 7/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 277ms/step - accuracy: 0.4994 - auc: 0.5097 - loss: 0.9055\n",
      "Epoch 7: val_auc did not improve from 0.57530\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 308ms/step - accuracy: 0.5076 - auc: 0.4934 - loss: 0.8986 - val_accuracy: 0.4848 - val_auc: 0.4945 - val_loss: 0.8723\n",
      "Epoch 8/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 278ms/step - accuracy: 0.4809 - auc: 0.5731 - loss: 0.8616\n",
      "Epoch 8: val_auc did not improve from 0.57530\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 306ms/step - accuracy: 0.4885 - auc: 0.5250 - loss: 0.8567 - val_accuracy: 0.5152 - val_auc: 0.5028 - val_loss: 0.8397\n",
      "Epoch 9/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 353ms/step - accuracy: 0.4826 - auc: 0.4882 - loss: 0.8391\n",
      "Epoch 9: val_auc did not improve from 0.57530\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 393ms/step - accuracy: 0.4924 - auc: 0.4920 - loss: 0.8347 - val_accuracy: 0.5000 - val_auc: 0.5005 - val_loss: 0.8227\n",
      "Epoch 10/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 385ms/step - accuracy: 0.4909 - auc: 0.5141 - loss: 0.8120\n",
      "Epoch 10: val_auc did not improve from 0.57530\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 428ms/step - accuracy: 0.4924 - auc: 0.4916 - loss: 0.8113 - val_accuracy: 0.4545 - val_auc: 0.5372 - val_loss: 0.7991\n",
      "Epoch 11/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 394ms/step - accuracy: 0.4597 - auc: 0.5238 - loss: 0.7964\n",
      "Epoch 11: val_auc did not improve from 0.57530\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 431ms/step - accuracy: 0.4885 - auc: 0.4947 - loss: 0.7953 - val_accuracy: 0.5000 - val_auc: 0.4587 - val_loss: 0.7873\n",
      "Epoch 12/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 385ms/step - accuracy: 0.5209 - auc: 0.4367 - loss: 0.7816\n",
      "Epoch 12: val_auc did not improve from 0.57530\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 430ms/step - accuracy: 0.5000 - auc: 0.5201 - loss: 0.7799 - val_accuracy: 0.5000 - val_auc: 0.4848 - val_loss: 0.7704\n",
      "Epoch 13/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 396ms/step - accuracy: 0.4813 - auc: 0.4293 - loss: 0.7708\n",
      "Epoch 13: val_auc did not improve from 0.57530\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 439ms/step - accuracy: 0.4695 - auc: 0.4379 - loss: 0.7709 - val_accuracy: 0.5000 - val_auc: 0.4688 - val_loss: 0.7598\n",
      "Epoch 14/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 389ms/step - accuracy: 0.5233 - auc: 0.3945 - loss: 0.7596\n",
      "Epoch 14: val_auc did not improve from 0.57530\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 435ms/step - accuracy: 0.5076 - auc: 0.4357 - loss: 0.7581 - val_accuracy: 0.4848 - val_auc: 0.4986 - val_loss: 0.7502\n",
      "Epoch 15/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 408ms/step - accuracy: 0.5184 - auc: 0.5084 - loss: 0.7479\n",
      "Epoch 15: val_auc did not improve from 0.57530\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 452ms/step - accuracy: 0.4924 - auc: 0.4954 - loss: 0.7467 - val_accuracy: 0.5152 - val_auc: 0.5500 - val_loss: 0.7414\n",
      "Epoch 16/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 399ms/step - accuracy: 0.5401 - auc: 0.5078 - loss: 0.7396\n",
      "Epoch 16: val_auc improved from 0.57530 to 0.59642, saving model to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\nca_smote_seq\\best_model.keras\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 454ms/step - accuracy: 0.5076 - auc: 0.4808 - loss: 0.7395 - val_accuracy: 0.5758 - val_auc: 0.5964 - val_loss: 0.7334\n",
      "Epoch 17/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 375ms/step - accuracy: 0.4536 - auc: 0.4726 - loss: 0.7337\n",
      "Epoch 17: val_auc improved from 0.59642 to 0.60285, saving model to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\nca_smote_seq\\best_model.keras\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 426ms/step - accuracy: 0.4580 - auc: 0.4956 - loss: 0.7324 - val_accuracy: 0.5152 - val_auc: 0.6028 - val_loss: 0.7263\n",
      "Epoch 18/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 379ms/step - accuracy: 0.5379 - auc: 0.5805 - loss: 0.7255\n",
      "Epoch 18: val_auc did not improve from 0.60285\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 419ms/step - accuracy: 0.5191 - auc: 0.5209 - loss: 0.7256 - val_accuracy: 0.5303 - val_auc: 0.5611 - val_loss: 0.7198\n",
      "Epoch 19/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 373ms/step - accuracy: 0.5785 - auc: 0.6382 - loss: 0.7142\n",
      "Epoch 19: val_auc improved from 0.60285 to 0.71258, saving model to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\nca_smote_seq\\best_model.keras\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 424ms/step - accuracy: 0.5687 - auc: 0.6075 - loss: 0.7163 - val_accuracy: 0.5606 - val_auc: 0.7126 - val_loss: 0.7121\n",
      "Epoch 20/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 384ms/step - accuracy: 0.5409 - auc: 0.4649 - loss: 0.7224\n",
      "Epoch 20: val_auc did not improve from 0.71258\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 422ms/step - accuracy: 0.5344 - auc: 0.4853 - loss: 0.7227 - val_accuracy: 0.5000 - val_auc: 0.5211 - val_loss: 0.7340\n",
      "Epoch 21/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 383ms/step - accuracy: 0.5081 - auc: 0.5812 - loss: 0.7157\n",
      "Epoch 21: val_auc did not improve from 0.71258\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 421ms/step - accuracy: 0.5000 - auc: 0.5237 - loss: 0.7201 - val_accuracy: 0.5000 - val_auc: 0.5000 - val_loss: 0.7182\n",
      "Epoch 22/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 388ms/step - accuracy: 0.4981 - auc: 0.5044 - loss: 0.7186\n",
      "Epoch 22: val_auc did not improve from 0.71258\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 428ms/step - accuracy: 0.5000 - auc: 0.4855 - loss: 0.7190 - val_accuracy: 0.5000 - val_auc: 0.4816 - val_loss: 0.7181\n",
      "Epoch 23/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 400ms/step - accuracy: 0.4862 - auc: 0.4800 - loss: 0.7188\n",
      "Epoch 23: val_auc did not improve from 0.71258\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 441ms/step - accuracy: 0.5000 - auc: 0.5051 - loss: 0.7162 - val_accuracy: 0.5000 - val_auc: 0.5197 - val_loss: 0.7138\n",
      "Epoch 24/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 380ms/step - accuracy: 0.4912 - auc: 0.4748 - loss: 0.7150\n",
      "Epoch 24: val_auc did not improve from 0.71258\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 419ms/step - accuracy: 0.5153 - auc: 0.4810 - loss: 0.7132 - val_accuracy: 0.5606 - val_auc: 0.5073 - val_loss: 0.7095\n",
      "Epoch 25/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 382ms/step - accuracy: 0.5747 - auc: 0.5343 - loss: 0.7086\n",
      "Epoch 25: val_auc did not improve from 0.71258\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 420ms/step - accuracy: 0.5420 - auc: 0.5194 - loss: 0.7105 - val_accuracy: 0.4545 - val_auc: 0.5152 - val_loss: 0.7043\n",
      "Epoch 26/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 382ms/step - accuracy: 0.4878 - auc: 0.5636 - loss: 0.7078\n",
      "Epoch 26: val_auc did not improve from 0.71258\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 420ms/step - accuracy: 0.4924 - auc: 0.5575 - loss: 0.7058 - val_accuracy: 0.5000 - val_auc: 0.5542 - val_loss: 0.6984\n",
      "Epoch 27/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 390ms/step - accuracy: 0.4868 - auc: 0.5761 - loss: 0.7034\n",
      "Epoch 27: val_auc did not improve from 0.71258\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 429ms/step - accuracy: 0.5115 - auc: 0.5492 - loss: 0.7041 - val_accuracy: 0.5455 - val_auc: 0.5606 - val_loss: 0.6938\n",
      "Epoch 28/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 478ms/step - accuracy: 0.5196 - auc: 0.5407 - loss: 0.7035\n",
      "Epoch 28: val_auc did not improve from 0.71258\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 515ms/step - accuracy: 0.5496 - auc: 0.5665 - loss: 0.7013 - val_accuracy: 0.5152 - val_auc: 0.5556 - val_loss: 0.6938\n",
      "Epoch 29/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 449ms/step - accuracy: 0.5333 - auc: 0.5868 - loss: 0.6955\n",
      "Epoch 29: val_auc did not improve from 0.71258\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 494ms/step - accuracy: 0.5611 - auc: 0.6012 - loss: 0.6921 - val_accuracy: 0.5455 - val_auc: 0.5826 - val_loss: 0.6877\n",
      "Epoch 30/30\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 486ms/step - accuracy: 0.5994 - auc: 0.6361 - loss: 0.6800\n",
      "Epoch 30: val_auc did not improve from 0.71258\n",
      "\u001B[1m9/9\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 534ms/step - accuracy: 0.5954 - auc: 0.6177 - loss: 0.6732 - val_accuracy: 0.5455 - val_auc: 0.5698 - val_loss: 0.9236\n",
      "Training history saved to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\nca_smote_seq\\history.csv\n",
      "Training history plot saved to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\nca_smote_seq\\plots\\training_history.png\n",
      "\n",
      "--- Evaluating Model ---\n",
      "\u001B[1m3/3\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 369ms/step\n",
      "Evaluation results saved to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\nca_smote_seq\\evaluation.csv\n",
      "Comprehensive performance plot saved to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\nca_smote_seq\\plots\n",
      "\n",
      "--- Running 5-Fold Cross-Validation ---\n",
      "\n",
      "Training fold 1/5\n",
      "--- Building the model ---\n",
      "Model built successfully.\n",
      "\n",
      "Training fold 2/5\n",
      "--- Building the model ---\n",
      "Model built successfully.\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000212A40004A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000212A40004A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Training fold 3/5\n",
      "--- Building the model ---\n",
      "Model built successfully.\n",
      "\n",
      "Training fold 4/5\n",
      "--- Building the model ---\n",
      "Model built successfully.\n",
      "\n",
      "Training fold 5/5\n",
      "--- Building the model ---\n",
      "Model built successfully.\n",
      "Cross-validation complete. Results saved to D:\\Projects\\Voice\\Parkinson-s-Disease-Detector-Using-AI\\Parkinson-s-Disease-Detector-Using-AI\\1\\UAMS\\results_A_DEFAULT\\nca_smote_seq\\comprehensive_analysis\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "    # Load best model for explainability analysis\n",
    "    if os.path.exists(BEST_MODEL_PATH):\n",
    "        print(\"\\n--- Loading Best Model for Explainability Analysis ---\")\n",
    "        try:\n",
    "            best_model = load_model(BEST_MODEL_PATH,\n",
    "                                  custom_objects={'ParkinsonDetectorModel': ParkinsonDetectorModel})\n",
    "\n",
    "            # SHAP Analysis\n",
    "            run_full_shap_analysis(best_model, X_train, X_test, y_test,\n",
    "                                 SHAP_OUTPUT_PATH, samples_per_class=50, top_n=20)\n",
    "\n",
    "            # Grad-CAM Analysis\n",
    "            run_gradcam_analysis(best_model, X_test, y_test, GRADCAM_OUTPUT_PATH, num_samples=50)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in explainability analysis: {e}\")\n",
    "\n",
    "    # Generate final summary report\n",
    "    print(\"\\n--- Generating Summary Report ---\")\n",
    "\n",
    "    # Load evaluation metrics\n",
    "    eval_df = pd.read_csv(EVALUATION_FILE_PATH)\n",
    "\n",
    "    summary_report = {\n",
    "        'Dataset': dataset,\n",
    "        'Mode': MODE,\n",
    "        'Feature_Mode': FEATURE_MODE,\n",
    "        'Total_Samples': len(y),\n",
    "        'Training_Samples': len(y_train),\n",
    "        'Test_Samples': len(y_test),\n",
    "        'Input_Shape': f\"{X.shape[1]}x{X.shape[2]}\",\n",
    "        'Model_Parameters': model.count_params(),\n",
    "        'Training_Epochs': len(history_df),\n",
    "        'Best_Val_AUC': max(history_df['val_auc']),\n",
    "        'Final_Test_Metrics': eval_df.to_dict('records')\n",
    "    }\n",
    "\n",
    "    # Save summary\n",
    "    with open(os.path.join(ANALYSIS_PATH, 'analysis_summary.txt'), 'w') as f:\n",
    "        for key, value in summary_report.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"All results saved to: {MODEL_PATH}\")\n",
    "    print(f\"- Plots: {PLOTS_PATH}\")\n",
    "    print(f\"- SHAP Analysis: {SHAP_OUTPUT_PATH}\")\n",
    "    print(f\"- Grad-CAM Analysis: {GRADCAM_OUTPUT_PATH}\")\n",
    "    print(f\"- Comprehensive Analysis: {ANALYSIS_PATH}\")\n",
    "    print(\"=\"*60)\n",
    "\n"
   ],
   "id": "924dbf4ae4df9432",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
