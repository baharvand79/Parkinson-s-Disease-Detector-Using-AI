{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf26c4ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshap\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ttest_ind\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bahar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\shap\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_explanation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Cohorts, Explanation\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# explainers\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexplainers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m other\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bahar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\shap\\_explanation.py:16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mslicer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Alias, Obj, Slicer\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_clustering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hclust_ordering\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DimensionError\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_general\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpChain\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bahar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\shap\\utils\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_clustering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     delta_minimization_order,\n\u001b[32m      3\u001b[39m     hclust,\n\u001b[32m      4\u001b[39m     hclust_ordering,\n\u001b[32m      5\u001b[39m     partition_tree,\n\u001b[32m      6\u001b[39m     partition_tree_shuffle,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_general\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     OpChain,\n\u001b[32m     10\u001b[39m     approximate_interactions,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     suppress_stderr,\n\u001b[32m     21\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_masked_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MaskedModel, make_masks\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bahar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\shap\\utils\\_clustering.py:12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspatial\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m njit\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DimensionError\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_progress\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_progress\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bahar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numba\\__init__.py:74\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m generate_version_info\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m types, errors\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Re-export typeof\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     78\u001b[39m     typeof, prange, pndindex, gdb, gdb_breakpoint, gdb_init,\n\u001b[32m     79\u001b[39m     literally, literal_unroll,\n\u001b[32m     80\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:995\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1091\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1190\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Dropout, Flatten,\n",
    "                                     Dense, LSTM, MultiHeadAttention, Concatenate, Reshape)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# =============================================================================\n",
    "# --- 🚀 CONFIGURATION ---\n",
    "# =============================================================================\n",
    "# Define the datasets to use for TRAINING. Can be one or more.\n",
    "# Example: TRAIN_DATASETS = [\"Italian\", \"mPower\"]\n",
    "TRAIN_DATASETS = [\"UAMS\", \"Neurovoz\"] \n",
    "\n",
    "# Define the single dataset to use for TESTING.\n",
    "# - Set to a different dataset name (e.g., \"mPower\") for a true unseen test.\n",
    "# - Set to None to use a validation split from the training data for the final evaluation.\n",
    "TEST_DATASET = None \n",
    "\n",
    "# Define parameters for the feature files\n",
    "MODE = \"ALL_VALIDS\"\n",
    "FEATURE_MODE = \"ALL\"\n",
    "\n",
    "# A unique name for this training run to save models and results\n",
    "RUN_ID = f\"trained_on_{'_'.join(TRAIN_DATASETS)}\"\n",
    "RESULTS_PATH = os.path.join(os.getcwd(), \"runs\", RUN_ID)\n",
    "PLOTS_PATH = os.path.join(RESULTS_PATH, \"plots\") # Centralized folder for all plots\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "os.makedirs(PLOTS_PATH, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT_RATE = 0.5\n",
    "L2_STRENGTH = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd113959",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# --- Your Custom CNN Models ---\n",
    "# =============================================================================\n",
    "@register_keras_serializable()\n",
    "class ParkinsonDetectorModel(Model):\n",
    "    \"\"\"Your end-to-end CNN model for original features.\"\"\"\n",
    "    def __init__(self, input_shape, **kwargs):\n",
    "        super(ParkinsonDetectorModel, self).__init__(**kwargs)\n",
    "        self.input_shape_config = input_shape\n",
    "        self.reshape_in = Reshape((input_shape[0], input_shape[1], 1))\n",
    "        self.conv1a = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv1b = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.pool1 = MaxPooling2D(5)\n",
    "        self.drop1 = Dropout(DROPOUT_RATE)\n",
    "        self.conv2a = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv2b = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same', name=\"last_conv_layer\")\n",
    "        self.pool2 = MaxPooling2D(5)\n",
    "        self.drop2 = Dropout(DROPOUT_RATE)\n",
    "        self.flatten_cnn = Flatten()\n",
    "        self.attention = MultiHeadAttention(num_heads=2, key_dim=64)\n",
    "        self.flatten_att = Flatten()\n",
    "        self.lstm1 = LSTM(128, return_sequences=True)\n",
    "        self.lstm2 = LSTM(128, return_sequences=False)\n",
    "        self.drop_lstm = Dropout(DROPOUT_RATE)\n",
    "        self.concat = Concatenate()\n",
    "        self.dense_bottleneck = Dense(128, activation='relu', name='bottleneck_features')\n",
    "        self.dense_output = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, extract_features=False, grad_cam=False):\n",
    "        x = self.reshape_in(inputs)\n",
    "        x = self.conv1a(x); x = self.conv1b(x); x = self.pool1(x); x = self.drop1(x, training=False)\n",
    "        x = self.conv2a(x)\n",
    "        last_conv_output = self.conv2b(x)\n",
    "        x = self.pool2(last_conv_output); x = self.drop2(x, training=False)\n",
    "        cnn_flat = self.flatten_cnn(x)\n",
    "        shape = tf.shape(x)\n",
    "        sequence = tf.reshape(x, [-1, shape[1] * shape[2], shape[3]])\n",
    "        att_out = self.attention(query=sequence, key=sequence, value=sequence)\n",
    "        att_flat = self.flatten_att(att_out)\n",
    "        lstm_seq = self.lstm1(sequence); lstm_out = self.lstm2(lstm_seq); lstm_out = self.drop_lstm(lstm_out, training=False)\n",
    "        concatenated = self.concat([cnn_flat, att_flat, lstm_out])\n",
    "        bottleneck = self.dense_bottleneck(concatenated)\n",
    "        final_output = self.dense_output(bottleneck)\n",
    "\n",
    "        if grad_cam: return final_output, last_conv_output\n",
    "        if extract_features: return bottleneck\n",
    "        return final_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ParkinsonDetectorModel, self).get_config()\n",
    "        config.update({\"input_shape\": self.input_shape_config}); return config\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "@register_keras_serializable()\n",
    "class ParkinsonDetectorModelNCA(Model):\n",
    "    \"\"\"A modified version of your model for the small NCA input.\"\"\"\n",
    "    def __init__(self, input_shape, **kwargs):\n",
    "        super(ParkinsonDetectorModelNCA, self).__init__(**kwargs)\n",
    "        self.input_shape_config = input_shape\n",
    "        self.reshape_in = Reshape((input_shape[0], input_shape[1], 1))\n",
    "        self.conv1a = Conv2D(64, 3, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv1b = Conv2D(64, 3, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.pool1 = MaxPooling2D(2)\n",
    "        self.drop1 = Dropout(DROPOUT_RATE)\n",
    "        self.conv2a = Conv2D(64, 3, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv2b = Conv2D(64, 3, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same', name=\"last_conv_layer\")\n",
    "        self.pool2 = MaxPooling2D(2)\n",
    "        self.drop2 = Dropout(DROPOUT_RATE)\n",
    "        self.flatten_cnn = Flatten()\n",
    "        self.attention = MultiHeadAttention(num_heads=2, key_dim=64)\n",
    "        self.flatten_att = Flatten()\n",
    "        self.lstm1 = LSTM(128, return_sequences=True)\n",
    "        self.lstm2 = LSTM(128, return_sequences=False)\n",
    "        self.drop_lstm = Dropout(DROPOUT_RATE)\n",
    "        self.concat = Concatenate()\n",
    "        self.dense_bottleneck = Dense(128, activation='relu', name='bottleneck_features')\n",
    "        self.dense_output = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, extract_features=False, grad_cam=False):\n",
    "        x = self.reshape_in(inputs)\n",
    "        x = self.conv1a(x); x = self.conv1b(x); x = self.pool1(x); x = self.drop1(x, training=False)\n",
    "        x = self.conv2a(x)\n",
    "        last_conv_output = self.conv2b(x)\n",
    "        x = self.pool2(last_conv_output); x = self.drop2(x, training=False)\n",
    "        cnn_flat = self.flatten_cnn(x)\n",
    "        shape = tf.shape(x)\n",
    "        sequence = tf.reshape(x, [-1, shape[1] * shape[2], shape[3]])\n",
    "        att_out = self.attention(query=sequence, key=sequence, value=sequence)\n",
    "        att_flat = self.flatten_att(att_out)\n",
    "        lstm_seq = self.lstm1(sequence); lstm_out = self.lstm2(lstm_seq); lstm_out = self.drop_lstm(lstm_out, training=False)\n",
    "        concatenated = self.concat([cnn_flat, att_flat, lstm_out])\n",
    "        bottleneck = self.dense_bottleneck(concatenated)\n",
    "        final_output = self.dense_output(bottleneck)\n",
    "\n",
    "        if grad_cam: return final_output, last_conv_output\n",
    "        if extract_features: return bottleneck\n",
    "        return final_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ParkinsonDetectorModelNCA, self).get_config()\n",
    "        config.update({\"input_shape\": self.input_shape_config}); return config\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "# =============================================================================\n",
    "# --- Data Loading & Helper Functions ---\n",
    "# =============================================================================\n",
    "def load_single_dataset(dataset_name, mode, feature_mode):\n",
    "    path = os.path.join(os.getcwd(), dataset_name, \"data\", f\"features_{mode}_{feature_mode}.npz\")\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"WARNING: Data file not found at {path}. Skipping.\"); return None\n",
    "    print(f\"--- Loading data from {path} ---\")\n",
    "    with np.load(path) as data:\n",
    "        X = np.concatenate((data['mel_spectrogram'], data['mfcc']), axis=1)\n",
    "        labels = data['labels']\n",
    "    print(f\"Loaded {dataset_name} successfully. Shape: {X.shape}\"); return X, labels\n",
    "\n",
    "def load_and_combine_data(dataset_names, mode, feature_mode):\n",
    "    all_X, all_y = [], []\n",
    "    for name in dataset_names:\n",
    "        data = load_single_dataset(name, mode, feature_mode)\n",
    "        if data: all_X.append(data[0]); all_y.append(data[1])\n",
    "    if not all_X: raise ValueError(\"No training data could be loaded. Aborting.\")\n",
    "    combined_X = np.concatenate(all_X, axis=0); combined_y = np.concatenate(all_y, axis=0)\n",
    "    print(f\"\\n--- All training data combined. Final shape: X={combined_X.shape}, y={combined_y.shape} ---\")\n",
    "    return combined_X, combined_y\n",
    "\n",
    "# =============================================================================\n",
    "# --- Plotting and Evaluation Functions ---\n",
    "# =============================================================================\n",
    "def plot_and_save_history(history, model_name, path):\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.to_csv(os.path.join(path, f\"{model_name}_history.csv\"))\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    axes[0].plot(history_df['loss'], label='Train Loss'); axes[0].plot(history_df['val_loss'], label='Val Loss', linestyle='--')\n",
    "    axes[0].set_title(f'{model_name} - Model Loss'); axes[0].set_xlabel('Epoch'); axes[0].legend()\n",
    "    axes[1].plot(history_df['accuracy'], label='Train Acc'); axes[1].plot(history_df['val_accuracy'], label='Val Acc', linestyle='--')\n",
    "    axes[1].set_title(f'{model_name} - Model Accuracy'); axes[1].set_xlabel('Epoch'); axes[1].legend()\n",
    "    if 'auc' in history_df.columns:\n",
    "        axes[2].plot(history_df['auc'], label='Train AUC'); axes[2].plot(history_df['val_auc'], label='Val AUC', linestyle='--')\n",
    "        axes[2].set_title(f'{model_name} - Model AUC'); axes[2].set_xlabel('Epoch'); axes[2].legend()\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(path, f\"{model_name}_history.png\"), dpi=300); plt.close()\n",
    "    print(f\"✅ Saved training history and plot for {model_name}.\")\n",
    "\n",
    "def plot_and_save_confusion_matrix(y_true, y_pred, model_name, path):\n",
    "    cm = confusion_matrix(y_true, y_pred); cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6)); class_names = ['Healthy', 'Parkinson']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], xticklabels=class_names, yticklabels=class_names)\n",
    "    axes[0].set_title(f'{model_name}\\nConfusion Matrix (Counts)'); axes[0].set_xlabel('Predicted'); axes[0].set_ylabel('True')\n",
    "    sns.heatmap(cm_percent, annot=True, fmt='.2%', cmap='Blues', ax=axes[1], xticklabels=class_names, yticklabels=class_names)\n",
    "    axes[1].set_title(f'{model_name}\\nConfusion Matrix (Percentages)'); axes[1].set_xlabel('Predicted'); axes[1].set_ylabel('True')\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(path, f\"{model_name}_confusion_matrix.png\"), dpi=300); plt.close()\n",
    "    print(f\"✅ Saved confusion matrix for {model_name}.\")\n",
    "\n",
    "def plot_and_save_roc_curve(y_true, y_pred_proba, model_name, path):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba); roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(8, 6)); plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:0.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--'); plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{model_name} - ROC Curve'); plt.legend(loc=\"lower right\"); plt.grid(True)\n",
    "    plt.savefig(os.path.join(path, f\"{model_name}_roc_curve.png\"), dpi=300); plt.close()\n",
    "    print(f\"✅ Saved ROC curve for {model_name}.\")\n",
    "\n",
    "# =============================================================================\n",
    "# --- Explainability Functions ---\n",
    "# =============================================================================\n",
    "def run_full_shap_analysis(model, X_train, X_test, y_test, output_path, num_samples=50):\n",
    "    print(\"\\n--- Running SHAP Analysis ---\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # FIX: Create a functional wrapper for the subclassed model to make it compatible with SHAP\n",
    "    inputs = tf.keras.Input(shape=model.input_shape_config)\n",
    "    outputs = model(inputs)\n",
    "    functional_model = Model(inputs, outputs)\n",
    "\n",
    "    idx = np.random.choice(len(X_test), min(num_samples, len(X_test)), replace=False)\n",
    "    test_samples, y_true_samples = X_test[idx], y_test[idx]\n",
    "    \n",
    "    # Use the functional wrapper model with the explainer\n",
    "    explainer = shap.GradientExplainer(functional_model, X_train[:50])\n",
    "    shap_values = explainer.shap_values(test_samples)\n",
    "    \n",
    "    if isinstance(shap_values, list): shap_values = shap_values[0]\n",
    "\n",
    "    # Plotting logic for SHAP\n",
    "    mean_abs_shap = np.mean(np.abs(shap_values.reshape(shap_values.shape[0], -1)), axis=0)\n",
    "    top_idx = np.argsort(mean_abs_shap)[::-1][:20]\n",
    "    coords = [np.unravel_index(i, (shap_values.shape[1], shap_values.shape[2])) for i in top_idx]\n",
    "    labels = [f\"T{t} F{f}\" for t, f in coords]\n",
    "    plt.figure(figsize=(12, 6)); plt.bar(range(20), mean_abs_shap[top_idx])\n",
    "    plt.xticks(range(20), labels, rotation=45, ha=\"right\"); plt.title(\"Top-20 Global SHAP Features\")\n",
    "    plt.ylabel(\"Mean |SHAP value|\"); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, \"shap_global_bar.png\"), dpi=300); plt.close()\n",
    "    print(\"-> Saved SHAP global bar plot.\")\n",
    "\n",
    "    hc_mask, pd_mask = (y_true_samples == 0), (y_true_samples == 1)\n",
    "    if np.any(hc_mask):\n",
    "        hc_mean = shap_values[hc_mask].mean(axis=0).squeeze()\n",
    "        plt.figure(); plt.imshow(hc_mean, cmap=\"bwr\", aspect=\"auto\"); plt.colorbar(); plt.title(\"Average SHAP - Healthy\")\n",
    "        plt.savefig(os.path.join(output_path, \"shap_summary_healthy.png\"), dpi=300); plt.close()\n",
    "    if np.any(pd_mask):\n",
    "        pd_mean = shap_values[pd_mask].mean(axis=0).squeeze()\n",
    "        plt.figure(); plt.imshow(pd_mean, cmap=\"bwr\", aspect=\"auto\"); plt.colorbar(); plt.title(\"Average SHAP - Parkinson's\")\n",
    "        plt.savefig(os.path.join(output_path, \"shap_summary_parkinson.png\"), dpi=300); plt.close()\n",
    "    print(\"-> Saved SHAP class heatmaps.\")\n",
    "    print(\"--- SHAP Analysis Complete ---\")\n",
    "\n",
    "\n",
    "def run_gradcam_analysis(model, X_test, y_test, output_path, num_samples=30):\n",
    "    print(\"\\n--- Running Grad-CAM Analysis ---\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int).flatten()\n",
    "    tp_idx = np.where((y_test == 1) & (y_pred == 1))[0]; tn_idx = np.where((y_test == 0) & (y_pred == 0))[0]\n",
    "    def get_avg_heatmap(indices):\n",
    "        heatmaps = []\n",
    "        for i in tqdm(indices, desc=\"Grad-CAM Progress\", leave=False):\n",
    "            img_array = X_test[i:i+1]\n",
    "            with tf.GradientTape() as tape:\n",
    "                final_preds, last_conv_output = model(img_array, grad_cam=True)\n",
    "                tape.watch(last_conv_output); loss = final_preds[0]\n",
    "            grads = tape.gradient(loss, last_conv_output)\n",
    "            pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "            heatmap = last_conv_output[0] @ pooled_grads[..., tf.newaxis]; heatmap = tf.squeeze(heatmap)\n",
    "            heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + 1e-10)\n",
    "            heatmaps.append(heatmap.numpy())\n",
    "        return np.mean(heatmaps, axis=0) if heatmaps else np.zeros(X_test.shape[1:3])\n",
    "    avg_tp_heatmap = get_avg_heatmap(np.random.choice(tp_idx, min(num_samples, len(tp_idx)), replace=False))\n",
    "    avg_tn_heatmap = get_avg_heatmap(np.random.choice(tn_idx, min(num_samples, len(tn_idx)), replace=False))\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    im1 = axes[0].imshow(avg_tp_heatmap, cmap='jet', aspect='auto'); axes[0].set_title(f'Avg Grad-CAM for Parkinson\\'s (TP)'); fig.colorbar(im1, ax=axes[0])\n",
    "    im2 = axes[1].imshow(avg_tn_heatmap, cmap='jet', aspect='auto'); axes[1].set_title(f'Avg Grad-CAM for Healthy (TN)'); fig.colorbar(im2, ax=axes[1])\n",
    "    plt.suptitle(\"Average Model Attention by Class\"); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, \"gradcam_average_comparison.png\"), dpi=300); plt.close()\n",
    "    print(\"-> Saved average Grad-CAM comparison.\"); print(\"--- Grad-CAM Analysis Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a26563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# --- Main Execution ---\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "    # =========================================================================\n",
    "    # --- PHASE 1: TRAINING ---\n",
    "    # =========================================================================\n",
    "    print(\"=\"*80); print(f\"🚀 STARTING TRAINING PHASE | RUN_ID: {RUN_ID}\"); print(\"=\"*80)\n",
    "    X_train_full, y_train_full = load_and_combine_data(TRAIN_DATASETS, MODE, FEATURE_MODE)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full)\n",
    "    n_samples, d1, d2 = X_train.shape; X_train_2d = X_train.reshape((n_samples, d1*d2)); X_val_2d = X_val.reshape((X_val.shape[0], d1*d2))\n",
    "    print(f\"\\nTraining data prepared. Train: {X_train.shape}, Validation: {X_val.shape}\")\n",
    "    print(\"\\nClass distribution in main training set:\"); print(pd.Series(y_train).value_counts())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42773d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # --- Train Exp 1: k-NN ---\n",
    "    print(\"\\n\" + \"-\"*80); print(\"TRAINING MODEL 1: k-NN (Baseline)\"); print(\"-\"*80)\n",
    "    X_train_full_2d = X_train_full.reshape((X_train_full.shape[0], d1*d2))\n",
    "    pipeline_knn = Pipeline([('scaler', StandardScaler()), ('smote', SMOTE(random_state=42)), ('nca', NeighborhoodComponentsAnalysis(random_state=42, max_iter=200)), ('classifier', KNeighborsClassifier())])\n",
    "    param_dist_knn = {'nca__n_components': [10, 20, 30, 40], 'classifier__n_neighbors': [3, 5, 7], 'classifier__weights': ['distance'], 'classifier__metric': ['manhattan']}\n",
    "    search_knn = RandomizedSearchCV(pipeline_knn, param_dist_knn, n_iter=10, cv=3, scoring='accuracy', n_jobs=1, random_state=42, verbose=1)\n",
    "    search_knn.fit(X_train_full_2d, y_train_full)\n",
    "    joblib.dump(search_knn.best_estimator_, os.path.join(RESULTS_PATH, \"model_1_knn.joblib\"))\n",
    "    print(\"✅ Best k-NN pipeline trained and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5edbb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # --- Train Exp 2: CNN ---\n",
    "    print(\"\\n\" + \"-\"*80); print(\"TRAINING MODEL 2: CNN (End-to-End)\"); print(\"-\"*80)\n",
    "    MODEL_CNN_PATH = os.path.join(RESULTS_PATH, \"model_2_cnn.keras\")\n",
    "    model_cnn = ParkinsonDetectorModel(input_shape=(d1, d2))\n",
    "    model_cnn.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "    history_cnn = model_cnn.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                                callbacks=[ModelCheckpoint(MODEL_CNN_PATH, save_best_only=True, monitor='val_auc', mode='max', verbose=1)])\n",
    "    plot_and_save_history(history_cnn, \"model_2_cnn\", RESULTS_PATH)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1035c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # --- Train Exp 3: CNN + k-NN ---\n",
    "    print(\"\\n\" + \"-\"*80); print(\"TRAINING MODEL 3: CNN Feature Extractor + k-NN\"); print(\"-\"*80)\n",
    "    best_model_cnn_extractor = tf.keras.models.load_model(MODEL_CNN_PATH)\n",
    "    X_train_features = best_model_cnn_extractor(X_train_full, extract_features=True).numpy()\n",
    "    search_cnn_knn = RandomizedSearchCV(pipeline_knn, param_dist_knn, n_iter=10, cv=3, scoring='accuracy', n_jobs=1, random_state=42, verbose=1)\n",
    "    search_cnn_knn.fit(X_train_features, y_train_full)\n",
    "    joblib.dump(search_cnn_knn.best_estimator_, os.path.join(RESULTS_PATH, \"model_3_cnn_knn.joblib\"))\n",
    "    print(\"✅ Best CNN+k-NN pipeline trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a5a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # --- Train Exp 4: NCA + CNN ---\n",
    "    print(\"\\n\" + \"-\"*80); print(\"TRAINING MODEL 4: NCA pre-processing + CNN\"); print(\"-\"*80)\n",
    "    NCA_COMPONENTS = 64; nca_img_dim = int(np.sqrt(NCA_COMPONENTS))\n",
    "    nca_preprocessor = Pipeline([('scaler', StandardScaler()), ('nca', NeighborhoodComponentsAnalysis(n_components=NCA_COMPONENTS, random_state=42, max_iter=200))])\n",
    "    X_train_nca = nca_preprocessor.fit_transform(X_train_full_2d, y_train_full)\n",
    "    joblib.dump(nca_preprocessor, os.path.join(RESULTS_PATH, \"model_4_nca_preprocessor.joblib\"))\n",
    "    X_train_nca_3d = X_train_nca.reshape(-1, nca_img_dim, nca_img_dim)\n",
    "    X_val_nca = nca_preprocessor.transform(X_val_2d); X_val_nca_3d = X_val_nca.reshape(-1, nca_img_dim, nca_img_dim)\n",
    "    MODEL_NCA_CNN_PATH = os.path.join(RESULTS_PATH, \"model_4_nca_cnn.keras\")\n",
    "    model_nca_cnn = ParkinsonDetectorModelNCA(input_shape=(nca_img_dim, nca_img_dim))\n",
    "    model_nca_cnn.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "    history_nca_cnn = model_nca_cnn.fit(X_train_nca_3d, y_train_full, validation_data=(X_val_nca_3d, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                                        callbacks=[ModelCheckpoint(MODEL_NCA_CNN_PATH, save_best_only=True, monitor='val_auc', mode='max', verbose=1)])\n",
    "    plot_and_save_history(history_nca_cnn, \"model_4_nca_cnn\", RESULTS_PATH)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095453a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # =========================================================================\n",
    "    # --- PHASE 2: TESTING AND EXPLAINABILITY ---\n",
    "    # =========================================================================\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    if TEST_DATASET is None:\n",
    "        print(f\"🔬 TESTING & EXPLAINING ON VALIDATION SPLIT FROM: {', '.join(TRAIN_DATASETS)}\")\n",
    "        X_test, y_test = X_val, y_val\n",
    "    else:\n",
    "        print(f\"🔬 TESTING & EXPLAINING ON UNSEEN DATASET: {TEST_DATASET}\")\n",
    "        test_data = load_single_dataset(TEST_DATASET, MODE, FEATURE_MODE)\n",
    "        if test_data is None: exit(\"No test data found. Aborting.\")\n",
    "        X_test, y_test = test_data\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    test_results = {}\n",
    "    \n",
    "    # --- Test Model 1: k-NN ---\n",
    "    print(\"\\n\" + \"-\"*80); print(\"EVALUATING MODEL 1: k-NN (Baseline)\"); print(\"-\"*80)\n",
    "    model_1 = joblib.load(os.path.join(RESULTS_PATH, \"model_1_knn.joblib\"))\n",
    "    X_test_2d = X_test.reshape((X_test.shape[0], d1 * d2))\n",
    "    y_pred_1 = model_1.predict(X_test_2d); y_pred_proba_1 = model_1.predict_proba(X_test_2d)[:, 1]\n",
    "    acc1 = accuracy_score(y_test, y_pred_1); test_results[\"1: k-NN (Baseline)\"] = acc1\n",
    "    print(f\"Accuracy on Test Set: {acc1:.4f}\\n{classification_report(y_test, y_pred_1, digits=4)}\")\n",
    "    plot_and_save_confusion_matrix(y_test, y_pred_1, \"model_1_knn\", PLOTS_PATH)\n",
    "    plot_and_save_roc_curve(y_test, y_pred_proba_1, \"model_1_knn\", PLOTS_PATH)\n",
    "\n",
    "    # --- Test Model 2: CNN ---\n",
    "    print(\"\\n\" + \"-\"*80); print(\"EVALUATING MODEL 2: CNN (End-to-End)\"); print(\"-\"*80)\n",
    "    model_2 = tf.keras.models.load_model(os.path.join(RESULTS_PATH, \"model_2_cnn.keras\"))\n",
    "    y_pred_proba_2 = model_2.predict(X_test); y_pred_2 = (y_pred_proba_2 > 0.5).astype(\"int32\")\n",
    "    acc2 = accuracy_score(y_test, y_pred_2); test_results[\"2: CNN (End-to-End)\"] = acc2\n",
    "    print(f\"Accuracy on Test Set: {acc2:.4f}\\n{classification_report(y_test, y_pred_2, digits=4)}\")\n",
    "    plot_and_save_confusion_matrix(y_test, y_pred_2, \"model_2_cnn\", PLOTS_PATH)\n",
    "    plot_and_save_roc_curve(y_test, y_pred_proba_2, \"model_2_cnn\", PLOTS_PATH)\n",
    "    \n",
    "    # --- Test Model 3: CNN + k-NN ---\n",
    "    print(\"\\n\" + \"-\"*80); print(\"EVALUATING MODEL 3: CNN + k-NN\"); print(\"-\"*80)\n",
    "    model_3_extractor = tf.keras.models.load_model(os.path.join(RESULTS_PATH, \"model_2_cnn.keras\"))\n",
    "    model_3_knn = joblib.load(os.path.join(RESULTS_PATH, \"model_3_cnn_knn.joblib\"))\n",
    "    X_test_features = model_3_extractor(X_test, extract_features=True).numpy()\n",
    "    y_pred_3 = model_3_knn.predict(X_test_features); y_pred_proba_3 = model_3_knn.predict_proba(X_test_features)[:, 1]\n",
    "    acc3 = accuracy_score(y_test, y_pred_3); test_results[\"3: CNN + k-NN\"] = acc3\n",
    "    print(f\"Accuracy on Test Set: {acc3:.4f}\\n{classification_report(y_test, y_pred_3, digits=4)}\")\n",
    "    plot_and_save_confusion_matrix(y_test, y_pred_3, \"model_3_cnn_knn\", PLOTS_PATH)\n",
    "    plot_and_save_roc_curve(y_test, y_pred_proba_3, \"model_3_cnn_knn\", PLOTS_PATH)\n",
    "    \n",
    "    # --- Test Model 4: NCA + CNN ---\n",
    "    print(\"\\n\" + \"-\"*80); print(\"EVALUATING MODEL 4: NCA + CNN\"); print(\"-\"*80)\n",
    "    model_4_preprocessor = joblib.load(os.path.join(RESULTS_PATH, \"model_4_nca_preprocessor.joblib\"))\n",
    "    model_4_cnn = tf.keras.models.load_model(os.path.join(RESULTS_PATH, \"model_4_nca_cnn.keras\"))\n",
    "    X_test_2d_4 = X_test.reshape((X_test.shape[0], d1 * d2))\n",
    "    X_test_nca = model_4_preprocessor.transform(X_test_2d_4)\n",
    "    X_test_nca_3d = X_test_nca.reshape(-1, nca_img_dim, nca_img_dim)\n",
    "    y_pred_proba_4 = model_4_cnn.predict(X_test_nca_3d); y_pred_4 = (y_pred_proba_4 > 0.5).astype(\"int32\")\n",
    "    acc4 = accuracy_score(y_test, y_pred_4); test_results[\"4: NCA + CNN\"] = acc4\n",
    "    print(f\"Accuracy on Test Set: {acc4:.4f}\\n{classification_report(y_test, y_pred_4, digits=4)}\")\n",
    "    plot_and_save_confusion_matrix(y_test, y_pred_4, \"model_4_nca_cnn\", PLOTS_PATH)\n",
    "    plot_and_save_roc_curve(y_test, y_pred_proba_4, \"model_4_nca_cnn\", PLOTS_PATH)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # --- Explainability for CNN-based models ---\n",
    "    print(\"\\n\\n\" + \"=\"*80); print(\"🔬 RUNNING EXPLAINABILITY ANALYSIS\"); print(\"=\"*80)\n",
    "    explain_path_cnn = os.path.join(PLOTS_PATH, \"explainability_cnn_models\")\n",
    "    print(\"\\n--- Explaining Model 2 (CNN) and Model 3 (CNN+k-NN) Feature Extractor ---\")\n",
    "    run_full_shap_analysis(model_2, X_train, X_test, y_test, os.path.join(explain_path_cnn, \"model_2_cnn\"))\n",
    "    run_gradcam_analysis(model_2, X_test, y_test, os.path.join(explain_path_cnn, \"model_2_cnn\"))\n",
    "    print(\"\\n--- Explaining Model 4 (NCA + CNN) ---\")\n",
    "    X_train_full_nca_3d = model_4_preprocessor.transform(X_train_full_2d).reshape(-1, nca_img_dim, nca_img_dim)\n",
    "    run_full_shap_analysis(model_4_cnn, X_train_full_nca_3d, X_test_nca_3d, y_test, os.path.join(explain_path_cnn, \"model_4_nca_cnn\"))\n",
    "    run_gradcam_analysis(model_4_cnn, X_test_nca_3d, y_test, os.path.join(explain_path_cnn, \"model_4_nca_cnn\"))\n",
    "    \n",
    "    # --- Final Summary ---\n",
    "    print(\"\\n\\n\" + \"=\"*80); print(f\"🏆 FINAL SUMMARY - PERFORMANCE ON TEST SET\"); print(\"=\"*80)\n",
    "    best_model_name = max(test_results, key=test_results.get); best_accuracy = test_results[best_model_name]\n",
    "    print(f\"{'Model':<40} | {'Test Accuracy':<20}\"); print(\"-\" * 65)\n",
    "    for name, acc in test_results.items(): print(f\"{name:<40} | {acc:<20.4f}\")\n",
    "    print(\"-\" * 65); print(f\"\\n🚀 Best Performing Model: '{best_model_name}' with an accuracy of {best_accuracy:.4f}\"); print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
